/*! For license information please see main.6f522686.js.LICENSE.txt */
!function(){var e={763:function(e,t,a){var n;e=a.nmd(e),function(){var o,i="Expected a function",r="__lodash_hash_undefined__",d="__lodash_placeholder__",s=16,c=32,l=64,u=128,p=256,f=1/0,h=9007199254740991,b=NaN,g=4294967295,m=[["ary",u],["bind",1],["bindKey",2],["curry",8],["curryRight",s],["flip",512],["partial",c],["partialRight",l],["rearg",p]],y="[object Arguments]",v="[object Array]",w="[object Boolean]",x="[object Date]",k="[object Error]",R="[object Function]",I="[object GeneratorFunction]",A="[object Map]",F="[object Number]",T="[object Object]",S="[object Promise]",C="[object RegExp]",E="[object Set]",M="[object String]",P="[object Symbol]",H="[object WeakMap]",_="[object ArrayBuffer]",z="[object DataView]",j="[object Float32Array]",D="[object Float64Array]",L="[object Int8Array]",U="[object Int16Array]",O="[object Int32Array]",N="[object Uint8Array]",W="[object Uint8ClampedArray]",B="[object Uint16Array]",K="[object Uint32Array]",G=/\b__p \+= '';/g,V=/\b(__p \+=) '' \+/g,J=/(__e\(.*?\)|\b__t\)) \+\n'';/g,Y=/&(?:amp|lt|gt|quot|#39);/g,q=/[&<>"']/g,Z=RegExp(Y.source),$=RegExp(q.source),Q=/<%-([\s\S]+?)%>/g,X=/<%([\s\S]+?)%>/g,ee=/<%=([\s\S]+?)%>/g,te=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,ae=/^\w*$/,ne=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,oe=/[\\^$.*+?()[\]{}|]/g,ie=RegExp(oe.source),re=/^\s+/,de=/\s/,se=/\{(?:\n\/\* \[wrapped with .+\] \*\/)?\n?/,ce=/\{\n\/\* \[wrapped with (.+)\] \*/,le=/,? & /,ue=/[^\x00-\x2f\x3a-\x40\x5b-\x60\x7b-\x7f]+/g,pe=/[()=,{}\[\]\/\s]/,fe=/\\(\\)?/g,he=/\$\{([^\\}]*(?:\\.[^\\}]*)*)\}/g,be=/\w*$/,ge=/^[-+]0x[0-9a-f]+$/i,me=/^0b[01]+$/i,ye=/^\[object .+?Constructor\]$/,ve=/^0o[0-7]+$/i,we=/^(?:0|[1-9]\d*)$/,xe=/[\xc0-\xd6\xd8-\xf6\xf8-\xff\u0100-\u017f]/g,ke=/($^)/,Re=/['\n\r\u2028\u2029\\]/g,Ie="\\u0300-\\u036f\\ufe20-\\ufe2f\\u20d0-\\u20ff",Ae="\\u2700-\\u27bf",Fe="a-z\\xdf-\\xf6\\xf8-\\xff",Te="A-Z\\xc0-\\xd6\\xd8-\\xde",Se="\\ufe0e\\ufe0f",Ce="\\xac\\xb1\\xd7\\xf7\\x00-\\x2f\\x3a-\\x40\\x5b-\\x60\\x7b-\\xbf\\u2000-\\u206f \\t\\x0b\\f\\xa0\\ufeff\\n\\r\\u2028\\u2029\\u1680\\u180e\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u202f\\u205f\\u3000",Ee="['\u2019]",Me="[\\ud800-\\udfff]",Pe="["+Ce+"]",He="["+Ie+"]",_e="\\d+",ze="[\\u2700-\\u27bf]",je="["+Fe+"]",De="[^\\ud800-\\udfff"+Ce+_e+Ae+Fe+Te+"]",Le="\\ud83c[\\udffb-\\udfff]",Ue="[^\\ud800-\\udfff]",Oe="(?:\\ud83c[\\udde6-\\uddff]){2}",Ne="[\\ud800-\\udbff][\\udc00-\\udfff]",We="["+Te+"]",Be="(?:"+je+"|"+De+")",Ke="(?:"+We+"|"+De+")",Ge="(?:['\u2019](?:d|ll|m|re|s|t|ve))?",Ve="(?:['\u2019](?:D|LL|M|RE|S|T|VE))?",Je="(?:"+He+"|"+Le+")"+"?",Ye="[\\ufe0e\\ufe0f]?",qe=Ye+Je+("(?:\\u200d(?:"+[Ue,Oe,Ne].join("|")+")"+Ye+Je+")*"),Ze="(?:"+[ze,Oe,Ne].join("|")+")"+qe,$e="(?:"+[Ue+He+"?",He,Oe,Ne,Me].join("|")+")",Qe=RegExp(Ee,"g"),Xe=RegExp(He,"g"),et=RegExp(Le+"(?="+Le+")|"+$e+qe,"g"),tt=RegExp([We+"?"+je+"+"+Ge+"(?="+[Pe,We,"$"].join("|")+")",Ke+"+"+Ve+"(?="+[Pe,We+Be,"$"].join("|")+")",We+"?"+Be+"+"+Ge,We+"+"+Ve,"\\d*(?:1ST|2ND|3RD|(?![123])\\dTH)(?=\\b|[a-z_])","\\d*(?:1st|2nd|3rd|(?![123])\\dth)(?=\\b|[A-Z_])",_e,Ze].join("|"),"g"),at=RegExp("[\\u200d\\ud800-\\udfff"+Ie+Se+"]"),nt=/[a-z][A-Z]|[A-Z]{2}[a-z]|[0-9][a-zA-Z]|[a-zA-Z][0-9]|[^a-zA-Z0-9 ]/,ot=["Array","Buffer","DataView","Date","Error","Float32Array","Float64Array","Function","Int8Array","Int16Array","Int32Array","Map","Math","Object","Promise","RegExp","Set","String","Symbol","TypeError","Uint8Array","Uint8ClampedArray","Uint16Array","Uint32Array","WeakMap","_","clearTimeout","isFinite","parseInt","setTimeout"],it=-1,rt={};rt[j]=rt[D]=rt[L]=rt[U]=rt[O]=rt[N]=rt[W]=rt[B]=rt[K]=!0,rt[y]=rt[v]=rt[_]=rt[w]=rt[z]=rt[x]=rt[k]=rt[R]=rt[A]=rt[F]=rt[T]=rt[C]=rt[E]=rt[M]=rt[H]=!1;var dt={};dt[y]=dt[v]=dt[_]=dt[z]=dt[w]=dt[x]=dt[j]=dt[D]=dt[L]=dt[U]=dt[O]=dt[A]=dt[F]=dt[T]=dt[C]=dt[E]=dt[M]=dt[P]=dt[N]=dt[W]=dt[B]=dt[K]=!0,dt[k]=dt[R]=dt[H]=!1;var st={"\\":"\\","'":"'","\n":"n","\r":"r","\u2028":"u2028","\u2029":"u2029"},ct=parseFloat,lt=parseInt,ut="object"==typeof a.g&&a.g&&a.g.Object===Object&&a.g,pt="object"==typeof self&&self&&self.Object===Object&&self,ft=ut||pt||Function("return this")(),ht=t&&!t.nodeType&&t,bt=ht&&e&&!e.nodeType&&e,gt=bt&&bt.exports===ht,mt=gt&&ut.process,yt=function(){try{var e=bt&&bt.require&&bt.require("util").types;return e||mt&&mt.binding&&mt.binding("util")}catch(t){}}(),vt=yt&&yt.isArrayBuffer,wt=yt&&yt.isDate,xt=yt&&yt.isMap,kt=yt&&yt.isRegExp,Rt=yt&&yt.isSet,It=yt&&yt.isTypedArray;function At(e,t,a){switch(a.length){case 0:return e.call(t);case 1:return e.call(t,a[0]);case 2:return e.call(t,a[0],a[1]);case 3:return e.call(t,a[0],a[1],a[2])}return e.apply(t,a)}function Ft(e,t,a,n){for(var o=-1,i=null==e?0:e.length;++o<i;){var r=e[o];t(n,r,a(r),e)}return n}function Tt(e,t){for(var a=-1,n=null==e?0:e.length;++a<n&&!1!==t(e[a],a,e););return e}function St(e,t){for(var a=null==e?0:e.length;a--&&!1!==t(e[a],a,e););return e}function Ct(e,t){for(var a=-1,n=null==e?0:e.length;++a<n;)if(!t(e[a],a,e))return!1;return!0}function Et(e,t){for(var a=-1,n=null==e?0:e.length,o=0,i=[];++a<n;){var r=e[a];t(r,a,e)&&(i[o++]=r)}return i}function Mt(e,t){return!!(null==e?0:e.length)&&Nt(e,t,0)>-1}function Pt(e,t,a){for(var n=-1,o=null==e?0:e.length;++n<o;)if(a(t,e[n]))return!0;return!1}function Ht(e,t){for(var a=-1,n=null==e?0:e.length,o=Array(n);++a<n;)o[a]=t(e[a],a,e);return o}function _t(e,t){for(var a=-1,n=t.length,o=e.length;++a<n;)e[o+a]=t[a];return e}function zt(e,t,a,n){var o=-1,i=null==e?0:e.length;for(n&&i&&(a=e[++o]);++o<i;)a=t(a,e[o],o,e);return a}function jt(e,t,a,n){var o=null==e?0:e.length;for(n&&o&&(a=e[--o]);o--;)a=t(a,e[o],o,e);return a}function Dt(e,t){for(var a=-1,n=null==e?0:e.length;++a<n;)if(t(e[a],a,e))return!0;return!1}var Lt=Gt("length");function Ut(e,t,a){var n;return a(e,(function(e,a,o){if(t(e,a,o))return n=a,!1})),n}function Ot(e,t,a,n){for(var o=e.length,i=a+(n?1:-1);n?i--:++i<o;)if(t(e[i],i,e))return i;return-1}function Nt(e,t,a){return t===t?function(e,t,a){var n=a-1,o=e.length;for(;++n<o;)if(e[n]===t)return n;return-1}(e,t,a):Ot(e,Bt,a)}function Wt(e,t,a,n){for(var o=a-1,i=e.length;++o<i;)if(n(e[o],t))return o;return-1}function Bt(e){return e!==e}function Kt(e,t){var a=null==e?0:e.length;return a?Yt(e,t)/a:b}function Gt(e){return function(t){return null==t?o:t[e]}}function Vt(e){return function(t){return null==e?o:e[t]}}function Jt(e,t,a,n,o){return o(e,(function(e,o,i){a=n?(n=!1,e):t(a,e,o,i)})),a}function Yt(e,t){for(var a,n=-1,i=e.length;++n<i;){var r=t(e[n]);r!==o&&(a=a===o?r:a+r)}return a}function qt(e,t){for(var a=-1,n=Array(e);++a<e;)n[a]=t(a);return n}function Zt(e){return e?e.slice(0,ha(e)+1).replace(re,""):e}function $t(e){return function(t){return e(t)}}function Qt(e,t){return Ht(t,(function(t){return e[t]}))}function Xt(e,t){return e.has(t)}function ea(e,t){for(var a=-1,n=e.length;++a<n&&Nt(t,e[a],0)>-1;);return a}function ta(e,t){for(var a=e.length;a--&&Nt(t,e[a],0)>-1;);return a}function aa(e,t){for(var a=e.length,n=0;a--;)e[a]===t&&++n;return n}var na=Vt({"\xc0":"A","\xc1":"A","\xc2":"A","\xc3":"A","\xc4":"A","\xc5":"A","\xe0":"a","\xe1":"a","\xe2":"a","\xe3":"a","\xe4":"a","\xe5":"a","\xc7":"C","\xe7":"c","\xd0":"D","\xf0":"d","\xc8":"E","\xc9":"E","\xca":"E","\xcb":"E","\xe8":"e","\xe9":"e","\xea":"e","\xeb":"e","\xcc":"I","\xcd":"I","\xce":"I","\xcf":"I","\xec":"i","\xed":"i","\xee":"i","\xef":"i","\xd1":"N","\xf1":"n","\xd2":"O","\xd3":"O","\xd4":"O","\xd5":"O","\xd6":"O","\xd8":"O","\xf2":"o","\xf3":"o","\xf4":"o","\xf5":"o","\xf6":"o","\xf8":"o","\xd9":"U","\xda":"U","\xdb":"U","\xdc":"U","\xf9":"u","\xfa":"u","\xfb":"u","\xfc":"u","\xdd":"Y","\xfd":"y","\xff":"y","\xc6":"Ae","\xe6":"ae","\xde":"Th","\xfe":"th","\xdf":"ss","\u0100":"A","\u0102":"A","\u0104":"A","\u0101":"a","\u0103":"a","\u0105":"a","\u0106":"C","\u0108":"C","\u010a":"C","\u010c":"C","\u0107":"c","\u0109":"c","\u010b":"c","\u010d":"c","\u010e":"D","\u0110":"D","\u010f":"d","\u0111":"d","\u0112":"E","\u0114":"E","\u0116":"E","\u0118":"E","\u011a":"E","\u0113":"e","\u0115":"e","\u0117":"e","\u0119":"e","\u011b":"e","\u011c":"G","\u011e":"G","\u0120":"G","\u0122":"G","\u011d":"g","\u011f":"g","\u0121":"g","\u0123":"g","\u0124":"H","\u0126":"H","\u0125":"h","\u0127":"h","\u0128":"I","\u012a":"I","\u012c":"I","\u012e":"I","\u0130":"I","\u0129":"i","\u012b":"i","\u012d":"i","\u012f":"i","\u0131":"i","\u0134":"J","\u0135":"j","\u0136":"K","\u0137":"k","\u0138":"k","\u0139":"L","\u013b":"L","\u013d":"L","\u013f":"L","\u0141":"L","\u013a":"l","\u013c":"l","\u013e":"l","\u0140":"l","\u0142":"l","\u0143":"N","\u0145":"N","\u0147":"N","\u014a":"N","\u0144":"n","\u0146":"n","\u0148":"n","\u014b":"n","\u014c":"O","\u014e":"O","\u0150":"O","\u014d":"o","\u014f":"o","\u0151":"o","\u0154":"R","\u0156":"R","\u0158":"R","\u0155":"r","\u0157":"r","\u0159":"r","\u015a":"S","\u015c":"S","\u015e":"S","\u0160":"S","\u015b":"s","\u015d":"s","\u015f":"s","\u0161":"s","\u0162":"T","\u0164":"T","\u0166":"T","\u0163":"t","\u0165":"t","\u0167":"t","\u0168":"U","\u016a":"U","\u016c":"U","\u016e":"U","\u0170":"U","\u0172":"U","\u0169":"u","\u016b":"u","\u016d":"u","\u016f":"u","\u0171":"u","\u0173":"u","\u0174":"W","\u0175":"w","\u0176":"Y","\u0177":"y","\u0178":"Y","\u0179":"Z","\u017b":"Z","\u017d":"Z","\u017a":"z","\u017c":"z","\u017e":"z","\u0132":"IJ","\u0133":"ij","\u0152":"Oe","\u0153":"oe","\u0149":"'n","\u017f":"s"}),oa=Vt({"&":"&amp;","<":"&lt;",">":"&gt;",'"':"&quot;","'":"&#39;"});function ia(e){return"\\"+st[e]}function ra(e){return at.test(e)}function da(e){var t=-1,a=Array(e.size);return e.forEach((function(e,n){a[++t]=[n,e]})),a}function sa(e,t){return function(a){return e(t(a))}}function ca(e,t){for(var a=-1,n=e.length,o=0,i=[];++a<n;){var r=e[a];r!==t&&r!==d||(e[a]=d,i[o++]=a)}return i}function la(e){var t=-1,a=Array(e.size);return e.forEach((function(e){a[++t]=e})),a}function ua(e){var t=-1,a=Array(e.size);return e.forEach((function(e){a[++t]=[e,e]})),a}function pa(e){return ra(e)?function(e){var t=et.lastIndex=0;for(;et.test(e);)++t;return t}(e):Lt(e)}function fa(e){return ra(e)?function(e){return e.match(et)||[]}(e):function(e){return e.split("")}(e)}function ha(e){for(var t=e.length;t--&&de.test(e.charAt(t)););return t}var ba=Vt({"&amp;":"&","&lt;":"<","&gt;":">","&quot;":'"',"&#39;":"'"});var ga=function e(t){var a=(t=null==t?ft:ga.defaults(ft.Object(),t,ga.pick(ft,ot))).Array,n=t.Date,de=t.Error,Ie=t.Function,Ae=t.Math,Fe=t.Object,Te=t.RegExp,Se=t.String,Ce=t.TypeError,Ee=a.prototype,Me=Ie.prototype,Pe=Fe.prototype,He=t["__core-js_shared__"],_e=Me.toString,ze=Pe.hasOwnProperty,je=0,De=function(){var e=/[^.]+$/.exec(He&&He.keys&&He.keys.IE_PROTO||"");return e?"Symbol(src)_1."+e:""}(),Le=Pe.toString,Ue=_e.call(Fe),Oe=ft._,Ne=Te("^"+_e.call(ze).replace(oe,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$"),We=gt?t.Buffer:o,Be=t.Symbol,Ke=t.Uint8Array,Ge=We?We.allocUnsafe:o,Ve=sa(Fe.getPrototypeOf,Fe),Je=Fe.create,Ye=Pe.propertyIsEnumerable,qe=Ee.splice,Ze=Be?Be.isConcatSpreadable:o,$e=Be?Be.iterator:o,et=Be?Be.toStringTag:o,at=function(){try{var e=fi(Fe,"defineProperty");return e({},"",{}),e}catch(t){}}(),st=t.clearTimeout!==ft.clearTimeout&&t.clearTimeout,ut=n&&n.now!==ft.Date.now&&n.now,pt=t.setTimeout!==ft.setTimeout&&t.setTimeout,ht=Ae.ceil,bt=Ae.floor,mt=Fe.getOwnPropertySymbols,yt=We?We.isBuffer:o,Lt=t.isFinite,Vt=Ee.join,ma=sa(Fe.keys,Fe),ya=Ae.max,va=Ae.min,wa=n.now,xa=t.parseInt,ka=Ae.random,Ra=Ee.reverse,Ia=fi(t,"DataView"),Aa=fi(t,"Map"),Fa=fi(t,"Promise"),Ta=fi(t,"Set"),Sa=fi(t,"WeakMap"),Ca=fi(Fe,"create"),Ea=Sa&&new Sa,Ma={},Pa=Oi(Ia),Ha=Oi(Aa),_a=Oi(Fa),za=Oi(Ta),ja=Oi(Sa),Da=Be?Be.prototype:o,La=Da?Da.valueOf:o,Ua=Da?Da.toString:o;function Oa(e){if(nd(e)&&!Vr(e)&&!(e instanceof Ka)){if(e instanceof Ba)return e;if(ze.call(e,"__wrapped__"))return Ni(e)}return new Ba(e)}var Na=function(){function e(){}return function(t){if(!ad(t))return{};if(Je)return Je(t);e.prototype=t;var a=new e;return e.prototype=o,a}}();function Wa(){}function Ba(e,t){this.__wrapped__=e,this.__actions__=[],this.__chain__=!!t,this.__index__=0,this.__values__=o}function Ka(e){this.__wrapped__=e,this.__actions__=[],this.__dir__=1,this.__filtered__=!1,this.__iteratees__=[],this.__takeCount__=g,this.__views__=[]}function Ga(e){var t=-1,a=null==e?0:e.length;for(this.clear();++t<a;){var n=e[t];this.set(n[0],n[1])}}function Va(e){var t=-1,a=null==e?0:e.length;for(this.clear();++t<a;){var n=e[t];this.set(n[0],n[1])}}function Ja(e){var t=-1,a=null==e?0:e.length;for(this.clear();++t<a;){var n=e[t];this.set(n[0],n[1])}}function Ya(e){var t=-1,a=null==e?0:e.length;for(this.__data__=new Ja;++t<a;)this.add(e[t])}function qa(e){var t=this.__data__=new Va(e);this.size=t.size}function Za(e,t){var a=Vr(e),n=!a&&Gr(e),o=!a&&!n&&Zr(e),i=!a&&!n&&!o&&ud(e),r=a||n||o||i,d=r?qt(e.length,Se):[],s=d.length;for(var c in e)!t&&!ze.call(e,c)||r&&("length"==c||o&&("offset"==c||"parent"==c)||i&&("buffer"==c||"byteLength"==c||"byteOffset"==c)||wi(c,s))||d.push(c);return d}function $a(e){var t=e.length;return t?e[qn(0,t-1)]:o}function Qa(e,t){return Di(Mo(e),sn(t,0,e.length))}function Xa(e){return Di(Mo(e))}function en(e,t,a){(a!==o&&!Wr(e[t],a)||a===o&&!(t in e))&&rn(e,t,a)}function tn(e,t,a){var n=e[t];ze.call(e,t)&&Wr(n,a)&&(a!==o||t in e)||rn(e,t,a)}function an(e,t){for(var a=e.length;a--;)if(Wr(e[a][0],t))return a;return-1}function nn(e,t,a,n){return fn(e,(function(e,o,i){t(n,e,a(e),i)})),n}function on(e,t){return e&&Po(t,Hd(t),e)}function rn(e,t,a){"__proto__"==t&&at?at(e,t,{configurable:!0,enumerable:!0,value:a,writable:!0}):e[t]=a}function dn(e,t){for(var n=-1,i=t.length,r=a(i),d=null==e;++n<i;)r[n]=d?o:Sd(e,t[n]);return r}function sn(e,t,a){return e===e&&(a!==o&&(e=e<=a?e:a),t!==o&&(e=e>=t?e:t)),e}function cn(e,t,a,n,i,r){var d,s=1&t,c=2&t,l=4&t;if(a&&(d=i?a(e,n,i,r):a(e)),d!==o)return d;if(!ad(e))return e;var u=Vr(e);if(u){if(d=function(e){var t=e.length,a=new e.constructor(t);t&&"string"==typeof e[0]&&ze.call(e,"index")&&(a.index=e.index,a.input=e.input);return a}(e),!s)return Mo(e,d)}else{var p=gi(e),f=p==R||p==I;if(Zr(e))return Ao(e,s);if(p==T||p==y||f&&!i){if(d=c||f?{}:yi(e),!s)return c?function(e,t){return Po(e,bi(e),t)}(e,function(e,t){return e&&Po(t,_d(t),e)}(d,e)):function(e,t){return Po(e,hi(e),t)}(e,on(d,e))}else{if(!dt[p])return i?e:{};d=function(e,t,a){var n=e.constructor;switch(t){case _:return Fo(e);case w:case x:return new n(+e);case z:return function(e,t){var a=t?Fo(e.buffer):e.buffer;return new e.constructor(a,e.byteOffset,e.byteLength)}(e,a);case j:case D:case L:case U:case O:case N:case W:case B:case K:return To(e,a);case A:return new n;case F:case M:return new n(e);case C:return function(e){var t=new e.constructor(e.source,be.exec(e));return t.lastIndex=e.lastIndex,t}(e);case E:return new n;case P:return o=e,La?Fe(La.call(o)):{}}var o}(e,p,s)}}r||(r=new qa);var h=r.get(e);if(h)return h;r.set(e,d),sd(e)?e.forEach((function(n){d.add(cn(n,t,a,n,e,r))})):od(e)&&e.forEach((function(n,o){d.set(o,cn(n,t,a,o,e,r))}));var b=u?o:(l?c?ri:ii:c?_d:Hd)(e);return Tt(b||e,(function(n,o){b&&(n=e[o=n]),tn(d,o,cn(n,t,a,o,e,r))})),d}function ln(e,t,a){var n=a.length;if(null==e)return!n;for(e=Fe(e);n--;){var i=a[n],r=t[i],d=e[i];if(d===o&&!(i in e)||!r(d))return!1}return!0}function un(e,t,a){if("function"!=typeof e)throw new Ce(i);return Hi((function(){e.apply(o,a)}),t)}function pn(e,t,a,n){var o=-1,i=Mt,r=!0,d=e.length,s=[],c=t.length;if(!d)return s;a&&(t=Ht(t,$t(a))),n?(i=Pt,r=!1):t.length>=200&&(i=Xt,r=!1,t=new Ya(t));e:for(;++o<d;){var l=e[o],u=null==a?l:a(l);if(l=n||0!==l?l:0,r&&u===u){for(var p=c;p--;)if(t[p]===u)continue e;s.push(l)}else i(t,u,n)||s.push(l)}return s}Oa.templateSettings={escape:Q,evaluate:X,interpolate:ee,variable:"",imports:{_:Oa}},Oa.prototype=Wa.prototype,Oa.prototype.constructor=Oa,Ba.prototype=Na(Wa.prototype),Ba.prototype.constructor=Ba,Ka.prototype=Na(Wa.prototype),Ka.prototype.constructor=Ka,Ga.prototype.clear=function(){this.__data__=Ca?Ca(null):{},this.size=0},Ga.prototype.delete=function(e){var t=this.has(e)&&delete this.__data__[e];return this.size-=t?1:0,t},Ga.prototype.get=function(e){var t=this.__data__;if(Ca){var a=t[e];return a===r?o:a}return ze.call(t,e)?t[e]:o},Ga.prototype.has=function(e){var t=this.__data__;return Ca?t[e]!==o:ze.call(t,e)},Ga.prototype.set=function(e,t){var a=this.__data__;return this.size+=this.has(e)?0:1,a[e]=Ca&&t===o?r:t,this},Va.prototype.clear=function(){this.__data__=[],this.size=0},Va.prototype.delete=function(e){var t=this.__data__,a=an(t,e);return!(a<0)&&(a==t.length-1?t.pop():qe.call(t,a,1),--this.size,!0)},Va.prototype.get=function(e){var t=this.__data__,a=an(t,e);return a<0?o:t[a][1]},Va.prototype.has=function(e){return an(this.__data__,e)>-1},Va.prototype.set=function(e,t){var a=this.__data__,n=an(a,e);return n<0?(++this.size,a.push([e,t])):a[n][1]=t,this},Ja.prototype.clear=function(){this.size=0,this.__data__={hash:new Ga,map:new(Aa||Va),string:new Ga}},Ja.prototype.delete=function(e){var t=ui(this,e).delete(e);return this.size-=t?1:0,t},Ja.prototype.get=function(e){return ui(this,e).get(e)},Ja.prototype.has=function(e){return ui(this,e).has(e)},Ja.prototype.set=function(e,t){var a=ui(this,e),n=a.size;return a.set(e,t),this.size+=a.size==n?0:1,this},Ya.prototype.add=Ya.prototype.push=function(e){return this.__data__.set(e,r),this},Ya.prototype.has=function(e){return this.__data__.has(e)},qa.prototype.clear=function(){this.__data__=new Va,this.size=0},qa.prototype.delete=function(e){var t=this.__data__,a=t.delete(e);return this.size=t.size,a},qa.prototype.get=function(e){return this.__data__.get(e)},qa.prototype.has=function(e){return this.__data__.has(e)},qa.prototype.set=function(e,t){var a=this.__data__;if(a instanceof Va){var n=a.__data__;if(!Aa||n.length<199)return n.push([e,t]),this.size=++a.size,this;a=this.__data__=new Ja(n)}return a.set(e,t),this.size=a.size,this};var fn=zo(xn),hn=zo(kn,!0);function bn(e,t){var a=!0;return fn(e,(function(e,n,o){return a=!!t(e,n,o)})),a}function gn(e,t,a){for(var n=-1,i=e.length;++n<i;){var r=e[n],d=t(r);if(null!=d&&(s===o?d===d&&!ld(d):a(d,s)))var s=d,c=r}return c}function mn(e,t){var a=[];return fn(e,(function(e,n,o){t(e,n,o)&&a.push(e)})),a}function yn(e,t,a,n,o){var i=-1,r=e.length;for(a||(a=vi),o||(o=[]);++i<r;){var d=e[i];t>0&&a(d)?t>1?yn(d,t-1,a,n,o):_t(o,d):n||(o[o.length]=d)}return o}var vn=jo(),wn=jo(!0);function xn(e,t){return e&&vn(e,t,Hd)}function kn(e,t){return e&&wn(e,t,Hd)}function Rn(e,t){return Et(t,(function(t){return Xr(e[t])}))}function In(e,t){for(var a=0,n=(t=xo(t,e)).length;null!=e&&a<n;)e=e[Ui(t[a++])];return a&&a==n?e:o}function An(e,t,a){var n=t(e);return Vr(e)?n:_t(n,a(e))}function Fn(e){return null==e?e===o?"[object Undefined]":"[object Null]":et&&et in Fe(e)?function(e){var t=ze.call(e,et),a=e[et];try{e[et]=o;var n=!0}catch(r){}var i=Le.call(e);n&&(t?e[et]=a:delete e[et]);return i}(e):function(e){return Le.call(e)}(e)}function Tn(e,t){return e>t}function Sn(e,t){return null!=e&&ze.call(e,t)}function Cn(e,t){return null!=e&&t in Fe(e)}function En(e,t,n){for(var i=n?Pt:Mt,r=e[0].length,d=e.length,s=d,c=a(d),l=1/0,u=[];s--;){var p=e[s];s&&t&&(p=Ht(p,$t(t))),l=va(p.length,l),c[s]=!n&&(t||r>=120&&p.length>=120)?new Ya(s&&p):o}p=e[0];var f=-1,h=c[0];e:for(;++f<r&&u.length<l;){var b=p[f],g=t?t(b):b;if(b=n||0!==b?b:0,!(h?Xt(h,g):i(u,g,n))){for(s=d;--s;){var m=c[s];if(!(m?Xt(m,g):i(e[s],g,n)))continue e}h&&h.push(g),u.push(b)}}return u}function Mn(e,t,a){var n=null==(e=Ci(e,t=xo(t,e)))?e:e[Ui(Qi(t))];return null==n?o:At(n,e,a)}function Pn(e){return nd(e)&&Fn(e)==y}function Hn(e,t,a,n,i){return e===t||(null==e||null==t||!nd(e)&&!nd(t)?e!==e&&t!==t:function(e,t,a,n,i,r){var d=Vr(e),s=Vr(t),c=d?v:gi(e),l=s?v:gi(t),u=(c=c==y?T:c)==T,p=(l=l==y?T:l)==T,f=c==l;if(f&&Zr(e)){if(!Zr(t))return!1;d=!0,u=!1}if(f&&!u)return r||(r=new qa),d||ud(e)?ni(e,t,a,n,i,r):function(e,t,a,n,o,i,r){switch(a){case z:if(e.byteLength!=t.byteLength||e.byteOffset!=t.byteOffset)return!1;e=e.buffer,t=t.buffer;case _:return!(e.byteLength!=t.byteLength||!i(new Ke(e),new Ke(t)));case w:case x:case F:return Wr(+e,+t);case k:return e.name==t.name&&e.message==t.message;case C:case M:return e==t+"";case A:var d=da;case E:var s=1&n;if(d||(d=la),e.size!=t.size&&!s)return!1;var c=r.get(e);if(c)return c==t;n|=2,r.set(e,t);var l=ni(d(e),d(t),n,o,i,r);return r.delete(e),l;case P:if(La)return La.call(e)==La.call(t)}return!1}(e,t,c,a,n,i,r);if(!(1&a)){var h=u&&ze.call(e,"__wrapped__"),b=p&&ze.call(t,"__wrapped__");if(h||b){var g=h?e.value():e,m=b?t.value():t;return r||(r=new qa),i(g,m,a,n,r)}}if(!f)return!1;return r||(r=new qa),function(e,t,a,n,i,r){var d=1&a,s=ii(e),c=s.length,l=ii(t).length;if(c!=l&&!d)return!1;var u=c;for(;u--;){var p=s[u];if(!(d?p in t:ze.call(t,p)))return!1}var f=r.get(e),h=r.get(t);if(f&&h)return f==t&&h==e;var b=!0;r.set(e,t),r.set(t,e);var g=d;for(;++u<c;){var m=e[p=s[u]],y=t[p];if(n)var v=d?n(y,m,p,t,e,r):n(m,y,p,e,t,r);if(!(v===o?m===y||i(m,y,a,n,r):v)){b=!1;break}g||(g="constructor"==p)}if(b&&!g){var w=e.constructor,x=t.constructor;w==x||!("constructor"in e)||!("constructor"in t)||"function"==typeof w&&w instanceof w&&"function"==typeof x&&x instanceof x||(b=!1)}return r.delete(e),r.delete(t),b}(e,t,a,n,i,r)}(e,t,a,n,Hn,i))}function _n(e,t,a,n){var i=a.length,r=i,d=!n;if(null==e)return!r;for(e=Fe(e);i--;){var s=a[i];if(d&&s[2]?s[1]!==e[s[0]]:!(s[0]in e))return!1}for(;++i<r;){var c=(s=a[i])[0],l=e[c],u=s[1];if(d&&s[2]){if(l===o&&!(c in e))return!1}else{var p=new qa;if(n)var f=n(l,u,c,e,t,p);if(!(f===o?Hn(u,l,3,n,p):f))return!1}}return!0}function zn(e){return!(!ad(e)||(t=e,De&&De in t))&&(Xr(e)?Ne:ye).test(Oi(e));var t}function jn(e){return"function"==typeof e?e:null==e?is:"object"==typeof e?Vr(e)?Wn(e[0],e[1]):Nn(e):hs(e)}function Dn(e){if(!Ai(e))return ma(e);var t=[];for(var a in Fe(e))ze.call(e,a)&&"constructor"!=a&&t.push(a);return t}function Ln(e){if(!ad(e))return function(e){var t=[];if(null!=e)for(var a in Fe(e))t.push(a);return t}(e);var t=Ai(e),a=[];for(var n in e)("constructor"!=n||!t&&ze.call(e,n))&&a.push(n);return a}function Un(e,t){return e<t}function On(e,t){var n=-1,o=Yr(e)?a(e.length):[];return fn(e,(function(e,a,i){o[++n]=t(e,a,i)})),o}function Nn(e){var t=pi(e);return 1==t.length&&t[0][2]?Ti(t[0][0],t[0][1]):function(a){return a===e||_n(a,e,t)}}function Wn(e,t){return ki(e)&&Fi(t)?Ti(Ui(e),t):function(a){var n=Sd(a,e);return n===o&&n===t?Cd(a,e):Hn(t,n,3)}}function Bn(e,t,a,n,i){e!==t&&vn(t,(function(r,d){if(i||(i=new qa),ad(r))!function(e,t,a,n,i,r,d){var s=Mi(e,a),c=Mi(t,a),l=d.get(c);if(l)return void en(e,a,l);var u=r?r(s,c,a+"",e,t,d):o,p=u===o;if(p){var f=Vr(c),h=!f&&Zr(c),b=!f&&!h&&ud(c);u=c,f||h||b?Vr(s)?u=s:qr(s)?u=Mo(s):h?(p=!1,u=Ao(c,!0)):b?(p=!1,u=To(c,!0)):u=[]:rd(c)||Gr(c)?(u=s,Gr(s)?u=vd(s):ad(s)&&!Xr(s)||(u=yi(c))):p=!1}p&&(d.set(c,u),i(u,c,n,r,d),d.delete(c));en(e,a,u)}(e,t,d,a,Bn,n,i);else{var s=n?n(Mi(e,d),r,d+"",e,t,i):o;s===o&&(s=r),en(e,d,s)}}),_d)}function Kn(e,t){var a=e.length;if(a)return wi(t+=t<0?a:0,a)?e[t]:o}function Gn(e,t,a){t=t.length?Ht(t,(function(e){return Vr(e)?function(t){return In(t,1===e.length?e[0]:e)}:e})):[is];var n=-1;t=Ht(t,$t(li()));var o=On(e,(function(e,a,o){var i=Ht(t,(function(t){return t(e)}));return{criteria:i,index:++n,value:e}}));return function(e,t){var a=e.length;for(e.sort(t);a--;)e[a]=e[a].value;return e}(o,(function(e,t){return function(e,t,a){var n=-1,o=e.criteria,i=t.criteria,r=o.length,d=a.length;for(;++n<r;){var s=So(o[n],i[n]);if(s)return n>=d?s:s*("desc"==a[n]?-1:1)}return e.index-t.index}(e,t,a)}))}function Vn(e,t,a){for(var n=-1,o=t.length,i={};++n<o;){var r=t[n],d=In(e,r);a(d,r)&&eo(i,xo(r,e),d)}return i}function Jn(e,t,a,n){var o=n?Wt:Nt,i=-1,r=t.length,d=e;for(e===t&&(t=Mo(t)),a&&(d=Ht(e,$t(a)));++i<r;)for(var s=0,c=t[i],l=a?a(c):c;(s=o(d,l,s,n))>-1;)d!==e&&qe.call(d,s,1),qe.call(e,s,1);return e}function Yn(e,t){for(var a=e?t.length:0,n=a-1;a--;){var o=t[a];if(a==n||o!==i){var i=o;wi(o)?qe.call(e,o,1):fo(e,o)}}return e}function qn(e,t){return e+bt(ka()*(t-e+1))}function Zn(e,t){var a="";if(!e||t<1||t>h)return a;do{t%2&&(a+=e),(t=bt(t/2))&&(e+=e)}while(t);return a}function $n(e,t){return _i(Si(e,t,is),e+"")}function Qn(e){return $a(Wd(e))}function Xn(e,t){var a=Wd(e);return Di(a,sn(t,0,a.length))}function eo(e,t,a,n){if(!ad(e))return e;for(var i=-1,r=(t=xo(t,e)).length,d=r-1,s=e;null!=s&&++i<r;){var c=Ui(t[i]),l=a;if("__proto__"===c||"constructor"===c||"prototype"===c)return e;if(i!=d){var u=s[c];(l=n?n(u,c,s):o)===o&&(l=ad(u)?u:wi(t[i+1])?[]:{})}tn(s,c,l),s=s[c]}return e}var to=Ea?function(e,t){return Ea.set(e,t),e}:is,ao=at?function(e,t){return at(e,"toString",{configurable:!0,enumerable:!1,value:as(t),writable:!0})}:is;function no(e){return Di(Wd(e))}function oo(e,t,n){var o=-1,i=e.length;t<0&&(t=-t>i?0:i+t),(n=n>i?i:n)<0&&(n+=i),i=t>n?0:n-t>>>0,t>>>=0;for(var r=a(i);++o<i;)r[o]=e[o+t];return r}function io(e,t){var a;return fn(e,(function(e,n,o){return!(a=t(e,n,o))})),!!a}function ro(e,t,a){var n=0,o=null==e?n:e.length;if("number"==typeof t&&t===t&&o<=2147483647){for(;n<o;){var i=n+o>>>1,r=e[i];null!==r&&!ld(r)&&(a?r<=t:r<t)?n=i+1:o=i}return o}return so(e,t,is,a)}function so(e,t,a,n){var i=0,r=null==e?0:e.length;if(0===r)return 0;for(var d=(t=a(t))!==t,s=null===t,c=ld(t),l=t===o;i<r;){var u=bt((i+r)/2),p=a(e[u]),f=p!==o,h=null===p,b=p===p,g=ld(p);if(d)var m=n||b;else m=l?b&&(n||f):s?b&&f&&(n||!h):c?b&&f&&!h&&(n||!g):!h&&!g&&(n?p<=t:p<t);m?i=u+1:r=u}return va(r,4294967294)}function co(e,t){for(var a=-1,n=e.length,o=0,i=[];++a<n;){var r=e[a],d=t?t(r):r;if(!a||!Wr(d,s)){var s=d;i[o++]=0===r?0:r}}return i}function lo(e){return"number"==typeof e?e:ld(e)?b:+e}function uo(e){if("string"==typeof e)return e;if(Vr(e))return Ht(e,uo)+"";if(ld(e))return Ua?Ua.call(e):"";var t=e+"";return"0"==t&&1/e==-1/0?"-0":t}function po(e,t,a){var n=-1,o=Mt,i=e.length,r=!0,d=[],s=d;if(a)r=!1,o=Pt;else if(i>=200){var c=t?null:$o(e);if(c)return la(c);r=!1,o=Xt,s=new Ya}else s=t?[]:d;e:for(;++n<i;){var l=e[n],u=t?t(l):l;if(l=a||0!==l?l:0,r&&u===u){for(var p=s.length;p--;)if(s[p]===u)continue e;t&&s.push(u),d.push(l)}else o(s,u,a)||(s!==d&&s.push(u),d.push(l))}return d}function fo(e,t){return null==(e=Ci(e,t=xo(t,e)))||delete e[Ui(Qi(t))]}function ho(e,t,a,n){return eo(e,t,a(In(e,t)),n)}function bo(e,t,a,n){for(var o=e.length,i=n?o:-1;(n?i--:++i<o)&&t(e[i],i,e););return a?oo(e,n?0:i,n?i+1:o):oo(e,n?i+1:0,n?o:i)}function go(e,t){var a=e;return a instanceof Ka&&(a=a.value()),zt(t,(function(e,t){return t.func.apply(t.thisArg,_t([e],t.args))}),a)}function mo(e,t,n){var o=e.length;if(o<2)return o?po(e[0]):[];for(var i=-1,r=a(o);++i<o;)for(var d=e[i],s=-1;++s<o;)s!=i&&(r[i]=pn(r[i]||d,e[s],t,n));return po(yn(r,1),t,n)}function yo(e,t,a){for(var n=-1,i=e.length,r=t.length,d={};++n<i;){var s=n<r?t[n]:o;a(d,e[n],s)}return d}function vo(e){return qr(e)?e:[]}function wo(e){return"function"==typeof e?e:is}function xo(e,t){return Vr(e)?e:ki(e,t)?[e]:Li(wd(e))}var ko=$n;function Ro(e,t,a){var n=e.length;return a=a===o?n:a,!t&&a>=n?e:oo(e,t,a)}var Io=st||function(e){return ft.clearTimeout(e)};function Ao(e,t){if(t)return e.slice();var a=e.length,n=Ge?Ge(a):new e.constructor(a);return e.copy(n),n}function Fo(e){var t=new e.constructor(e.byteLength);return new Ke(t).set(new Ke(e)),t}function To(e,t){var a=t?Fo(e.buffer):e.buffer;return new e.constructor(a,e.byteOffset,e.length)}function So(e,t){if(e!==t){var a=e!==o,n=null===e,i=e===e,r=ld(e),d=t!==o,s=null===t,c=t===t,l=ld(t);if(!s&&!l&&!r&&e>t||r&&d&&c&&!s&&!l||n&&d&&c||!a&&c||!i)return 1;if(!n&&!r&&!l&&e<t||l&&a&&i&&!n&&!r||s&&a&&i||!d&&i||!c)return-1}return 0}function Co(e,t,n,o){for(var i=-1,r=e.length,d=n.length,s=-1,c=t.length,l=ya(r-d,0),u=a(c+l),p=!o;++s<c;)u[s]=t[s];for(;++i<d;)(p||i<r)&&(u[n[i]]=e[i]);for(;l--;)u[s++]=e[i++];return u}function Eo(e,t,n,o){for(var i=-1,r=e.length,d=-1,s=n.length,c=-1,l=t.length,u=ya(r-s,0),p=a(u+l),f=!o;++i<u;)p[i]=e[i];for(var h=i;++c<l;)p[h+c]=t[c];for(;++d<s;)(f||i<r)&&(p[h+n[d]]=e[i++]);return p}function Mo(e,t){var n=-1,o=e.length;for(t||(t=a(o));++n<o;)t[n]=e[n];return t}function Po(e,t,a,n){var i=!a;a||(a={});for(var r=-1,d=t.length;++r<d;){var s=t[r],c=n?n(a[s],e[s],s,a,e):o;c===o&&(c=e[s]),i?rn(a,s,c):tn(a,s,c)}return a}function Ho(e,t){return function(a,n){var o=Vr(a)?Ft:nn,i=t?t():{};return o(a,e,li(n,2),i)}}function _o(e){return $n((function(t,a){var n=-1,i=a.length,r=i>1?a[i-1]:o,d=i>2?a[2]:o;for(r=e.length>3&&"function"==typeof r?(i--,r):o,d&&xi(a[0],a[1],d)&&(r=i<3?o:r,i=1),t=Fe(t);++n<i;){var s=a[n];s&&e(t,s,n,r)}return t}))}function zo(e,t){return function(a,n){if(null==a)return a;if(!Yr(a))return e(a,n);for(var o=a.length,i=t?o:-1,r=Fe(a);(t?i--:++i<o)&&!1!==n(r[i],i,r););return a}}function jo(e){return function(t,a,n){for(var o=-1,i=Fe(t),r=n(t),d=r.length;d--;){var s=r[e?d:++o];if(!1===a(i[s],s,i))break}return t}}function Do(e){return function(t){var a=ra(t=wd(t))?fa(t):o,n=a?a[0]:t.charAt(0),i=a?Ro(a,1).join(""):t.slice(1);return n[e]()+i}}function Lo(e){return function(t){return zt(Xd(Gd(t).replace(Qe,"")),e,"")}}function Uo(e){return function(){var t=arguments;switch(t.length){case 0:return new e;case 1:return new e(t[0]);case 2:return new e(t[0],t[1]);case 3:return new e(t[0],t[1],t[2]);case 4:return new e(t[0],t[1],t[2],t[3]);case 5:return new e(t[0],t[1],t[2],t[3],t[4]);case 6:return new e(t[0],t[1],t[2],t[3],t[4],t[5]);case 7:return new e(t[0],t[1],t[2],t[3],t[4],t[5],t[6])}var a=Na(e.prototype),n=e.apply(a,t);return ad(n)?n:a}}function Oo(e){return function(t,a,n){var i=Fe(t);if(!Yr(t)){var r=li(a,3);t=Hd(t),a=function(e){return r(i[e],e,i)}}var d=e(t,a,n);return d>-1?i[r?t[d]:d]:o}}function No(e){return oi((function(t){var a=t.length,n=a,r=Ba.prototype.thru;for(e&&t.reverse();n--;){var d=t[n];if("function"!=typeof d)throw new Ce(i);if(r&&!s&&"wrapper"==si(d))var s=new Ba([],!0)}for(n=s?n:a;++n<a;){var c=si(d=t[n]),l="wrapper"==c?di(d):o;s=l&&Ri(l[0])&&424==l[1]&&!l[4].length&&1==l[9]?s[si(l[0])].apply(s,l[3]):1==d.length&&Ri(d)?s[c]():s.thru(d)}return function(){var e=arguments,n=e[0];if(s&&1==e.length&&Vr(n))return s.plant(n).value();for(var o=0,i=a?t[o].apply(this,e):n;++o<a;)i=t[o].call(this,i);return i}}))}function Wo(e,t,n,i,r,d,s,c,l,p){var f=t&u,h=1&t,b=2&t,g=24&t,m=512&t,y=b?o:Uo(e);return function o(){for(var u=arguments.length,v=a(u),w=u;w--;)v[w]=arguments[w];if(g)var x=ci(o),k=aa(v,x);if(i&&(v=Co(v,i,r,g)),d&&(v=Eo(v,d,s,g)),u-=k,g&&u<p){var R=ca(v,x);return qo(e,t,Wo,o.placeholder,n,v,R,c,l,p-u)}var I=h?n:this,A=b?I[e]:e;return u=v.length,c?v=Ei(v,c):m&&u>1&&v.reverse(),f&&l<u&&(v.length=l),this&&this!==ft&&this instanceof o&&(A=y||Uo(A)),A.apply(I,v)}}function Bo(e,t){return function(a,n){return function(e,t,a,n){return xn(e,(function(e,o,i){t(n,a(e),o,i)})),n}(a,e,t(n),{})}}function Ko(e,t){return function(a,n){var i;if(a===o&&n===o)return t;if(a!==o&&(i=a),n!==o){if(i===o)return n;"string"==typeof a||"string"==typeof n?(a=uo(a),n=uo(n)):(a=lo(a),n=lo(n)),i=e(a,n)}return i}}function Go(e){return oi((function(t){return t=Ht(t,$t(li())),$n((function(a){var n=this;return e(t,(function(e){return At(e,n,a)}))}))}))}function Vo(e,t){var a=(t=t===o?" ":uo(t)).length;if(a<2)return a?Zn(t,e):t;var n=Zn(t,ht(e/pa(t)));return ra(t)?Ro(fa(n),0,e).join(""):n.slice(0,e)}function Jo(e){return function(t,n,i){return i&&"number"!=typeof i&&xi(t,n,i)&&(n=i=o),t=bd(t),n===o?(n=t,t=0):n=bd(n),function(e,t,n,o){for(var i=-1,r=ya(ht((t-e)/(n||1)),0),d=a(r);r--;)d[o?r:++i]=e,e+=n;return d}(t,n,i=i===o?t<n?1:-1:bd(i),e)}}function Yo(e){return function(t,a){return"string"==typeof t&&"string"==typeof a||(t=yd(t),a=yd(a)),e(t,a)}}function qo(e,t,a,n,i,r,d,s,u,p){var f=8&t;t|=f?c:l,4&(t&=~(f?l:c))||(t&=-4);var h=[e,t,i,f?r:o,f?d:o,f?o:r,f?o:d,s,u,p],b=a.apply(o,h);return Ri(e)&&Pi(b,h),b.placeholder=n,zi(b,e,t)}function Zo(e){var t=Ae[e];return function(e,a){if(e=yd(e),(a=null==a?0:va(gd(a),292))&&Lt(e)){var n=(wd(e)+"e").split("e");return+((n=(wd(t(n[0]+"e"+(+n[1]+a)))+"e").split("e"))[0]+"e"+(+n[1]-a))}return t(e)}}var $o=Ta&&1/la(new Ta([,-0]))[1]==f?function(e){return new Ta(e)}:ls;function Qo(e){return function(t){var a=gi(t);return a==A?da(t):a==E?ua(t):function(e,t){return Ht(t,(function(t){return[t,e[t]]}))}(t,e(t))}}function Xo(e,t,n,r,f,h,b,g){var m=2&t;if(!m&&"function"!=typeof e)throw new Ce(i);var y=r?r.length:0;if(y||(t&=-97,r=f=o),b=b===o?b:ya(gd(b),0),g=g===o?g:gd(g),y-=f?f.length:0,t&l){var v=r,w=f;r=f=o}var x=m?o:di(e),k=[e,t,n,r,f,v,w,h,b,g];if(x&&function(e,t){var a=e[1],n=t[1],o=a|n,i=o<131,r=n==u&&8==a||n==u&&a==p&&e[7].length<=t[8]||384==n&&t[7].length<=t[8]&&8==a;if(!i&&!r)return e;1&n&&(e[2]=t[2],o|=1&a?0:4);var s=t[3];if(s){var c=e[3];e[3]=c?Co(c,s,t[4]):s,e[4]=c?ca(e[3],d):t[4]}(s=t[5])&&(c=e[5],e[5]=c?Eo(c,s,t[6]):s,e[6]=c?ca(e[5],d):t[6]);(s=t[7])&&(e[7]=s);n&u&&(e[8]=null==e[8]?t[8]:va(e[8],t[8]));null==e[9]&&(e[9]=t[9]);e[0]=t[0],e[1]=o}(k,x),e=k[0],t=k[1],n=k[2],r=k[3],f=k[4],!(g=k[9]=k[9]===o?m?0:e.length:ya(k[9]-y,0))&&24&t&&(t&=-25),t&&1!=t)R=8==t||t==s?function(e,t,n){var i=Uo(e);return function r(){for(var d=arguments.length,s=a(d),c=d,l=ci(r);c--;)s[c]=arguments[c];var u=d<3&&s[0]!==l&&s[d-1]!==l?[]:ca(s,l);return(d-=u.length)<n?qo(e,t,Wo,r.placeholder,o,s,u,o,o,n-d):At(this&&this!==ft&&this instanceof r?i:e,this,s)}}(e,t,g):t!=c&&33!=t||f.length?Wo.apply(o,k):function(e,t,n,o){var i=1&t,r=Uo(e);return function t(){for(var d=-1,s=arguments.length,c=-1,l=o.length,u=a(l+s),p=this&&this!==ft&&this instanceof t?r:e;++c<l;)u[c]=o[c];for(;s--;)u[c++]=arguments[++d];return At(p,i?n:this,u)}}(e,t,n,r);else var R=function(e,t,a){var n=1&t,o=Uo(e);return function t(){return(this&&this!==ft&&this instanceof t?o:e).apply(n?a:this,arguments)}}(e,t,n);return zi((x?to:Pi)(R,k),e,t)}function ei(e,t,a,n){return e===o||Wr(e,Pe[a])&&!ze.call(n,a)?t:e}function ti(e,t,a,n,i,r){return ad(e)&&ad(t)&&(r.set(t,e),Bn(e,t,o,ti,r),r.delete(t)),e}function ai(e){return rd(e)?o:e}function ni(e,t,a,n,i,r){var d=1&a,s=e.length,c=t.length;if(s!=c&&!(d&&c>s))return!1;var l=r.get(e),u=r.get(t);if(l&&u)return l==t&&u==e;var p=-1,f=!0,h=2&a?new Ya:o;for(r.set(e,t),r.set(t,e);++p<s;){var b=e[p],g=t[p];if(n)var m=d?n(g,b,p,t,e,r):n(b,g,p,e,t,r);if(m!==o){if(m)continue;f=!1;break}if(h){if(!Dt(t,(function(e,t){if(!Xt(h,t)&&(b===e||i(b,e,a,n,r)))return h.push(t)}))){f=!1;break}}else if(b!==g&&!i(b,g,a,n,r)){f=!1;break}}return r.delete(e),r.delete(t),f}function oi(e){return _i(Si(e,o,Ji),e+"")}function ii(e){return An(e,Hd,hi)}function ri(e){return An(e,_d,bi)}var di=Ea?function(e){return Ea.get(e)}:ls;function si(e){for(var t=e.name+"",a=Ma[t],n=ze.call(Ma,t)?a.length:0;n--;){var o=a[n],i=o.func;if(null==i||i==e)return o.name}return t}function ci(e){return(ze.call(Oa,"placeholder")?Oa:e).placeholder}function li(){var e=Oa.iteratee||rs;return e=e===rs?jn:e,arguments.length?e(arguments[0],arguments[1]):e}function ui(e,t){var a=e.__data__;return function(e){var t=typeof e;return"string"==t||"number"==t||"symbol"==t||"boolean"==t?"__proto__"!==e:null===e}(t)?a["string"==typeof t?"string":"hash"]:a.map}function pi(e){for(var t=Hd(e),a=t.length;a--;){var n=t[a],o=e[n];t[a]=[n,o,Fi(o)]}return t}function fi(e,t){var a=function(e,t){return null==e?o:e[t]}(e,t);return zn(a)?a:o}var hi=mt?function(e){return null==e?[]:(e=Fe(e),Et(mt(e),(function(t){return Ye.call(e,t)})))}:ms,bi=mt?function(e){for(var t=[];e;)_t(t,hi(e)),e=Ve(e);return t}:ms,gi=Fn;function mi(e,t,a){for(var n=-1,o=(t=xo(t,e)).length,i=!1;++n<o;){var r=Ui(t[n]);if(!(i=null!=e&&a(e,r)))break;e=e[r]}return i||++n!=o?i:!!(o=null==e?0:e.length)&&td(o)&&wi(r,o)&&(Vr(e)||Gr(e))}function yi(e){return"function"!=typeof e.constructor||Ai(e)?{}:Na(Ve(e))}function vi(e){return Vr(e)||Gr(e)||!!(Ze&&e&&e[Ze])}function wi(e,t){var a=typeof e;return!!(t=null==t?h:t)&&("number"==a||"symbol"!=a&&we.test(e))&&e>-1&&e%1==0&&e<t}function xi(e,t,a){if(!ad(a))return!1;var n=typeof t;return!!("number"==n?Yr(a)&&wi(t,a.length):"string"==n&&t in a)&&Wr(a[t],e)}function ki(e,t){if(Vr(e))return!1;var a=typeof e;return!("number"!=a&&"symbol"!=a&&"boolean"!=a&&null!=e&&!ld(e))||(ae.test(e)||!te.test(e)||null!=t&&e in Fe(t))}function Ri(e){var t=si(e),a=Oa[t];if("function"!=typeof a||!(t in Ka.prototype))return!1;if(e===a)return!0;var n=di(a);return!!n&&e===n[0]}(Ia&&gi(new Ia(new ArrayBuffer(1)))!=z||Aa&&gi(new Aa)!=A||Fa&&gi(Fa.resolve())!=S||Ta&&gi(new Ta)!=E||Sa&&gi(new Sa)!=H)&&(gi=function(e){var t=Fn(e),a=t==T?e.constructor:o,n=a?Oi(a):"";if(n)switch(n){case Pa:return z;case Ha:return A;case _a:return S;case za:return E;case ja:return H}return t});var Ii=He?Xr:ys;function Ai(e){var t=e&&e.constructor;return e===("function"==typeof t&&t.prototype||Pe)}function Fi(e){return e===e&&!ad(e)}function Ti(e,t){return function(a){return null!=a&&(a[e]===t&&(t!==o||e in Fe(a)))}}function Si(e,t,n){return t=ya(t===o?e.length-1:t,0),function(){for(var o=arguments,i=-1,r=ya(o.length-t,0),d=a(r);++i<r;)d[i]=o[t+i];i=-1;for(var s=a(t+1);++i<t;)s[i]=o[i];return s[t]=n(d),At(e,this,s)}}function Ci(e,t){return t.length<2?e:In(e,oo(t,0,-1))}function Ei(e,t){for(var a=e.length,n=va(t.length,a),i=Mo(e);n--;){var r=t[n];e[n]=wi(r,a)?i[r]:o}return e}function Mi(e,t){if(("constructor"!==t||"function"!==typeof e[t])&&"__proto__"!=t)return e[t]}var Pi=ji(to),Hi=pt||function(e,t){return ft.setTimeout(e,t)},_i=ji(ao);function zi(e,t,a){var n=t+"";return _i(e,function(e,t){var a=t.length;if(!a)return e;var n=a-1;return t[n]=(a>1?"& ":"")+t[n],t=t.join(a>2?", ":" "),e.replace(se,"{\n/* [wrapped with "+t+"] */\n")}(n,function(e,t){return Tt(m,(function(a){var n="_."+a[0];t&a[1]&&!Mt(e,n)&&e.push(n)})),e.sort()}(function(e){var t=e.match(ce);return t?t[1].split(le):[]}(n),a)))}function ji(e){var t=0,a=0;return function(){var n=wa(),i=16-(n-a);if(a=n,i>0){if(++t>=800)return arguments[0]}else t=0;return e.apply(o,arguments)}}function Di(e,t){var a=-1,n=e.length,i=n-1;for(t=t===o?n:t;++a<t;){var r=qn(a,i),d=e[r];e[r]=e[a],e[a]=d}return e.length=t,e}var Li=function(e){var t=jr(e,(function(e){return 500===a.size&&a.clear(),e})),a=t.cache;return t}((function(e){var t=[];return 46===e.charCodeAt(0)&&t.push(""),e.replace(ne,(function(e,a,n,o){t.push(n?o.replace(fe,"$1"):a||e)})),t}));function Ui(e){if("string"==typeof e||ld(e))return e;var t=e+"";return"0"==t&&1/e==-1/0?"-0":t}function Oi(e){if(null!=e){try{return _e.call(e)}catch(t){}try{return e+""}catch(t){}}return""}function Ni(e){if(e instanceof Ka)return e.clone();var t=new Ba(e.__wrapped__,e.__chain__);return t.__actions__=Mo(e.__actions__),t.__index__=e.__index__,t.__values__=e.__values__,t}var Wi=$n((function(e,t){return qr(e)?pn(e,yn(t,1,qr,!0)):[]})),Bi=$n((function(e,t){var a=Qi(t);return qr(a)&&(a=o),qr(e)?pn(e,yn(t,1,qr,!0),li(a,2)):[]})),Ki=$n((function(e,t){var a=Qi(t);return qr(a)&&(a=o),qr(e)?pn(e,yn(t,1,qr,!0),o,a):[]}));function Gi(e,t,a){var n=null==e?0:e.length;if(!n)return-1;var o=null==a?0:gd(a);return o<0&&(o=ya(n+o,0)),Ot(e,li(t,3),o)}function Vi(e,t,a){var n=null==e?0:e.length;if(!n)return-1;var i=n-1;return a!==o&&(i=gd(a),i=a<0?ya(n+i,0):va(i,n-1)),Ot(e,li(t,3),i,!0)}function Ji(e){return(null==e?0:e.length)?yn(e,1):[]}function Yi(e){return e&&e.length?e[0]:o}var qi=$n((function(e){var t=Ht(e,vo);return t.length&&t[0]===e[0]?En(t):[]})),Zi=$n((function(e){var t=Qi(e),a=Ht(e,vo);return t===Qi(a)?t=o:a.pop(),a.length&&a[0]===e[0]?En(a,li(t,2)):[]})),$i=$n((function(e){var t=Qi(e),a=Ht(e,vo);return(t="function"==typeof t?t:o)&&a.pop(),a.length&&a[0]===e[0]?En(a,o,t):[]}));function Qi(e){var t=null==e?0:e.length;return t?e[t-1]:o}var Xi=$n(er);function er(e,t){return e&&e.length&&t&&t.length?Jn(e,t):e}var tr=oi((function(e,t){var a=null==e?0:e.length,n=dn(e,t);return Yn(e,Ht(t,(function(e){return wi(e,a)?+e:e})).sort(So)),n}));function ar(e){return null==e?e:Ra.call(e)}var nr=$n((function(e){return po(yn(e,1,qr,!0))})),or=$n((function(e){var t=Qi(e);return qr(t)&&(t=o),po(yn(e,1,qr,!0),li(t,2))})),ir=$n((function(e){var t=Qi(e);return t="function"==typeof t?t:o,po(yn(e,1,qr,!0),o,t)}));function rr(e){if(!e||!e.length)return[];var t=0;return e=Et(e,(function(e){if(qr(e))return t=ya(e.length,t),!0})),qt(t,(function(t){return Ht(e,Gt(t))}))}function dr(e,t){if(!e||!e.length)return[];var a=rr(e);return null==t?a:Ht(a,(function(e){return At(t,o,e)}))}var sr=$n((function(e,t){return qr(e)?pn(e,t):[]})),cr=$n((function(e){return mo(Et(e,qr))})),lr=$n((function(e){var t=Qi(e);return qr(t)&&(t=o),mo(Et(e,qr),li(t,2))})),ur=$n((function(e){var t=Qi(e);return t="function"==typeof t?t:o,mo(Et(e,qr),o,t)})),pr=$n(rr);var fr=$n((function(e){var t=e.length,a=t>1?e[t-1]:o;return a="function"==typeof a?(e.pop(),a):o,dr(e,a)}));function hr(e){var t=Oa(e);return t.__chain__=!0,t}function br(e,t){return t(e)}var gr=oi((function(e){var t=e.length,a=t?e[0]:0,n=this.__wrapped__,i=function(t){return dn(t,e)};return!(t>1||this.__actions__.length)&&n instanceof Ka&&wi(a)?((n=n.slice(a,+a+(t?1:0))).__actions__.push({func:br,args:[i],thisArg:o}),new Ba(n,this.__chain__).thru((function(e){return t&&!e.length&&e.push(o),e}))):this.thru(i)}));var mr=Ho((function(e,t,a){ze.call(e,a)?++e[a]:rn(e,a,1)}));var yr=Oo(Gi),vr=Oo(Vi);function wr(e,t){return(Vr(e)?Tt:fn)(e,li(t,3))}function xr(e,t){return(Vr(e)?St:hn)(e,li(t,3))}var kr=Ho((function(e,t,a){ze.call(e,a)?e[a].push(t):rn(e,a,[t])}));var Rr=$n((function(e,t,n){var o=-1,i="function"==typeof t,r=Yr(e)?a(e.length):[];return fn(e,(function(e){r[++o]=i?At(t,e,n):Mn(e,t,n)})),r})),Ir=Ho((function(e,t,a){rn(e,a,t)}));function Ar(e,t){return(Vr(e)?Ht:On)(e,li(t,3))}var Fr=Ho((function(e,t,a){e[a?0:1].push(t)}),(function(){return[[],[]]}));var Tr=$n((function(e,t){if(null==e)return[];var a=t.length;return a>1&&xi(e,t[0],t[1])?t=[]:a>2&&xi(t[0],t[1],t[2])&&(t=[t[0]]),Gn(e,yn(t,1),[])})),Sr=ut||function(){return ft.Date.now()};function Cr(e,t,a){return t=a?o:t,t=e&&null==t?e.length:t,Xo(e,u,o,o,o,o,t)}function Er(e,t){var a;if("function"!=typeof t)throw new Ce(i);return e=gd(e),function(){return--e>0&&(a=t.apply(this,arguments)),e<=1&&(t=o),a}}var Mr=$n((function(e,t,a){var n=1;if(a.length){var o=ca(a,ci(Mr));n|=c}return Xo(e,n,t,a,o)})),Pr=$n((function(e,t,a){var n=3;if(a.length){var o=ca(a,ci(Pr));n|=c}return Xo(t,n,e,a,o)}));function Hr(e,t,a){var n,r,d,s,c,l,u=0,p=!1,f=!1,h=!0;if("function"!=typeof e)throw new Ce(i);function b(t){var a=n,i=r;return n=r=o,u=t,s=e.apply(i,a)}function g(e){return u=e,c=Hi(y,t),p?b(e):s}function m(e){var a=e-l;return l===o||a>=t||a<0||f&&e-u>=d}function y(){var e=Sr();if(m(e))return v(e);c=Hi(y,function(e){var a=t-(e-l);return f?va(a,d-(e-u)):a}(e))}function v(e){return c=o,h&&n?b(e):(n=r=o,s)}function w(){var e=Sr(),a=m(e);if(n=arguments,r=this,l=e,a){if(c===o)return g(l);if(f)return Io(c),c=Hi(y,t),b(l)}return c===o&&(c=Hi(y,t)),s}return t=yd(t)||0,ad(a)&&(p=!!a.leading,d=(f="maxWait"in a)?ya(yd(a.maxWait)||0,t):d,h="trailing"in a?!!a.trailing:h),w.cancel=function(){c!==o&&Io(c),u=0,n=l=r=c=o},w.flush=function(){return c===o?s:v(Sr())},w}var _r=$n((function(e,t){return un(e,1,t)})),zr=$n((function(e,t,a){return un(e,yd(t)||0,a)}));function jr(e,t){if("function"!=typeof e||null!=t&&"function"!=typeof t)throw new Ce(i);var a=function a(){var n=arguments,o=t?t.apply(this,n):n[0],i=a.cache;if(i.has(o))return i.get(o);var r=e.apply(this,n);return a.cache=i.set(o,r)||i,r};return a.cache=new(jr.Cache||Ja),a}function Dr(e){if("function"!=typeof e)throw new Ce(i);return function(){var t=arguments;switch(t.length){case 0:return!e.call(this);case 1:return!e.call(this,t[0]);case 2:return!e.call(this,t[0],t[1]);case 3:return!e.call(this,t[0],t[1],t[2])}return!e.apply(this,t)}}jr.Cache=Ja;var Lr=ko((function(e,t){var a=(t=1==t.length&&Vr(t[0])?Ht(t[0],$t(li())):Ht(yn(t,1),$t(li()))).length;return $n((function(n){for(var o=-1,i=va(n.length,a);++o<i;)n[o]=t[o].call(this,n[o]);return At(e,this,n)}))})),Ur=$n((function(e,t){var a=ca(t,ci(Ur));return Xo(e,c,o,t,a)})),Or=$n((function(e,t){var a=ca(t,ci(Or));return Xo(e,l,o,t,a)})),Nr=oi((function(e,t){return Xo(e,p,o,o,o,t)}));function Wr(e,t){return e===t||e!==e&&t!==t}var Br=Yo(Tn),Kr=Yo((function(e,t){return e>=t})),Gr=Pn(function(){return arguments}())?Pn:function(e){return nd(e)&&ze.call(e,"callee")&&!Ye.call(e,"callee")},Vr=a.isArray,Jr=vt?$t(vt):function(e){return nd(e)&&Fn(e)==_};function Yr(e){return null!=e&&td(e.length)&&!Xr(e)}function qr(e){return nd(e)&&Yr(e)}var Zr=yt||ys,$r=wt?$t(wt):function(e){return nd(e)&&Fn(e)==x};function Qr(e){if(!nd(e))return!1;var t=Fn(e);return t==k||"[object DOMException]"==t||"string"==typeof e.message&&"string"==typeof e.name&&!rd(e)}function Xr(e){if(!ad(e))return!1;var t=Fn(e);return t==R||t==I||"[object AsyncFunction]"==t||"[object Proxy]"==t}function ed(e){return"number"==typeof e&&e==gd(e)}function td(e){return"number"==typeof e&&e>-1&&e%1==0&&e<=h}function ad(e){var t=typeof e;return null!=e&&("object"==t||"function"==t)}function nd(e){return null!=e&&"object"==typeof e}var od=xt?$t(xt):function(e){return nd(e)&&gi(e)==A};function id(e){return"number"==typeof e||nd(e)&&Fn(e)==F}function rd(e){if(!nd(e)||Fn(e)!=T)return!1;var t=Ve(e);if(null===t)return!0;var a=ze.call(t,"constructor")&&t.constructor;return"function"==typeof a&&a instanceof a&&_e.call(a)==Ue}var dd=kt?$t(kt):function(e){return nd(e)&&Fn(e)==C};var sd=Rt?$t(Rt):function(e){return nd(e)&&gi(e)==E};function cd(e){return"string"==typeof e||!Vr(e)&&nd(e)&&Fn(e)==M}function ld(e){return"symbol"==typeof e||nd(e)&&Fn(e)==P}var ud=It?$t(It):function(e){return nd(e)&&td(e.length)&&!!rt[Fn(e)]};var pd=Yo(Un),fd=Yo((function(e,t){return e<=t}));function hd(e){if(!e)return[];if(Yr(e))return cd(e)?fa(e):Mo(e);if($e&&e[$e])return function(e){for(var t,a=[];!(t=e.next()).done;)a.push(t.value);return a}(e[$e]());var t=gi(e);return(t==A?da:t==E?la:Wd)(e)}function bd(e){return e?(e=yd(e))===f||e===-1/0?17976931348623157e292*(e<0?-1:1):e===e?e:0:0===e?e:0}function gd(e){var t=bd(e),a=t%1;return t===t?a?t-a:t:0}function md(e){return e?sn(gd(e),0,g):0}function yd(e){if("number"==typeof e)return e;if(ld(e))return b;if(ad(e)){var t="function"==typeof e.valueOf?e.valueOf():e;e=ad(t)?t+"":t}if("string"!=typeof e)return 0===e?e:+e;e=Zt(e);var a=me.test(e);return a||ve.test(e)?lt(e.slice(2),a?2:8):ge.test(e)?b:+e}function vd(e){return Po(e,_d(e))}function wd(e){return null==e?"":uo(e)}var xd=_o((function(e,t){if(Ai(t)||Yr(t))Po(t,Hd(t),e);else for(var a in t)ze.call(t,a)&&tn(e,a,t[a])})),kd=_o((function(e,t){Po(t,_d(t),e)})),Rd=_o((function(e,t,a,n){Po(t,_d(t),e,n)})),Id=_o((function(e,t,a,n){Po(t,Hd(t),e,n)})),Ad=oi(dn);var Fd=$n((function(e,t){e=Fe(e);var a=-1,n=t.length,i=n>2?t[2]:o;for(i&&xi(t[0],t[1],i)&&(n=1);++a<n;)for(var r=t[a],d=_d(r),s=-1,c=d.length;++s<c;){var l=d[s],u=e[l];(u===o||Wr(u,Pe[l])&&!ze.call(e,l))&&(e[l]=r[l])}return e})),Td=$n((function(e){return e.push(o,ti),At(jd,o,e)}));function Sd(e,t,a){var n=null==e?o:In(e,t);return n===o?a:n}function Cd(e,t){return null!=e&&mi(e,t,Cn)}var Ed=Bo((function(e,t,a){null!=t&&"function"!=typeof t.toString&&(t=Le.call(t)),e[t]=a}),as(is)),Md=Bo((function(e,t,a){null!=t&&"function"!=typeof t.toString&&(t=Le.call(t)),ze.call(e,t)?e[t].push(a):e[t]=[a]}),li),Pd=$n(Mn);function Hd(e){return Yr(e)?Za(e):Dn(e)}function _d(e){return Yr(e)?Za(e,!0):Ln(e)}var zd=_o((function(e,t,a){Bn(e,t,a)})),jd=_o((function(e,t,a,n){Bn(e,t,a,n)})),Dd=oi((function(e,t){var a={};if(null==e)return a;var n=!1;t=Ht(t,(function(t){return t=xo(t,e),n||(n=t.length>1),t})),Po(e,ri(e),a),n&&(a=cn(a,7,ai));for(var o=t.length;o--;)fo(a,t[o]);return a}));var Ld=oi((function(e,t){return null==e?{}:function(e,t){return Vn(e,t,(function(t,a){return Cd(e,a)}))}(e,t)}));function Ud(e,t){if(null==e)return{};var a=Ht(ri(e),(function(e){return[e]}));return t=li(t),Vn(e,a,(function(e,a){return t(e,a[0])}))}var Od=Qo(Hd),Nd=Qo(_d);function Wd(e){return null==e?[]:Qt(e,Hd(e))}var Bd=Lo((function(e,t,a){return t=t.toLowerCase(),e+(a?Kd(t):t)}));function Kd(e){return Qd(wd(e).toLowerCase())}function Gd(e){return(e=wd(e))&&e.replace(xe,na).replace(Xe,"")}var Vd=Lo((function(e,t,a){return e+(a?"-":"")+t.toLowerCase()})),Jd=Lo((function(e,t,a){return e+(a?" ":"")+t.toLowerCase()})),Yd=Do("toLowerCase");var qd=Lo((function(e,t,a){return e+(a?"_":"")+t.toLowerCase()}));var Zd=Lo((function(e,t,a){return e+(a?" ":"")+Qd(t)}));var $d=Lo((function(e,t,a){return e+(a?" ":"")+t.toUpperCase()})),Qd=Do("toUpperCase");function Xd(e,t,a){return e=wd(e),(t=a?o:t)===o?function(e){return nt.test(e)}(e)?function(e){return e.match(tt)||[]}(e):function(e){return e.match(ue)||[]}(e):e.match(t)||[]}var es=$n((function(e,t){try{return At(e,o,t)}catch(a){return Qr(a)?a:new de(a)}})),ts=oi((function(e,t){return Tt(t,(function(t){t=Ui(t),rn(e,t,Mr(e[t],e))})),e}));function as(e){return function(){return e}}var ns=No(),os=No(!0);function is(e){return e}function rs(e){return jn("function"==typeof e?e:cn(e,1))}var ds=$n((function(e,t){return function(a){return Mn(a,e,t)}})),ss=$n((function(e,t){return function(a){return Mn(e,a,t)}}));function cs(e,t,a){var n=Hd(t),o=Rn(t,n);null!=a||ad(t)&&(o.length||!n.length)||(a=t,t=e,e=this,o=Rn(t,Hd(t)));var i=!(ad(a)&&"chain"in a)||!!a.chain,r=Xr(e);return Tt(o,(function(a){var n=t[a];e[a]=n,r&&(e.prototype[a]=function(){var t=this.__chain__;if(i||t){var a=e(this.__wrapped__),o=a.__actions__=Mo(this.__actions__);return o.push({func:n,args:arguments,thisArg:e}),a.__chain__=t,a}return n.apply(e,_t([this.value()],arguments))})})),e}function ls(){}var us=Go(Ht),ps=Go(Ct),fs=Go(Dt);function hs(e){return ki(e)?Gt(Ui(e)):function(e){return function(t){return In(t,e)}}(e)}var bs=Jo(),gs=Jo(!0);function ms(){return[]}function ys(){return!1}var vs=Ko((function(e,t){return e+t}),0),ws=Zo("ceil"),xs=Ko((function(e,t){return e/t}),1),ks=Zo("floor");var Rs=Ko((function(e,t){return e*t}),1),Is=Zo("round"),As=Ko((function(e,t){return e-t}),0);return Oa.after=function(e,t){if("function"!=typeof t)throw new Ce(i);return e=gd(e),function(){if(--e<1)return t.apply(this,arguments)}},Oa.ary=Cr,Oa.assign=xd,Oa.assignIn=kd,Oa.assignInWith=Rd,Oa.assignWith=Id,Oa.at=Ad,Oa.before=Er,Oa.bind=Mr,Oa.bindAll=ts,Oa.bindKey=Pr,Oa.castArray=function(){if(!arguments.length)return[];var e=arguments[0];return Vr(e)?e:[e]},Oa.chain=hr,Oa.chunk=function(e,t,n){t=(n?xi(e,t,n):t===o)?1:ya(gd(t),0);var i=null==e?0:e.length;if(!i||t<1)return[];for(var r=0,d=0,s=a(ht(i/t));r<i;)s[d++]=oo(e,r,r+=t);return s},Oa.compact=function(e){for(var t=-1,a=null==e?0:e.length,n=0,o=[];++t<a;){var i=e[t];i&&(o[n++]=i)}return o},Oa.concat=function(){var e=arguments.length;if(!e)return[];for(var t=a(e-1),n=arguments[0],o=e;o--;)t[o-1]=arguments[o];return _t(Vr(n)?Mo(n):[n],yn(t,1))},Oa.cond=function(e){var t=null==e?0:e.length,a=li();return e=t?Ht(e,(function(e){if("function"!=typeof e[1])throw new Ce(i);return[a(e[0]),e[1]]})):[],$n((function(a){for(var n=-1;++n<t;){var o=e[n];if(At(o[0],this,a))return At(o[1],this,a)}}))},Oa.conforms=function(e){return function(e){var t=Hd(e);return function(a){return ln(a,e,t)}}(cn(e,1))},Oa.constant=as,Oa.countBy=mr,Oa.create=function(e,t){var a=Na(e);return null==t?a:on(a,t)},Oa.curry=function e(t,a,n){var i=Xo(t,8,o,o,o,o,o,a=n?o:a);return i.placeholder=e.placeholder,i},Oa.curryRight=function e(t,a,n){var i=Xo(t,s,o,o,o,o,o,a=n?o:a);return i.placeholder=e.placeholder,i},Oa.debounce=Hr,Oa.defaults=Fd,Oa.defaultsDeep=Td,Oa.defer=_r,Oa.delay=zr,Oa.difference=Wi,Oa.differenceBy=Bi,Oa.differenceWith=Ki,Oa.drop=function(e,t,a){var n=null==e?0:e.length;return n?oo(e,(t=a||t===o?1:gd(t))<0?0:t,n):[]},Oa.dropRight=function(e,t,a){var n=null==e?0:e.length;return n?oo(e,0,(t=n-(t=a||t===o?1:gd(t)))<0?0:t):[]},Oa.dropRightWhile=function(e,t){return e&&e.length?bo(e,li(t,3),!0,!0):[]},Oa.dropWhile=function(e,t){return e&&e.length?bo(e,li(t,3),!0):[]},Oa.fill=function(e,t,a,n){var i=null==e?0:e.length;return i?(a&&"number"!=typeof a&&xi(e,t,a)&&(a=0,n=i),function(e,t,a,n){var i=e.length;for((a=gd(a))<0&&(a=-a>i?0:i+a),(n=n===o||n>i?i:gd(n))<0&&(n+=i),n=a>n?0:md(n);a<n;)e[a++]=t;return e}(e,t,a,n)):[]},Oa.filter=function(e,t){return(Vr(e)?Et:mn)(e,li(t,3))},Oa.flatMap=function(e,t){return yn(Ar(e,t),1)},Oa.flatMapDeep=function(e,t){return yn(Ar(e,t),f)},Oa.flatMapDepth=function(e,t,a){return a=a===o?1:gd(a),yn(Ar(e,t),a)},Oa.flatten=Ji,Oa.flattenDeep=function(e){return(null==e?0:e.length)?yn(e,f):[]},Oa.flattenDepth=function(e,t){return(null==e?0:e.length)?yn(e,t=t===o?1:gd(t)):[]},Oa.flip=function(e){return Xo(e,512)},Oa.flow=ns,Oa.flowRight=os,Oa.fromPairs=function(e){for(var t=-1,a=null==e?0:e.length,n={};++t<a;){var o=e[t];n[o[0]]=o[1]}return n},Oa.functions=function(e){return null==e?[]:Rn(e,Hd(e))},Oa.functionsIn=function(e){return null==e?[]:Rn(e,_d(e))},Oa.groupBy=kr,Oa.initial=function(e){return(null==e?0:e.length)?oo(e,0,-1):[]},Oa.intersection=qi,Oa.intersectionBy=Zi,Oa.intersectionWith=$i,Oa.invert=Ed,Oa.invertBy=Md,Oa.invokeMap=Rr,Oa.iteratee=rs,Oa.keyBy=Ir,Oa.keys=Hd,Oa.keysIn=_d,Oa.map=Ar,Oa.mapKeys=function(e,t){var a={};return t=li(t,3),xn(e,(function(e,n,o){rn(a,t(e,n,o),e)})),a},Oa.mapValues=function(e,t){var a={};return t=li(t,3),xn(e,(function(e,n,o){rn(a,n,t(e,n,o))})),a},Oa.matches=function(e){return Nn(cn(e,1))},Oa.matchesProperty=function(e,t){return Wn(e,cn(t,1))},Oa.memoize=jr,Oa.merge=zd,Oa.mergeWith=jd,Oa.method=ds,Oa.methodOf=ss,Oa.mixin=cs,Oa.negate=Dr,Oa.nthArg=function(e){return e=gd(e),$n((function(t){return Kn(t,e)}))},Oa.omit=Dd,Oa.omitBy=function(e,t){return Ud(e,Dr(li(t)))},Oa.once=function(e){return Er(2,e)},Oa.orderBy=function(e,t,a,n){return null==e?[]:(Vr(t)||(t=null==t?[]:[t]),Vr(a=n?o:a)||(a=null==a?[]:[a]),Gn(e,t,a))},Oa.over=us,Oa.overArgs=Lr,Oa.overEvery=ps,Oa.overSome=fs,Oa.partial=Ur,Oa.partialRight=Or,Oa.partition=Fr,Oa.pick=Ld,Oa.pickBy=Ud,Oa.property=hs,Oa.propertyOf=function(e){return function(t){return null==e?o:In(e,t)}},Oa.pull=Xi,Oa.pullAll=er,Oa.pullAllBy=function(e,t,a){return e&&e.length&&t&&t.length?Jn(e,t,li(a,2)):e},Oa.pullAllWith=function(e,t,a){return e&&e.length&&t&&t.length?Jn(e,t,o,a):e},Oa.pullAt=tr,Oa.range=bs,Oa.rangeRight=gs,Oa.rearg=Nr,Oa.reject=function(e,t){return(Vr(e)?Et:mn)(e,Dr(li(t,3)))},Oa.remove=function(e,t){var a=[];if(!e||!e.length)return a;var n=-1,o=[],i=e.length;for(t=li(t,3);++n<i;){var r=e[n];t(r,n,e)&&(a.push(r),o.push(n))}return Yn(e,o),a},Oa.rest=function(e,t){if("function"!=typeof e)throw new Ce(i);return $n(e,t=t===o?t:gd(t))},Oa.reverse=ar,Oa.sampleSize=function(e,t,a){return t=(a?xi(e,t,a):t===o)?1:gd(t),(Vr(e)?Qa:Xn)(e,t)},Oa.set=function(e,t,a){return null==e?e:eo(e,t,a)},Oa.setWith=function(e,t,a,n){return n="function"==typeof n?n:o,null==e?e:eo(e,t,a,n)},Oa.shuffle=function(e){return(Vr(e)?Xa:no)(e)},Oa.slice=function(e,t,a){var n=null==e?0:e.length;return n?(a&&"number"!=typeof a&&xi(e,t,a)?(t=0,a=n):(t=null==t?0:gd(t),a=a===o?n:gd(a)),oo(e,t,a)):[]},Oa.sortBy=Tr,Oa.sortedUniq=function(e){return e&&e.length?co(e):[]},Oa.sortedUniqBy=function(e,t){return e&&e.length?co(e,li(t,2)):[]},Oa.split=function(e,t,a){return a&&"number"!=typeof a&&xi(e,t,a)&&(t=a=o),(a=a===o?g:a>>>0)?(e=wd(e))&&("string"==typeof t||null!=t&&!dd(t))&&!(t=uo(t))&&ra(e)?Ro(fa(e),0,a):e.split(t,a):[]},Oa.spread=function(e,t){if("function"!=typeof e)throw new Ce(i);return t=null==t?0:ya(gd(t),0),$n((function(a){var n=a[t],o=Ro(a,0,t);return n&&_t(o,n),At(e,this,o)}))},Oa.tail=function(e){var t=null==e?0:e.length;return t?oo(e,1,t):[]},Oa.take=function(e,t,a){return e&&e.length?oo(e,0,(t=a||t===o?1:gd(t))<0?0:t):[]},Oa.takeRight=function(e,t,a){var n=null==e?0:e.length;return n?oo(e,(t=n-(t=a||t===o?1:gd(t)))<0?0:t,n):[]},Oa.takeRightWhile=function(e,t){return e&&e.length?bo(e,li(t,3),!1,!0):[]},Oa.takeWhile=function(e,t){return e&&e.length?bo(e,li(t,3)):[]},Oa.tap=function(e,t){return t(e),e},Oa.throttle=function(e,t,a){var n=!0,o=!0;if("function"!=typeof e)throw new Ce(i);return ad(a)&&(n="leading"in a?!!a.leading:n,o="trailing"in a?!!a.trailing:o),Hr(e,t,{leading:n,maxWait:t,trailing:o})},Oa.thru=br,Oa.toArray=hd,Oa.toPairs=Od,Oa.toPairsIn=Nd,Oa.toPath=function(e){return Vr(e)?Ht(e,Ui):ld(e)?[e]:Mo(Li(wd(e)))},Oa.toPlainObject=vd,Oa.transform=function(e,t,a){var n=Vr(e),o=n||Zr(e)||ud(e);if(t=li(t,4),null==a){var i=e&&e.constructor;a=o?n?new i:[]:ad(e)&&Xr(i)?Na(Ve(e)):{}}return(o?Tt:xn)(e,(function(e,n,o){return t(a,e,n,o)})),a},Oa.unary=function(e){return Cr(e,1)},Oa.union=nr,Oa.unionBy=or,Oa.unionWith=ir,Oa.uniq=function(e){return e&&e.length?po(e):[]},Oa.uniqBy=function(e,t){return e&&e.length?po(e,li(t,2)):[]},Oa.uniqWith=function(e,t){return t="function"==typeof t?t:o,e&&e.length?po(e,o,t):[]},Oa.unset=function(e,t){return null==e||fo(e,t)},Oa.unzip=rr,Oa.unzipWith=dr,Oa.update=function(e,t,a){return null==e?e:ho(e,t,wo(a))},Oa.updateWith=function(e,t,a,n){return n="function"==typeof n?n:o,null==e?e:ho(e,t,wo(a),n)},Oa.values=Wd,Oa.valuesIn=function(e){return null==e?[]:Qt(e,_d(e))},Oa.without=sr,Oa.words=Xd,Oa.wrap=function(e,t){return Ur(wo(t),e)},Oa.xor=cr,Oa.xorBy=lr,Oa.xorWith=ur,Oa.zip=pr,Oa.zipObject=function(e,t){return yo(e||[],t||[],tn)},Oa.zipObjectDeep=function(e,t){return yo(e||[],t||[],eo)},Oa.zipWith=fr,Oa.entries=Od,Oa.entriesIn=Nd,Oa.extend=kd,Oa.extendWith=Rd,cs(Oa,Oa),Oa.add=vs,Oa.attempt=es,Oa.camelCase=Bd,Oa.capitalize=Kd,Oa.ceil=ws,Oa.clamp=function(e,t,a){return a===o&&(a=t,t=o),a!==o&&(a=(a=yd(a))===a?a:0),t!==o&&(t=(t=yd(t))===t?t:0),sn(yd(e),t,a)},Oa.clone=function(e){return cn(e,4)},Oa.cloneDeep=function(e){return cn(e,5)},Oa.cloneDeepWith=function(e,t){return cn(e,5,t="function"==typeof t?t:o)},Oa.cloneWith=function(e,t){return cn(e,4,t="function"==typeof t?t:o)},Oa.conformsTo=function(e,t){return null==t||ln(e,t,Hd(t))},Oa.deburr=Gd,Oa.defaultTo=function(e,t){return null==e||e!==e?t:e},Oa.divide=xs,Oa.endsWith=function(e,t,a){e=wd(e),t=uo(t);var n=e.length,i=a=a===o?n:sn(gd(a),0,n);return(a-=t.length)>=0&&e.slice(a,i)==t},Oa.eq=Wr,Oa.escape=function(e){return(e=wd(e))&&$.test(e)?e.replace(q,oa):e},Oa.escapeRegExp=function(e){return(e=wd(e))&&ie.test(e)?e.replace(oe,"\\$&"):e},Oa.every=function(e,t,a){var n=Vr(e)?Ct:bn;return a&&xi(e,t,a)&&(t=o),n(e,li(t,3))},Oa.find=yr,Oa.findIndex=Gi,Oa.findKey=function(e,t){return Ut(e,li(t,3),xn)},Oa.findLast=vr,Oa.findLastIndex=Vi,Oa.findLastKey=function(e,t){return Ut(e,li(t,3),kn)},Oa.floor=ks,Oa.forEach=wr,Oa.forEachRight=xr,Oa.forIn=function(e,t){return null==e?e:vn(e,li(t,3),_d)},Oa.forInRight=function(e,t){return null==e?e:wn(e,li(t,3),_d)},Oa.forOwn=function(e,t){return e&&xn(e,li(t,3))},Oa.forOwnRight=function(e,t){return e&&kn(e,li(t,3))},Oa.get=Sd,Oa.gt=Br,Oa.gte=Kr,Oa.has=function(e,t){return null!=e&&mi(e,t,Sn)},Oa.hasIn=Cd,Oa.head=Yi,Oa.identity=is,Oa.includes=function(e,t,a,n){e=Yr(e)?e:Wd(e),a=a&&!n?gd(a):0;var o=e.length;return a<0&&(a=ya(o+a,0)),cd(e)?a<=o&&e.indexOf(t,a)>-1:!!o&&Nt(e,t,a)>-1},Oa.indexOf=function(e,t,a){var n=null==e?0:e.length;if(!n)return-1;var o=null==a?0:gd(a);return o<0&&(o=ya(n+o,0)),Nt(e,t,o)},Oa.inRange=function(e,t,a){return t=bd(t),a===o?(a=t,t=0):a=bd(a),function(e,t,a){return e>=va(t,a)&&e<ya(t,a)}(e=yd(e),t,a)},Oa.invoke=Pd,Oa.isArguments=Gr,Oa.isArray=Vr,Oa.isArrayBuffer=Jr,Oa.isArrayLike=Yr,Oa.isArrayLikeObject=qr,Oa.isBoolean=function(e){return!0===e||!1===e||nd(e)&&Fn(e)==w},Oa.isBuffer=Zr,Oa.isDate=$r,Oa.isElement=function(e){return nd(e)&&1===e.nodeType&&!rd(e)},Oa.isEmpty=function(e){if(null==e)return!0;if(Yr(e)&&(Vr(e)||"string"==typeof e||"function"==typeof e.splice||Zr(e)||ud(e)||Gr(e)))return!e.length;var t=gi(e);if(t==A||t==E)return!e.size;if(Ai(e))return!Dn(e).length;for(var a in e)if(ze.call(e,a))return!1;return!0},Oa.isEqual=function(e,t){return Hn(e,t)},Oa.isEqualWith=function(e,t,a){var n=(a="function"==typeof a?a:o)?a(e,t):o;return n===o?Hn(e,t,o,a):!!n},Oa.isError=Qr,Oa.isFinite=function(e){return"number"==typeof e&&Lt(e)},Oa.isFunction=Xr,Oa.isInteger=ed,Oa.isLength=td,Oa.isMap=od,Oa.isMatch=function(e,t){return e===t||_n(e,t,pi(t))},Oa.isMatchWith=function(e,t,a){return a="function"==typeof a?a:o,_n(e,t,pi(t),a)},Oa.isNaN=function(e){return id(e)&&e!=+e},Oa.isNative=function(e){if(Ii(e))throw new de("Unsupported core-js use. Try https://npms.io/search?q=ponyfill.");return zn(e)},Oa.isNil=function(e){return null==e},Oa.isNull=function(e){return null===e},Oa.isNumber=id,Oa.isObject=ad,Oa.isObjectLike=nd,Oa.isPlainObject=rd,Oa.isRegExp=dd,Oa.isSafeInteger=function(e){return ed(e)&&e>=-9007199254740991&&e<=h},Oa.isSet=sd,Oa.isString=cd,Oa.isSymbol=ld,Oa.isTypedArray=ud,Oa.isUndefined=function(e){return e===o},Oa.isWeakMap=function(e){return nd(e)&&gi(e)==H},Oa.isWeakSet=function(e){return nd(e)&&"[object WeakSet]"==Fn(e)},Oa.join=function(e,t){return null==e?"":Vt.call(e,t)},Oa.kebabCase=Vd,Oa.last=Qi,Oa.lastIndexOf=function(e,t,a){var n=null==e?0:e.length;if(!n)return-1;var i=n;return a!==o&&(i=(i=gd(a))<0?ya(n+i,0):va(i,n-1)),t===t?function(e,t,a){for(var n=a+1;n--;)if(e[n]===t)return n;return n}(e,t,i):Ot(e,Bt,i,!0)},Oa.lowerCase=Jd,Oa.lowerFirst=Yd,Oa.lt=pd,Oa.lte=fd,Oa.max=function(e){return e&&e.length?gn(e,is,Tn):o},Oa.maxBy=function(e,t){return e&&e.length?gn(e,li(t,2),Tn):o},Oa.mean=function(e){return Kt(e,is)},Oa.meanBy=function(e,t){return Kt(e,li(t,2))},Oa.min=function(e){return e&&e.length?gn(e,is,Un):o},Oa.minBy=function(e,t){return e&&e.length?gn(e,li(t,2),Un):o},Oa.stubArray=ms,Oa.stubFalse=ys,Oa.stubObject=function(){return{}},Oa.stubString=function(){return""},Oa.stubTrue=function(){return!0},Oa.multiply=Rs,Oa.nth=function(e,t){return e&&e.length?Kn(e,gd(t)):o},Oa.noConflict=function(){return ft._===this&&(ft._=Oe),this},Oa.noop=ls,Oa.now=Sr,Oa.pad=function(e,t,a){e=wd(e);var n=(t=gd(t))?pa(e):0;if(!t||n>=t)return e;var o=(t-n)/2;return Vo(bt(o),a)+e+Vo(ht(o),a)},Oa.padEnd=function(e,t,a){e=wd(e);var n=(t=gd(t))?pa(e):0;return t&&n<t?e+Vo(t-n,a):e},Oa.padStart=function(e,t,a){e=wd(e);var n=(t=gd(t))?pa(e):0;return t&&n<t?Vo(t-n,a)+e:e},Oa.parseInt=function(e,t,a){return a||null==t?t=0:t&&(t=+t),xa(wd(e).replace(re,""),t||0)},Oa.random=function(e,t,a){if(a&&"boolean"!=typeof a&&xi(e,t,a)&&(t=a=o),a===o&&("boolean"==typeof t?(a=t,t=o):"boolean"==typeof e&&(a=e,e=o)),e===o&&t===o?(e=0,t=1):(e=bd(e),t===o?(t=e,e=0):t=bd(t)),e>t){var n=e;e=t,t=n}if(a||e%1||t%1){var i=ka();return va(e+i*(t-e+ct("1e-"+((i+"").length-1))),t)}return qn(e,t)},Oa.reduce=function(e,t,a){var n=Vr(e)?zt:Jt,o=arguments.length<3;return n(e,li(t,4),a,o,fn)},Oa.reduceRight=function(e,t,a){var n=Vr(e)?jt:Jt,o=arguments.length<3;return n(e,li(t,4),a,o,hn)},Oa.repeat=function(e,t,a){return t=(a?xi(e,t,a):t===o)?1:gd(t),Zn(wd(e),t)},Oa.replace=function(){var e=arguments,t=wd(e[0]);return e.length<3?t:t.replace(e[1],e[2])},Oa.result=function(e,t,a){var n=-1,i=(t=xo(t,e)).length;for(i||(i=1,e=o);++n<i;){var r=null==e?o:e[Ui(t[n])];r===o&&(n=i,r=a),e=Xr(r)?r.call(e):r}return e},Oa.round=Is,Oa.runInContext=e,Oa.sample=function(e){return(Vr(e)?$a:Qn)(e)},Oa.size=function(e){if(null==e)return 0;if(Yr(e))return cd(e)?pa(e):e.length;var t=gi(e);return t==A||t==E?e.size:Dn(e).length},Oa.snakeCase=qd,Oa.some=function(e,t,a){var n=Vr(e)?Dt:io;return a&&xi(e,t,a)&&(t=o),n(e,li(t,3))},Oa.sortedIndex=function(e,t){return ro(e,t)},Oa.sortedIndexBy=function(e,t,a){return so(e,t,li(a,2))},Oa.sortedIndexOf=function(e,t){var a=null==e?0:e.length;if(a){var n=ro(e,t);if(n<a&&Wr(e[n],t))return n}return-1},Oa.sortedLastIndex=function(e,t){return ro(e,t,!0)},Oa.sortedLastIndexBy=function(e,t,a){return so(e,t,li(a,2),!0)},Oa.sortedLastIndexOf=function(e,t){if(null==e?0:e.length){var a=ro(e,t,!0)-1;if(Wr(e[a],t))return a}return-1},Oa.startCase=Zd,Oa.startsWith=function(e,t,a){return e=wd(e),a=null==a?0:sn(gd(a),0,e.length),t=uo(t),e.slice(a,a+t.length)==t},Oa.subtract=As,Oa.sum=function(e){return e&&e.length?Yt(e,is):0},Oa.sumBy=function(e,t){return e&&e.length?Yt(e,li(t,2)):0},Oa.template=function(e,t,a){var n=Oa.templateSettings;a&&xi(e,t,a)&&(t=o),e=wd(e),t=Rd({},t,n,ei);var i,r,d=Rd({},t.imports,n.imports,ei),s=Hd(d),c=Qt(d,s),l=0,u=t.interpolate||ke,p="__p += '",f=Te((t.escape||ke).source+"|"+u.source+"|"+(u===ee?he:ke).source+"|"+(t.evaluate||ke).source+"|$","g"),h="//# sourceURL="+(ze.call(t,"sourceURL")?(t.sourceURL+"").replace(/\s/g," "):"lodash.templateSources["+ ++it+"]")+"\n";e.replace(f,(function(t,a,n,o,d,s){return n||(n=o),p+=e.slice(l,s).replace(Re,ia),a&&(i=!0,p+="' +\n__e("+a+") +\n'"),d&&(r=!0,p+="';\n"+d+";\n__p += '"),n&&(p+="' +\n((__t = ("+n+")) == null ? '' : __t) +\n'"),l=s+t.length,t})),p+="';\n";var b=ze.call(t,"variable")&&t.variable;if(b){if(pe.test(b))throw new de("Invalid `variable` option passed into `_.template`")}else p="with (obj) {\n"+p+"\n}\n";p=(r?p.replace(G,""):p).replace(V,"$1").replace(J,"$1;"),p="function("+(b||"obj")+") {\n"+(b?"":"obj || (obj = {});\n")+"var __t, __p = ''"+(i?", __e = _.escape":"")+(r?", __j = Array.prototype.join;\nfunction print() { __p += __j.call(arguments, '') }\n":";\n")+p+"return __p\n}";var g=es((function(){return Ie(s,h+"return "+p).apply(o,c)}));if(g.source=p,Qr(g))throw g;return g},Oa.times=function(e,t){if((e=gd(e))<1||e>h)return[];var a=g,n=va(e,g);t=li(t),e-=g;for(var o=qt(n,t);++a<e;)t(a);return o},Oa.toFinite=bd,Oa.toInteger=gd,Oa.toLength=md,Oa.toLower=function(e){return wd(e).toLowerCase()},Oa.toNumber=yd,Oa.toSafeInteger=function(e){return e?sn(gd(e),-9007199254740991,h):0===e?e:0},Oa.toString=wd,Oa.toUpper=function(e){return wd(e).toUpperCase()},Oa.trim=function(e,t,a){if((e=wd(e))&&(a||t===o))return Zt(e);if(!e||!(t=uo(t)))return e;var n=fa(e),i=fa(t);return Ro(n,ea(n,i),ta(n,i)+1).join("")},Oa.trimEnd=function(e,t,a){if((e=wd(e))&&(a||t===o))return e.slice(0,ha(e)+1);if(!e||!(t=uo(t)))return e;var n=fa(e);return Ro(n,0,ta(n,fa(t))+1).join("")},Oa.trimStart=function(e,t,a){if((e=wd(e))&&(a||t===o))return e.replace(re,"");if(!e||!(t=uo(t)))return e;var n=fa(e);return Ro(n,ea(n,fa(t))).join("")},Oa.truncate=function(e,t){var a=30,n="...";if(ad(t)){var i="separator"in t?t.separator:i;a="length"in t?gd(t.length):a,n="omission"in t?uo(t.omission):n}var r=(e=wd(e)).length;if(ra(e)){var d=fa(e);r=d.length}if(a>=r)return e;var s=a-pa(n);if(s<1)return n;var c=d?Ro(d,0,s).join(""):e.slice(0,s);if(i===o)return c+n;if(d&&(s+=c.length-s),dd(i)){if(e.slice(s).search(i)){var l,u=c;for(i.global||(i=Te(i.source,wd(be.exec(i))+"g")),i.lastIndex=0;l=i.exec(u);)var p=l.index;c=c.slice(0,p===o?s:p)}}else if(e.indexOf(uo(i),s)!=s){var f=c.lastIndexOf(i);f>-1&&(c=c.slice(0,f))}return c+n},Oa.unescape=function(e){return(e=wd(e))&&Z.test(e)?e.replace(Y,ba):e},Oa.uniqueId=function(e){var t=++je;return wd(e)+t},Oa.upperCase=$d,Oa.upperFirst=Qd,Oa.each=wr,Oa.eachRight=xr,Oa.first=Yi,cs(Oa,function(){var e={};return xn(Oa,(function(t,a){ze.call(Oa.prototype,a)||(e[a]=t)})),e}(),{chain:!1}),Oa.VERSION="4.17.21",Tt(["bind","bindKey","curry","curryRight","partial","partialRight"],(function(e){Oa[e].placeholder=Oa})),Tt(["drop","take"],(function(e,t){Ka.prototype[e]=function(a){a=a===o?1:ya(gd(a),0);var n=this.__filtered__&&!t?new Ka(this):this.clone();return n.__filtered__?n.__takeCount__=va(a,n.__takeCount__):n.__views__.push({size:va(a,g),type:e+(n.__dir__<0?"Right":"")}),n},Ka.prototype[e+"Right"]=function(t){return this.reverse()[e](t).reverse()}})),Tt(["filter","map","takeWhile"],(function(e,t){var a=t+1,n=1==a||3==a;Ka.prototype[e]=function(e){var t=this.clone();return t.__iteratees__.push({iteratee:li(e,3),type:a}),t.__filtered__=t.__filtered__||n,t}})),Tt(["head","last"],(function(e,t){var a="take"+(t?"Right":"");Ka.prototype[e]=function(){return this[a](1).value()[0]}})),Tt(["initial","tail"],(function(e,t){var a="drop"+(t?"":"Right");Ka.prototype[e]=function(){return this.__filtered__?new Ka(this):this[a](1)}})),Ka.prototype.compact=function(){return this.filter(is)},Ka.prototype.find=function(e){return this.filter(e).head()},Ka.prototype.findLast=function(e){return this.reverse().find(e)},Ka.prototype.invokeMap=$n((function(e,t){return"function"==typeof e?new Ka(this):this.map((function(a){return Mn(a,e,t)}))})),Ka.prototype.reject=function(e){return this.filter(Dr(li(e)))},Ka.prototype.slice=function(e,t){e=gd(e);var a=this;return a.__filtered__&&(e>0||t<0)?new Ka(a):(e<0?a=a.takeRight(-e):e&&(a=a.drop(e)),t!==o&&(a=(t=gd(t))<0?a.dropRight(-t):a.take(t-e)),a)},Ka.prototype.takeRightWhile=function(e){return this.reverse().takeWhile(e).reverse()},Ka.prototype.toArray=function(){return this.take(g)},xn(Ka.prototype,(function(e,t){var a=/^(?:filter|find|map|reject)|While$/.test(t),n=/^(?:head|last)$/.test(t),i=Oa[n?"take"+("last"==t?"Right":""):t],r=n||/^find/.test(t);i&&(Oa.prototype[t]=function(){var t=this.__wrapped__,d=n?[1]:arguments,s=t instanceof Ka,c=d[0],l=s||Vr(t),u=function(e){var t=i.apply(Oa,_t([e],d));return n&&p?t[0]:t};l&&a&&"function"==typeof c&&1!=c.length&&(s=l=!1);var p=this.__chain__,f=!!this.__actions__.length,h=r&&!p,b=s&&!f;if(!r&&l){t=b?t:new Ka(this);var g=e.apply(t,d);return g.__actions__.push({func:br,args:[u],thisArg:o}),new Ba(g,p)}return h&&b?e.apply(this,d):(g=this.thru(u),h?n?g.value()[0]:g.value():g)})})),Tt(["pop","push","shift","sort","splice","unshift"],(function(e){var t=Ee[e],a=/^(?:push|sort|unshift)$/.test(e)?"tap":"thru",n=/^(?:pop|shift)$/.test(e);Oa.prototype[e]=function(){var e=arguments;if(n&&!this.__chain__){var o=this.value();return t.apply(Vr(o)?o:[],e)}return this[a]((function(a){return t.apply(Vr(a)?a:[],e)}))}})),xn(Ka.prototype,(function(e,t){var a=Oa[t];if(a){var n=a.name+"";ze.call(Ma,n)||(Ma[n]=[]),Ma[n].push({name:t,func:a})}})),Ma[Wo(o,2).name]=[{name:"wrapper",func:o}],Ka.prototype.clone=function(){var e=new Ka(this.__wrapped__);return e.__actions__=Mo(this.__actions__),e.__dir__=this.__dir__,e.__filtered__=this.__filtered__,e.__iteratees__=Mo(this.__iteratees__),e.__takeCount__=this.__takeCount__,e.__views__=Mo(this.__views__),e},Ka.prototype.reverse=function(){if(this.__filtered__){var e=new Ka(this);e.__dir__=-1,e.__filtered__=!0}else(e=this.clone()).__dir__*=-1;return e},Ka.prototype.value=function(){var e=this.__wrapped__.value(),t=this.__dir__,a=Vr(e),n=t<0,o=a?e.length:0,i=function(e,t,a){var n=-1,o=a.length;for(;++n<o;){var i=a[n],r=i.size;switch(i.type){case"drop":e+=r;break;case"dropRight":t-=r;break;case"take":t=va(t,e+r);break;case"takeRight":e=ya(e,t-r)}}return{start:e,end:t}}(0,o,this.__views__),r=i.start,d=i.end,s=d-r,c=n?d:r-1,l=this.__iteratees__,u=l.length,p=0,f=va(s,this.__takeCount__);if(!a||!n&&o==s&&f==s)return go(e,this.__actions__);var h=[];e:for(;s--&&p<f;){for(var b=-1,g=e[c+=t];++b<u;){var m=l[b],y=m.iteratee,v=m.type,w=y(g);if(2==v)g=w;else if(!w){if(1==v)continue e;break e}}h[p++]=g}return h},Oa.prototype.at=gr,Oa.prototype.chain=function(){return hr(this)},Oa.prototype.commit=function(){return new Ba(this.value(),this.__chain__)},Oa.prototype.next=function(){this.__values__===o&&(this.__values__=hd(this.value()));var e=this.__index__>=this.__values__.length;return{done:e,value:e?o:this.__values__[this.__index__++]}},Oa.prototype.plant=function(e){for(var t,a=this;a instanceof Wa;){var n=Ni(a);n.__index__=0,n.__values__=o,t?i.__wrapped__=n:t=n;var i=n;a=a.__wrapped__}return i.__wrapped__=e,t},Oa.prototype.reverse=function(){var e=this.__wrapped__;if(e instanceof Ka){var t=e;return this.__actions__.length&&(t=new Ka(this)),(t=t.reverse()).__actions__.push({func:br,args:[ar],thisArg:o}),new Ba(t,this.__chain__)}return this.thru(ar)},Oa.prototype.toJSON=Oa.prototype.valueOf=Oa.prototype.value=function(){return go(this.__wrapped__,this.__actions__)},Oa.prototype.first=Oa.prototype.head,$e&&(Oa.prototype[$e]=function(){return this}),Oa}();ft._=ga,(n=function(){return ga}.call(t,a,t,e))===o||(e.exports=n)}.call(this)},725:function(e){"use strict";var t=Object.getOwnPropertySymbols,a=Object.prototype.hasOwnProperty,n=Object.prototype.propertyIsEnumerable;function o(e){if(null===e||void 0===e)throw new TypeError("Object.assign cannot be called with null or undefined");return Object(e)}e.exports=function(){try{if(!Object.assign)return!1;var e=new String("abc");if(e[5]="de","5"===Object.getOwnPropertyNames(e)[0])return!1;for(var t={},a=0;a<10;a++)t["_"+String.fromCharCode(a)]=a;if("0123456789"!==Object.getOwnPropertyNames(t).map((function(e){return t[e]})).join(""))return!1;var n={};return"abcdefghijklmnopqrst".split("").forEach((function(e){n[e]=e})),"abcdefghijklmnopqrst"===Object.keys(Object.assign({},n)).join("")}catch(o){return!1}}()?Object.assign:function(e,i){for(var r,d,s=o(e),c=1;c<arguments.length;c++){for(var l in r=Object(arguments[c]))a.call(r,l)&&(s[l]=r[l]);if(t){d=t(r);for(var u=0;u<d.length;u++)n.call(r,d[u])&&(s[d[u]]=r[d[u]])}}return s}},463:function(e,t,a){"use strict";var n=a(791),o=a(725),i=a(296);function r(e){for(var t="https://reactjs.org/docs/error-decoder.html?invariant="+e,a=1;a<arguments.length;a++)t+="&args[]="+encodeURIComponent(arguments[a]);return"Minified React error #"+e+"; visit "+t+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}if(!n)throw Error(r(227));var d=new Set,s={};function c(e,t){l(e,t),l(e+"Capture",t)}function l(e,t){for(s[e]=t,e=0;e<t.length;e++)d.add(t[e])}var u=!("undefined"===typeof window||"undefined"===typeof window.document||"undefined"===typeof window.document.createElement),p=/^[:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD][:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD\-.0-9\u00B7\u0300-\u036F\u203F-\u2040]*$/,f=Object.prototype.hasOwnProperty,h={},b={};function g(e,t,a,n,o,i,r){this.acceptsBooleans=2===t||3===t||4===t,this.attributeName=n,this.attributeNamespace=o,this.mustUseProperty=a,this.propertyName=e,this.type=t,this.sanitizeURL=i,this.removeEmptyString=r}var m={};"children dangerouslySetInnerHTML defaultValue defaultChecked innerHTML suppressContentEditableWarning suppressHydrationWarning style".split(" ").forEach((function(e){m[e]=new g(e,0,!1,e,null,!1,!1)})),[["acceptCharset","accept-charset"],["className","class"],["htmlFor","for"],["httpEquiv","http-equiv"]].forEach((function(e){var t=e[0];m[t]=new g(t,1,!1,e[1],null,!1,!1)})),["contentEditable","draggable","spellCheck","value"].forEach((function(e){m[e]=new g(e,2,!1,e.toLowerCase(),null,!1,!1)})),["autoReverse","externalResourcesRequired","focusable","preserveAlpha"].forEach((function(e){m[e]=new g(e,2,!1,e,null,!1,!1)})),"allowFullScreen async autoFocus autoPlay controls default defer disabled disablePictureInPicture disableRemotePlayback formNoValidate hidden loop noModule noValidate open playsInline readOnly required reversed scoped seamless itemScope".split(" ").forEach((function(e){m[e]=new g(e,3,!1,e.toLowerCase(),null,!1,!1)})),["checked","multiple","muted","selected"].forEach((function(e){m[e]=new g(e,3,!0,e,null,!1,!1)})),["capture","download"].forEach((function(e){m[e]=new g(e,4,!1,e,null,!1,!1)})),["cols","rows","size","span"].forEach((function(e){m[e]=new g(e,6,!1,e,null,!1,!1)})),["rowSpan","start"].forEach((function(e){m[e]=new g(e,5,!1,e.toLowerCase(),null,!1,!1)}));var y=/[\-:]([a-z])/g;function v(e){return e[1].toUpperCase()}function w(e,t,a,n){var o=m.hasOwnProperty(t)?m[t]:null;(null!==o?0===o.type:!n&&(2<t.length&&("o"===t[0]||"O"===t[0])&&("n"===t[1]||"N"===t[1])))||(function(e,t,a,n){if(null===t||"undefined"===typeof t||function(e,t,a,n){if(null!==a&&0===a.type)return!1;switch(typeof t){case"function":case"symbol":return!0;case"boolean":return!n&&(null!==a?!a.acceptsBooleans:"data-"!==(e=e.toLowerCase().slice(0,5))&&"aria-"!==e);default:return!1}}(e,t,a,n))return!0;if(n)return!1;if(null!==a)switch(a.type){case 3:return!t;case 4:return!1===t;case 5:return isNaN(t);case 6:return isNaN(t)||1>t}return!1}(t,a,o,n)&&(a=null),n||null===o?function(e){return!!f.call(b,e)||!f.call(h,e)&&(p.test(e)?b[e]=!0:(h[e]=!0,!1))}(t)&&(null===a?e.removeAttribute(t):e.setAttribute(t,""+a)):o.mustUseProperty?e[o.propertyName]=null===a?3!==o.type&&"":a:(t=o.attributeName,n=o.attributeNamespace,null===a?e.removeAttribute(t):(a=3===(o=o.type)||4===o&&!0===a?"":""+a,n?e.setAttributeNS(n,t,a):e.setAttribute(t,a))))}"accent-height alignment-baseline arabic-form baseline-shift cap-height clip-path clip-rule color-interpolation color-interpolation-filters color-profile color-rendering dominant-baseline enable-background fill-opacity fill-rule flood-color flood-opacity font-family font-size font-size-adjust font-stretch font-style font-variant font-weight glyph-name glyph-orientation-horizontal glyph-orientation-vertical horiz-adv-x horiz-origin-x image-rendering letter-spacing lighting-color marker-end marker-mid marker-start overline-position overline-thickness paint-order panose-1 pointer-events rendering-intent shape-rendering stop-color stop-opacity strikethrough-position strikethrough-thickness stroke-dasharray stroke-dashoffset stroke-linecap stroke-linejoin stroke-miterlimit stroke-opacity stroke-width text-anchor text-decoration text-rendering underline-position underline-thickness unicode-bidi unicode-range units-per-em v-alphabetic v-hanging v-ideographic v-mathematical vector-effect vert-adv-y vert-origin-x vert-origin-y word-spacing writing-mode xmlns:xlink x-height".split(" ").forEach((function(e){var t=e.replace(y,v);m[t]=new g(t,1,!1,e,null,!1,!1)})),"xlink:actuate xlink:arcrole xlink:role xlink:show xlink:title xlink:type".split(" ").forEach((function(e){var t=e.replace(y,v);m[t]=new g(t,1,!1,e,"http://www.w3.org/1999/xlink",!1,!1)})),["xml:base","xml:lang","xml:space"].forEach((function(e){var t=e.replace(y,v);m[t]=new g(t,1,!1,e,"http://www.w3.org/XML/1998/namespace",!1,!1)})),["tabIndex","crossOrigin"].forEach((function(e){m[e]=new g(e,1,!1,e.toLowerCase(),null,!1,!1)})),m.xlinkHref=new g("xlinkHref",1,!1,"xlink:href","http://www.w3.org/1999/xlink",!0,!1),["src","href","action","formAction"].forEach((function(e){m[e]=new g(e,1,!1,e.toLowerCase(),null,!0,!0)}));var x=n.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED,k=60103,R=60106,I=60107,A=60108,F=60114,T=60109,S=60110,C=60112,E=60113,M=60120,P=60115,H=60116,_=60121,z=60128,j=60129,D=60130,L=60131;if("function"===typeof Symbol&&Symbol.for){var U=Symbol.for;k=U("react.element"),R=U("react.portal"),I=U("react.fragment"),A=U("react.strict_mode"),F=U("react.profiler"),T=U("react.provider"),S=U("react.context"),C=U("react.forward_ref"),E=U("react.suspense"),M=U("react.suspense_list"),P=U("react.memo"),H=U("react.lazy"),_=U("react.block"),U("react.scope"),z=U("react.opaque.id"),j=U("react.debug_trace_mode"),D=U("react.offscreen"),L=U("react.legacy_hidden")}var O,N="function"===typeof Symbol&&Symbol.iterator;function W(e){return null===e||"object"!==typeof e?null:"function"===typeof(e=N&&e[N]||e["@@iterator"])?e:null}function B(e){if(void 0===O)try{throw Error()}catch(a){var t=a.stack.trim().match(/\n( *(at )?)/);O=t&&t[1]||""}return"\n"+O+e}var K=!1;function G(e,t){if(!e||K)return"";K=!0;var a=Error.prepareStackTrace;Error.prepareStackTrace=void 0;try{if(t)if(t=function(){throw Error()},Object.defineProperty(t.prototype,"props",{set:function(){throw Error()}}),"object"===typeof Reflect&&Reflect.construct){try{Reflect.construct(t,[])}catch(s){var n=s}Reflect.construct(e,[],t)}else{try{t.call()}catch(s){n=s}e.call(t.prototype)}else{try{throw Error()}catch(s){n=s}e()}}catch(s){if(s&&n&&"string"===typeof s.stack){for(var o=s.stack.split("\n"),i=n.stack.split("\n"),r=o.length-1,d=i.length-1;1<=r&&0<=d&&o[r]!==i[d];)d--;for(;1<=r&&0<=d;r--,d--)if(o[r]!==i[d]){if(1!==r||1!==d)do{if(r--,0>--d||o[r]!==i[d])return"\n"+o[r].replace(" at new "," at ")}while(1<=r&&0<=d);break}}}finally{K=!1,Error.prepareStackTrace=a}return(e=e?e.displayName||e.name:"")?B(e):""}function V(e){switch(e.tag){case 5:return B(e.type);case 16:return B("Lazy");case 13:return B("Suspense");case 19:return B("SuspenseList");case 0:case 2:case 15:return e=G(e.type,!1);case 11:return e=G(e.type.render,!1);case 22:return e=G(e.type._render,!1);case 1:return e=G(e.type,!0);default:return""}}function J(e){if(null==e)return null;if("function"===typeof e)return e.displayName||e.name||null;if("string"===typeof e)return e;switch(e){case I:return"Fragment";case R:return"Portal";case F:return"Profiler";case A:return"StrictMode";case E:return"Suspense";case M:return"SuspenseList"}if("object"===typeof e)switch(e.$$typeof){case S:return(e.displayName||"Context")+".Consumer";case T:return(e._context.displayName||"Context")+".Provider";case C:var t=e.render;return t=t.displayName||t.name||"",e.displayName||(""!==t?"ForwardRef("+t+")":"ForwardRef");case P:return J(e.type);case _:return J(e._render);case H:t=e._payload,e=e._init;try{return J(e(t))}catch(a){}}return null}function Y(e){switch(typeof e){case"boolean":case"number":case"object":case"string":case"undefined":return e;default:return""}}function q(e){var t=e.type;return(e=e.nodeName)&&"input"===e.toLowerCase()&&("checkbox"===t||"radio"===t)}function Z(e){e._valueTracker||(e._valueTracker=function(e){var t=q(e)?"checked":"value",a=Object.getOwnPropertyDescriptor(e.constructor.prototype,t),n=""+e[t];if(!e.hasOwnProperty(t)&&"undefined"!==typeof a&&"function"===typeof a.get&&"function"===typeof a.set){var o=a.get,i=a.set;return Object.defineProperty(e,t,{configurable:!0,get:function(){return o.call(this)},set:function(e){n=""+e,i.call(this,e)}}),Object.defineProperty(e,t,{enumerable:a.enumerable}),{getValue:function(){return n},setValue:function(e){n=""+e},stopTracking:function(){e._valueTracker=null,delete e[t]}}}}(e))}function $(e){if(!e)return!1;var t=e._valueTracker;if(!t)return!0;var a=t.getValue(),n="";return e&&(n=q(e)?e.checked?"true":"false":e.value),(e=n)!==a&&(t.setValue(e),!0)}function Q(e){if("undefined"===typeof(e=e||("undefined"!==typeof document?document:void 0)))return null;try{return e.activeElement||e.body}catch(t){return e.body}}function X(e,t){var a=t.checked;return o({},t,{defaultChecked:void 0,defaultValue:void 0,value:void 0,checked:null!=a?a:e._wrapperState.initialChecked})}function ee(e,t){var a=null==t.defaultValue?"":t.defaultValue,n=null!=t.checked?t.checked:t.defaultChecked;a=Y(null!=t.value?t.value:a),e._wrapperState={initialChecked:n,initialValue:a,controlled:"checkbox"===t.type||"radio"===t.type?null!=t.checked:null!=t.value}}function te(e,t){null!=(t=t.checked)&&w(e,"checked",t,!1)}function ae(e,t){te(e,t);var a=Y(t.value),n=t.type;if(null!=a)"number"===n?(0===a&&""===e.value||e.value!=a)&&(e.value=""+a):e.value!==""+a&&(e.value=""+a);else if("submit"===n||"reset"===n)return void e.removeAttribute("value");t.hasOwnProperty("value")?oe(e,t.type,a):t.hasOwnProperty("defaultValue")&&oe(e,t.type,Y(t.defaultValue)),null==t.checked&&null!=t.defaultChecked&&(e.defaultChecked=!!t.defaultChecked)}function ne(e,t,a){if(t.hasOwnProperty("value")||t.hasOwnProperty("defaultValue")){var n=t.type;if(!("submit"!==n&&"reset"!==n||void 0!==t.value&&null!==t.value))return;t=""+e._wrapperState.initialValue,a||t===e.value||(e.value=t),e.defaultValue=t}""!==(a=e.name)&&(e.name=""),e.defaultChecked=!!e._wrapperState.initialChecked,""!==a&&(e.name=a)}function oe(e,t,a){"number"===t&&Q(e.ownerDocument)===e||(null==a?e.defaultValue=""+e._wrapperState.initialValue:e.defaultValue!==""+a&&(e.defaultValue=""+a))}function ie(e,t){return e=o({children:void 0},t),(t=function(e){var t="";return n.Children.forEach(e,(function(e){null!=e&&(t+=e)})),t}(t.children))&&(e.children=t),e}function re(e,t,a,n){if(e=e.options,t){t={};for(var o=0;o<a.length;o++)t["$"+a[o]]=!0;for(a=0;a<e.length;a++)o=t.hasOwnProperty("$"+e[a].value),e[a].selected!==o&&(e[a].selected=o),o&&n&&(e[a].defaultSelected=!0)}else{for(a=""+Y(a),t=null,o=0;o<e.length;o++){if(e[o].value===a)return e[o].selected=!0,void(n&&(e[o].defaultSelected=!0));null!==t||e[o].disabled||(t=e[o])}null!==t&&(t.selected=!0)}}function de(e,t){if(null!=t.dangerouslySetInnerHTML)throw Error(r(91));return o({},t,{value:void 0,defaultValue:void 0,children:""+e._wrapperState.initialValue})}function se(e,t){var a=t.value;if(null==a){if(a=t.children,t=t.defaultValue,null!=a){if(null!=t)throw Error(r(92));if(Array.isArray(a)){if(!(1>=a.length))throw Error(r(93));a=a[0]}t=a}null==t&&(t=""),a=t}e._wrapperState={initialValue:Y(a)}}function ce(e,t){var a=Y(t.value),n=Y(t.defaultValue);null!=a&&((a=""+a)!==e.value&&(e.value=a),null==t.defaultValue&&e.defaultValue!==a&&(e.defaultValue=a)),null!=n&&(e.defaultValue=""+n)}function le(e){var t=e.textContent;t===e._wrapperState.initialValue&&""!==t&&null!==t&&(e.value=t)}var ue="http://www.w3.org/1999/xhtml",pe="http://www.w3.org/2000/svg";function fe(e){switch(e){case"svg":return"http://www.w3.org/2000/svg";case"math":return"http://www.w3.org/1998/Math/MathML";default:return"http://www.w3.org/1999/xhtml"}}function he(e,t){return null==e||"http://www.w3.org/1999/xhtml"===e?fe(t):"http://www.w3.org/2000/svg"===e&&"foreignObject"===t?"http://www.w3.org/1999/xhtml":e}var be,ge,me=(ge=function(e,t){if(e.namespaceURI!==pe||"innerHTML"in e)e.innerHTML=t;else{for((be=be||document.createElement("div")).innerHTML="<svg>"+t.valueOf().toString()+"</svg>",t=be.firstChild;e.firstChild;)e.removeChild(e.firstChild);for(;t.firstChild;)e.appendChild(t.firstChild)}},"undefined"!==typeof MSApp&&MSApp.execUnsafeLocalFunction?function(e,t,a,n){MSApp.execUnsafeLocalFunction((function(){return ge(e,t)}))}:ge);function ye(e,t){if(t){var a=e.firstChild;if(a&&a===e.lastChild&&3===a.nodeType)return void(a.nodeValue=t)}e.textContent=t}var ve={animationIterationCount:!0,borderImageOutset:!0,borderImageSlice:!0,borderImageWidth:!0,boxFlex:!0,boxFlexGroup:!0,boxOrdinalGroup:!0,columnCount:!0,columns:!0,flex:!0,flexGrow:!0,flexPositive:!0,flexShrink:!0,flexNegative:!0,flexOrder:!0,gridArea:!0,gridRow:!0,gridRowEnd:!0,gridRowSpan:!0,gridRowStart:!0,gridColumn:!0,gridColumnEnd:!0,gridColumnSpan:!0,gridColumnStart:!0,fontWeight:!0,lineClamp:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,tabSize:!0,widows:!0,zIndex:!0,zoom:!0,fillOpacity:!0,floodOpacity:!0,stopOpacity:!0,strokeDasharray:!0,strokeDashoffset:!0,strokeMiterlimit:!0,strokeOpacity:!0,strokeWidth:!0},we=["Webkit","ms","Moz","O"];function xe(e,t,a){return null==t||"boolean"===typeof t||""===t?"":a||"number"!==typeof t||0===t||ve.hasOwnProperty(e)&&ve[e]?(""+t).trim():t+"px"}function ke(e,t){for(var a in e=e.style,t)if(t.hasOwnProperty(a)){var n=0===a.indexOf("--"),o=xe(a,t[a],n);"float"===a&&(a="cssFloat"),n?e.setProperty(a,o):e[a]=o}}Object.keys(ve).forEach((function(e){we.forEach((function(t){t=t+e.charAt(0).toUpperCase()+e.substring(1),ve[t]=ve[e]}))}));var Re=o({menuitem:!0},{area:!0,base:!0,br:!0,col:!0,embed:!0,hr:!0,img:!0,input:!0,keygen:!0,link:!0,meta:!0,param:!0,source:!0,track:!0,wbr:!0});function Ie(e,t){if(t){if(Re[e]&&(null!=t.children||null!=t.dangerouslySetInnerHTML))throw Error(r(137,e));if(null!=t.dangerouslySetInnerHTML){if(null!=t.children)throw Error(r(60));if("object"!==typeof t.dangerouslySetInnerHTML||!("__html"in t.dangerouslySetInnerHTML))throw Error(r(61))}if(null!=t.style&&"object"!==typeof t.style)throw Error(r(62))}}function Ae(e,t){if(-1===e.indexOf("-"))return"string"===typeof t.is;switch(e){case"annotation-xml":case"color-profile":case"font-face":case"font-face-src":case"font-face-uri":case"font-face-format":case"font-face-name":case"missing-glyph":return!1;default:return!0}}function Fe(e){return(e=e.target||e.srcElement||window).correspondingUseElement&&(e=e.correspondingUseElement),3===e.nodeType?e.parentNode:e}var Te=null,Se=null,Ce=null;function Ee(e){if(e=no(e)){if("function"!==typeof Te)throw Error(r(280));var t=e.stateNode;t&&(t=io(t),Te(e.stateNode,e.type,t))}}function Me(e){Se?Ce?Ce.push(e):Ce=[e]:Se=e}function Pe(){if(Se){var e=Se,t=Ce;if(Ce=Se=null,Ee(e),t)for(e=0;e<t.length;e++)Ee(t[e])}}function He(e,t){return e(t)}function _e(e,t,a,n,o){return e(t,a,n,o)}function ze(){}var je=He,De=!1,Le=!1;function Ue(){null===Se&&null===Ce||(ze(),Pe())}function Oe(e,t){var a=e.stateNode;if(null===a)return null;var n=io(a);if(null===n)return null;a=n[t];e:switch(t){case"onClick":case"onClickCapture":case"onDoubleClick":case"onDoubleClickCapture":case"onMouseDown":case"onMouseDownCapture":case"onMouseMove":case"onMouseMoveCapture":case"onMouseUp":case"onMouseUpCapture":case"onMouseEnter":(n=!n.disabled)||(n=!("button"===(e=e.type)||"input"===e||"select"===e||"textarea"===e)),e=!n;break e;default:e=!1}if(e)return null;if(a&&"function"!==typeof a)throw Error(r(231,t,typeof a));return a}var Ne=!1;if(u)try{var We={};Object.defineProperty(We,"passive",{get:function(){Ne=!0}}),window.addEventListener("test",We,We),window.removeEventListener("test",We,We)}catch(ge){Ne=!1}function Be(e,t,a,n,o,i,r,d,s){var c=Array.prototype.slice.call(arguments,3);try{t.apply(a,c)}catch(l){this.onError(l)}}var Ke=!1,Ge=null,Ve=!1,Je=null,Ye={onError:function(e){Ke=!0,Ge=e}};function qe(e,t,a,n,o,i,r,d,s){Ke=!1,Ge=null,Be.apply(Ye,arguments)}function Ze(e){var t=e,a=e;if(e.alternate)for(;t.return;)t=t.return;else{e=t;do{0!==(1026&(t=e).flags)&&(a=t.return),e=t.return}while(e)}return 3===t.tag?a:null}function $e(e){if(13===e.tag){var t=e.memoizedState;if(null===t&&(null!==(e=e.alternate)&&(t=e.memoizedState)),null!==t)return t.dehydrated}return null}function Qe(e){if(Ze(e)!==e)throw Error(r(188))}function Xe(e){if(e=function(e){var t=e.alternate;if(!t){if(null===(t=Ze(e)))throw Error(r(188));return t!==e?null:e}for(var a=e,n=t;;){var o=a.return;if(null===o)break;var i=o.alternate;if(null===i){if(null!==(n=o.return)){a=n;continue}break}if(o.child===i.child){for(i=o.child;i;){if(i===a)return Qe(o),e;if(i===n)return Qe(o),t;i=i.sibling}throw Error(r(188))}if(a.return!==n.return)a=o,n=i;else{for(var d=!1,s=o.child;s;){if(s===a){d=!0,a=o,n=i;break}if(s===n){d=!0,n=o,a=i;break}s=s.sibling}if(!d){for(s=i.child;s;){if(s===a){d=!0,a=i,n=o;break}if(s===n){d=!0,n=i,a=o;break}s=s.sibling}if(!d)throw Error(r(189))}}if(a.alternate!==n)throw Error(r(190))}if(3!==a.tag)throw Error(r(188));return a.stateNode.current===a?e:t}(e),!e)return null;for(var t=e;;){if(5===t.tag||6===t.tag)return t;if(t.child)t.child.return=t,t=t.child;else{if(t===e)break;for(;!t.sibling;){if(!t.return||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}}return null}function et(e,t){for(var a=e.alternate;null!==t;){if(t===e||t===a)return!0;t=t.return}return!1}var tt,at,nt,ot,it=!1,rt=[],dt=null,st=null,ct=null,lt=new Map,ut=new Map,pt=[],ft="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput copy cut paste click change contextmenu reset submit".split(" ");function ht(e,t,a,n,o){return{blockedOn:e,domEventName:t,eventSystemFlags:16|a,nativeEvent:o,targetContainers:[n]}}function bt(e,t){switch(e){case"focusin":case"focusout":dt=null;break;case"dragenter":case"dragleave":st=null;break;case"mouseover":case"mouseout":ct=null;break;case"pointerover":case"pointerout":lt.delete(t.pointerId);break;case"gotpointercapture":case"lostpointercapture":ut.delete(t.pointerId)}}function gt(e,t,a,n,o,i){return null===e||e.nativeEvent!==i?(e=ht(t,a,n,o,i),null!==t&&(null!==(t=no(t))&&at(t)),e):(e.eventSystemFlags|=n,t=e.targetContainers,null!==o&&-1===t.indexOf(o)&&t.push(o),e)}function mt(e){var t=ao(e.target);if(null!==t){var a=Ze(t);if(null!==a)if(13===(t=a.tag)){if(null!==(t=$e(a)))return e.blockedOn=t,void ot(e.lanePriority,(function(){i.unstable_runWithPriority(e.priority,(function(){nt(a)}))}))}else if(3===t&&a.stateNode.hydrate)return void(e.blockedOn=3===a.tag?a.stateNode.containerInfo:null)}e.blockedOn=null}function yt(e){if(null!==e.blockedOn)return!1;for(var t=e.targetContainers;0<t.length;){var a=Xt(e.domEventName,e.eventSystemFlags,t[0],e.nativeEvent);if(null!==a)return null!==(t=no(a))&&at(t),e.blockedOn=a,!1;t.shift()}return!0}function vt(e,t,a){yt(e)&&a.delete(t)}function wt(){for(it=!1;0<rt.length;){var e=rt[0];if(null!==e.blockedOn){null!==(e=no(e.blockedOn))&&tt(e);break}for(var t=e.targetContainers;0<t.length;){var a=Xt(e.domEventName,e.eventSystemFlags,t[0],e.nativeEvent);if(null!==a){e.blockedOn=a;break}t.shift()}null===e.blockedOn&&rt.shift()}null!==dt&&yt(dt)&&(dt=null),null!==st&&yt(st)&&(st=null),null!==ct&&yt(ct)&&(ct=null),lt.forEach(vt),ut.forEach(vt)}function xt(e,t){e.blockedOn===t&&(e.blockedOn=null,it||(it=!0,i.unstable_scheduleCallback(i.unstable_NormalPriority,wt)))}function kt(e){function t(t){return xt(t,e)}if(0<rt.length){xt(rt[0],e);for(var a=1;a<rt.length;a++){var n=rt[a];n.blockedOn===e&&(n.blockedOn=null)}}for(null!==dt&&xt(dt,e),null!==st&&xt(st,e),null!==ct&&xt(ct,e),lt.forEach(t),ut.forEach(t),a=0;a<pt.length;a++)(n=pt[a]).blockedOn===e&&(n.blockedOn=null);for(;0<pt.length&&null===(a=pt[0]).blockedOn;)mt(a),null===a.blockedOn&&pt.shift()}function Rt(e,t){var a={};return a[e.toLowerCase()]=t.toLowerCase(),a["Webkit"+e]="webkit"+t,a["Moz"+e]="moz"+t,a}var It={animationend:Rt("Animation","AnimationEnd"),animationiteration:Rt("Animation","AnimationIteration"),animationstart:Rt("Animation","AnimationStart"),transitionend:Rt("Transition","TransitionEnd")},At={},Ft={};function Tt(e){if(At[e])return At[e];if(!It[e])return e;var t,a=It[e];for(t in a)if(a.hasOwnProperty(t)&&t in Ft)return At[e]=a[t];return e}u&&(Ft=document.createElement("div").style,"AnimationEvent"in window||(delete It.animationend.animation,delete It.animationiteration.animation,delete It.animationstart.animation),"TransitionEvent"in window||delete It.transitionend.transition);var St=Tt("animationend"),Ct=Tt("animationiteration"),Et=Tt("animationstart"),Mt=Tt("transitionend"),Pt=new Map,Ht=new Map,_t=["abort","abort",St,"animationEnd",Ct,"animationIteration",Et,"animationStart","canplay","canPlay","canplaythrough","canPlayThrough","durationchange","durationChange","emptied","emptied","encrypted","encrypted","ended","ended","error","error","gotpointercapture","gotPointerCapture","load","load","loadeddata","loadedData","loadedmetadata","loadedMetadata","loadstart","loadStart","lostpointercapture","lostPointerCapture","playing","playing","progress","progress","seeking","seeking","stalled","stalled","suspend","suspend","timeupdate","timeUpdate",Mt,"transitionEnd","waiting","waiting"];function zt(e,t){for(var a=0;a<e.length;a+=2){var n=e[a],o=e[a+1];o="on"+(o[0].toUpperCase()+o.slice(1)),Ht.set(n,t),Pt.set(n,o),c(o,[n])}}(0,i.unstable_now)();var jt=8;function Dt(e){if(0!==(1&e))return jt=15,1;if(0!==(2&e))return jt=14,2;if(0!==(4&e))return jt=13,4;var t=24&e;return 0!==t?(jt=12,t):0!==(32&e)?(jt=11,32):0!==(t=192&e)?(jt=10,t):0!==(256&e)?(jt=9,256):0!==(t=3584&e)?(jt=8,t):0!==(4096&e)?(jt=7,4096):0!==(t=4186112&e)?(jt=6,t):0!==(t=62914560&e)?(jt=5,t):67108864&e?(jt=4,67108864):0!==(134217728&e)?(jt=3,134217728):0!==(t=805306368&e)?(jt=2,t):0!==(1073741824&e)?(jt=1,1073741824):(jt=8,e)}function Lt(e,t){var a=e.pendingLanes;if(0===a)return jt=0;var n=0,o=0,i=e.expiredLanes,r=e.suspendedLanes,d=e.pingedLanes;if(0!==i)n=i,o=jt=15;else if(0!==(i=134217727&a)){var s=i&~r;0!==s?(n=Dt(s),o=jt):0!==(d&=i)&&(n=Dt(d),o=jt)}else 0!==(i=a&~r)?(n=Dt(i),o=jt):0!==d&&(n=Dt(d),o=jt);if(0===n)return 0;if(n=a&((0>(n=31-Kt(n))?0:1<<n)<<1)-1,0!==t&&t!==n&&0===(t&r)){if(Dt(t),o<=jt)return t;jt=o}if(0!==(t=e.entangledLanes))for(e=e.entanglements,t&=n;0<t;)o=1<<(a=31-Kt(t)),n|=e[a],t&=~o;return n}function Ut(e){return 0!==(e=-1073741825&e.pendingLanes)?e:1073741824&e?1073741824:0}function Ot(e,t){switch(e){case 15:return 1;case 14:return 2;case 12:return 0===(e=Nt(24&~t))?Ot(10,t):e;case 10:return 0===(e=Nt(192&~t))?Ot(8,t):e;case 8:return 0===(e=Nt(3584&~t))&&(0===(e=Nt(4186112&~t))&&(e=512)),e;case 2:return 0===(t=Nt(805306368&~t))&&(t=268435456),t}throw Error(r(358,e))}function Nt(e){return e&-e}function Wt(e){for(var t=[],a=0;31>a;a++)t.push(e);return t}function Bt(e,t,a){e.pendingLanes|=t;var n=t-1;e.suspendedLanes&=n,e.pingedLanes&=n,(e=e.eventTimes)[t=31-Kt(t)]=a}var Kt=Math.clz32?Math.clz32:function(e){return 0===e?32:31-(Gt(e)/Vt|0)|0},Gt=Math.log,Vt=Math.LN2;var Jt=i.unstable_UserBlockingPriority,Yt=i.unstable_runWithPriority,qt=!0;function Zt(e,t,a,n){De||ze();var o=Qt,i=De;De=!0;try{_e(o,e,t,a,n)}finally{(De=i)||Ue()}}function $t(e,t,a,n){Yt(Jt,Qt.bind(null,e,t,a,n))}function Qt(e,t,a,n){var o;if(qt)if((o=0===(4&t))&&0<rt.length&&-1<ft.indexOf(e))e=ht(null,e,t,a,n),rt.push(e);else{var i=Xt(e,t,a,n);if(null===i)o&&bt(e,n);else{if(o){if(-1<ft.indexOf(e))return e=ht(i,e,t,a,n),void rt.push(e);if(function(e,t,a,n,o){switch(t){case"focusin":return dt=gt(dt,e,t,a,n,o),!0;case"dragenter":return st=gt(st,e,t,a,n,o),!0;case"mouseover":return ct=gt(ct,e,t,a,n,o),!0;case"pointerover":var i=o.pointerId;return lt.set(i,gt(lt.get(i)||null,e,t,a,n,o)),!0;case"gotpointercapture":return i=o.pointerId,ut.set(i,gt(ut.get(i)||null,e,t,a,n,o)),!0}return!1}(i,e,t,a,n))return;bt(e,n)}zn(e,t,n,null,a)}}}function Xt(e,t,a,n){var o=Fe(n);if(null!==(o=ao(o))){var i=Ze(o);if(null===i)o=null;else{var r=i.tag;if(13===r){if(null!==(o=$e(i)))return o;o=null}else if(3===r){if(i.stateNode.hydrate)return 3===i.tag?i.stateNode.containerInfo:null;o=null}else i!==o&&(o=null)}}return zn(e,t,n,o,a),null}var ea=null,ta=null,aa=null;function na(){if(aa)return aa;var e,t,a=ta,n=a.length,o="value"in ea?ea.value:ea.textContent,i=o.length;for(e=0;e<n&&a[e]===o[e];e++);var r=n-e;for(t=1;t<=r&&a[n-t]===o[i-t];t++);return aa=o.slice(e,1<t?1-t:void 0)}function oa(e){var t=e.keyCode;return"charCode"in e?0===(e=e.charCode)&&13===t&&(e=13):e=t,10===e&&(e=13),32<=e||13===e?e:0}function ia(){return!0}function ra(){return!1}function da(e){function t(t,a,n,o,i){for(var r in this._reactName=t,this._targetInst=n,this.type=a,this.nativeEvent=o,this.target=i,this.currentTarget=null,e)e.hasOwnProperty(r)&&(t=e[r],this[r]=t?t(o):o[r]);return this.isDefaultPrevented=(null!=o.defaultPrevented?o.defaultPrevented:!1===o.returnValue)?ia:ra,this.isPropagationStopped=ra,this}return o(t.prototype,{preventDefault:function(){this.defaultPrevented=!0;var e=this.nativeEvent;e&&(e.preventDefault?e.preventDefault():"unknown"!==typeof e.returnValue&&(e.returnValue=!1),this.isDefaultPrevented=ia)},stopPropagation:function(){var e=this.nativeEvent;e&&(e.stopPropagation?e.stopPropagation():"unknown"!==typeof e.cancelBubble&&(e.cancelBubble=!0),this.isPropagationStopped=ia)},persist:function(){},isPersistent:ia}),t}var sa,ca,la,ua={eventPhase:0,bubbles:0,cancelable:0,timeStamp:function(e){return e.timeStamp||Date.now()},defaultPrevented:0,isTrusted:0},pa=da(ua),fa=o({},ua,{view:0,detail:0}),ha=da(fa),ba=o({},fa,{screenX:0,screenY:0,clientX:0,clientY:0,pageX:0,pageY:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,getModifierState:Ta,button:0,buttons:0,relatedTarget:function(e){return void 0===e.relatedTarget?e.fromElement===e.srcElement?e.toElement:e.fromElement:e.relatedTarget},movementX:function(e){return"movementX"in e?e.movementX:(e!==la&&(la&&"mousemove"===e.type?(sa=e.screenX-la.screenX,ca=e.screenY-la.screenY):ca=sa=0,la=e),sa)},movementY:function(e){return"movementY"in e?e.movementY:ca}}),ga=da(ba),ma=da(o({},ba,{dataTransfer:0})),ya=da(o({},fa,{relatedTarget:0})),va=da(o({},ua,{animationName:0,elapsedTime:0,pseudoElement:0})),wa=o({},ua,{clipboardData:function(e){return"clipboardData"in e?e.clipboardData:window.clipboardData}}),xa=da(wa),ka=da(o({},ua,{data:0})),Ra={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},Ia={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",224:"Meta"},Aa={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"};function Fa(e){var t=this.nativeEvent;return t.getModifierState?t.getModifierState(e):!!(e=Aa[e])&&!!t[e]}function Ta(){return Fa}var Sa=o({},fa,{key:function(e){if(e.key){var t=Ra[e.key]||e.key;if("Unidentified"!==t)return t}return"keypress"===e.type?13===(e=oa(e))?"Enter":String.fromCharCode(e):"keydown"===e.type||"keyup"===e.type?Ia[e.keyCode]||"Unidentified":""},code:0,location:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,repeat:0,locale:0,getModifierState:Ta,charCode:function(e){return"keypress"===e.type?oa(e):0},keyCode:function(e){return"keydown"===e.type||"keyup"===e.type?e.keyCode:0},which:function(e){return"keypress"===e.type?oa(e):"keydown"===e.type||"keyup"===e.type?e.keyCode:0}}),Ca=da(Sa),Ea=da(o({},ba,{pointerId:0,width:0,height:0,pressure:0,tangentialPressure:0,tiltX:0,tiltY:0,twist:0,pointerType:0,isPrimary:0})),Ma=da(o({},fa,{touches:0,targetTouches:0,changedTouches:0,altKey:0,metaKey:0,ctrlKey:0,shiftKey:0,getModifierState:Ta})),Pa=da(o({},ua,{propertyName:0,elapsedTime:0,pseudoElement:0})),Ha=o({},ba,{deltaX:function(e){return"deltaX"in e?e.deltaX:"wheelDeltaX"in e?-e.wheelDeltaX:0},deltaY:function(e){return"deltaY"in e?e.deltaY:"wheelDeltaY"in e?-e.wheelDeltaY:"wheelDelta"in e?-e.wheelDelta:0},deltaZ:0,deltaMode:0}),_a=da(Ha),za=[9,13,27,32],ja=u&&"CompositionEvent"in window,Da=null;u&&"documentMode"in document&&(Da=document.documentMode);var La=u&&"TextEvent"in window&&!Da,Ua=u&&(!ja||Da&&8<Da&&11>=Da),Oa=String.fromCharCode(32),Na=!1;function Wa(e,t){switch(e){case"keyup":return-1!==za.indexOf(t.keyCode);case"keydown":return 229!==t.keyCode;case"keypress":case"mousedown":case"focusout":return!0;default:return!1}}function Ba(e){return"object"===typeof(e=e.detail)&&"data"in e?e.data:null}var Ka=!1;var Ga={color:!0,date:!0,datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0};function Va(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return"input"===t?!!Ga[e.type]:"textarea"===t}function Ja(e,t,a,n){Me(n),0<(t=Dn(t,"onChange")).length&&(a=new pa("onChange","change",null,a,n),e.push({event:a,listeners:t}))}var Ya=null,qa=null;function Za(e){Cn(e,0)}function $a(e){if($(oo(e)))return e}function Qa(e,t){if("change"===e)return t}var Xa=!1;if(u){var en;if(u){var tn="oninput"in document;if(!tn){var an=document.createElement("div");an.setAttribute("oninput","return;"),tn="function"===typeof an.oninput}en=tn}else en=!1;Xa=en&&(!document.documentMode||9<document.documentMode)}function nn(){Ya&&(Ya.detachEvent("onpropertychange",on),qa=Ya=null)}function on(e){if("value"===e.propertyName&&$a(qa)){var t=[];if(Ja(t,qa,e,Fe(e)),e=Za,De)e(t);else{De=!0;try{He(e,t)}finally{De=!1,Ue()}}}}function rn(e,t,a){"focusin"===e?(nn(),qa=a,(Ya=t).attachEvent("onpropertychange",on)):"focusout"===e&&nn()}function dn(e){if("selectionchange"===e||"keyup"===e||"keydown"===e)return $a(qa)}function sn(e,t){if("click"===e)return $a(t)}function cn(e,t){if("input"===e||"change"===e)return $a(t)}var ln="function"===typeof Object.is?Object.is:function(e,t){return e===t&&(0!==e||1/e===1/t)||e!==e&&t!==t},un=Object.prototype.hasOwnProperty;function pn(e,t){if(ln(e,t))return!0;if("object"!==typeof e||null===e||"object"!==typeof t||null===t)return!1;var a=Object.keys(e),n=Object.keys(t);if(a.length!==n.length)return!1;for(n=0;n<a.length;n++)if(!un.call(t,a[n])||!ln(e[a[n]],t[a[n]]))return!1;return!0}function fn(e){for(;e&&e.firstChild;)e=e.firstChild;return e}function hn(e,t){var a,n=fn(e);for(e=0;n;){if(3===n.nodeType){if(a=e+n.textContent.length,e<=t&&a>=t)return{node:n,offset:t-e};e=a}e:{for(;n;){if(n.nextSibling){n=n.nextSibling;break e}n=n.parentNode}n=void 0}n=fn(n)}}function bn(e,t){return!(!e||!t)&&(e===t||(!e||3!==e.nodeType)&&(t&&3===t.nodeType?bn(e,t.parentNode):"contains"in e?e.contains(t):!!e.compareDocumentPosition&&!!(16&e.compareDocumentPosition(t))))}function gn(){for(var e=window,t=Q();t instanceof e.HTMLIFrameElement;){try{var a="string"===typeof t.contentWindow.location.href}catch(n){a=!1}if(!a)break;t=Q((e=t.contentWindow).document)}return t}function mn(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t&&("input"===t&&("text"===e.type||"search"===e.type||"tel"===e.type||"url"===e.type||"password"===e.type)||"textarea"===t||"true"===e.contentEditable)}var yn=u&&"documentMode"in document&&11>=document.documentMode,vn=null,wn=null,xn=null,kn=!1;function Rn(e,t,a){var n=a.window===a?a.document:9===a.nodeType?a:a.ownerDocument;kn||null==vn||vn!==Q(n)||("selectionStart"in(n=vn)&&mn(n)?n={start:n.selectionStart,end:n.selectionEnd}:n={anchorNode:(n=(n.ownerDocument&&n.ownerDocument.defaultView||window).getSelection()).anchorNode,anchorOffset:n.anchorOffset,focusNode:n.focusNode,focusOffset:n.focusOffset},xn&&pn(xn,n)||(xn=n,0<(n=Dn(wn,"onSelect")).length&&(t=new pa("onSelect","select",null,t,a),e.push({event:t,listeners:n}),t.target=vn)))}zt("cancel cancel click click close close contextmenu contextMenu copy copy cut cut auxclick auxClick dblclick doubleClick dragend dragEnd dragstart dragStart drop drop focusin focus focusout blur input input invalid invalid keydown keyDown keypress keyPress keyup keyUp mousedown mouseDown mouseup mouseUp paste paste pause pause play play pointercancel pointerCancel pointerdown pointerDown pointerup pointerUp ratechange rateChange reset reset seeked seeked submit submit touchcancel touchCancel touchend touchEnd touchstart touchStart volumechange volumeChange".split(" "),0),zt("drag drag dragenter dragEnter dragexit dragExit dragleave dragLeave dragover dragOver mousemove mouseMove mouseout mouseOut mouseover mouseOver pointermove pointerMove pointerout pointerOut pointerover pointerOver scroll scroll toggle toggle touchmove touchMove wheel wheel".split(" "),1),zt(_t,2);for(var In="change selectionchange textInput compositionstart compositionend compositionupdate".split(" "),An=0;An<In.length;An++)Ht.set(In[An],0);l("onMouseEnter",["mouseout","mouseover"]),l("onMouseLeave",["mouseout","mouseover"]),l("onPointerEnter",["pointerout","pointerover"]),l("onPointerLeave",["pointerout","pointerover"]),c("onChange","change click focusin focusout input keydown keyup selectionchange".split(" ")),c("onSelect","focusout contextmenu dragend focusin keydown keyup mousedown mouseup selectionchange".split(" ")),c("onBeforeInput",["compositionend","keypress","textInput","paste"]),c("onCompositionEnd","compositionend focusout keydown keypress keyup mousedown".split(" ")),c("onCompositionStart","compositionstart focusout keydown keypress keyup mousedown".split(" ")),c("onCompositionUpdate","compositionupdate focusout keydown keypress keyup mousedown".split(" "));var Fn="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),Tn=new Set("cancel close invalid load scroll toggle".split(" ").concat(Fn));function Sn(e,t,a){var n=e.type||"unknown-event";e.currentTarget=a,function(e,t,a,n,o,i,d,s,c){if(qe.apply(this,arguments),Ke){if(!Ke)throw Error(r(198));var l=Ge;Ke=!1,Ge=null,Ve||(Ve=!0,Je=l)}}(n,t,void 0,e),e.currentTarget=null}function Cn(e,t){t=0!==(4&t);for(var a=0;a<e.length;a++){var n=e[a],o=n.event;n=n.listeners;e:{var i=void 0;if(t)for(var r=n.length-1;0<=r;r--){var d=n[r],s=d.instance,c=d.currentTarget;if(d=d.listener,s!==i&&o.isPropagationStopped())break e;Sn(o,d,c),i=s}else for(r=0;r<n.length;r++){if(s=(d=n[r]).instance,c=d.currentTarget,d=d.listener,s!==i&&o.isPropagationStopped())break e;Sn(o,d,c),i=s}}}if(Ve)throw e=Je,Ve=!1,Je=null,e}function En(e,t){var a=ro(t),n=e+"__bubble";a.has(n)||(_n(t,e,2,!1),a.add(n))}var Mn="_reactListening"+Math.random().toString(36).slice(2);function Pn(e){e[Mn]||(e[Mn]=!0,d.forEach((function(t){Tn.has(t)||Hn(t,!1,e,null),Hn(t,!0,e,null)})))}function Hn(e,t,a,n){var o=4<arguments.length&&void 0!==arguments[4]?arguments[4]:0,i=a;if("selectionchange"===e&&9!==a.nodeType&&(i=a.ownerDocument),null!==n&&!t&&Tn.has(e)){if("scroll"!==e)return;o|=2,i=n}var r=ro(i),d=e+"__"+(t?"capture":"bubble");r.has(d)||(t&&(o|=4),_n(i,e,o,t),r.add(d))}function _n(e,t,a,n){var o=Ht.get(t);switch(void 0===o?2:o){case 0:o=Zt;break;case 1:o=$t;break;default:o=Qt}a=o.bind(null,t,a,e),o=void 0,!Ne||"touchstart"!==t&&"touchmove"!==t&&"wheel"!==t||(o=!0),n?void 0!==o?e.addEventListener(t,a,{capture:!0,passive:o}):e.addEventListener(t,a,!0):void 0!==o?e.addEventListener(t,a,{passive:o}):e.addEventListener(t,a,!1)}function zn(e,t,a,n,o){var i=n;if(0===(1&t)&&0===(2&t)&&null!==n)e:for(;;){if(null===n)return;var r=n.tag;if(3===r||4===r){var d=n.stateNode.containerInfo;if(d===o||8===d.nodeType&&d.parentNode===o)break;if(4===r)for(r=n.return;null!==r;){var s=r.tag;if((3===s||4===s)&&((s=r.stateNode.containerInfo)===o||8===s.nodeType&&s.parentNode===o))return;r=r.return}for(;null!==d;){if(null===(r=ao(d)))return;if(5===(s=r.tag)||6===s){n=i=r;continue e}d=d.parentNode}}n=n.return}!function(e,t,a){if(Le)return e(t,a);Le=!0;try{je(e,t,a)}finally{Le=!1,Ue()}}((function(){var n=i,o=Fe(a),r=[];e:{var d=Pt.get(e);if(void 0!==d){var s=pa,c=e;switch(e){case"keypress":if(0===oa(a))break e;case"keydown":case"keyup":s=Ca;break;case"focusin":c="focus",s=ya;break;case"focusout":c="blur",s=ya;break;case"beforeblur":case"afterblur":s=ya;break;case"click":if(2===a.button)break e;case"auxclick":case"dblclick":case"mousedown":case"mousemove":case"mouseup":case"mouseout":case"mouseover":case"contextmenu":s=ga;break;case"drag":case"dragend":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"dragstart":case"drop":s=ma;break;case"touchcancel":case"touchend":case"touchmove":case"touchstart":s=Ma;break;case St:case Ct:case Et:s=va;break;case Mt:s=Pa;break;case"scroll":s=ha;break;case"wheel":s=_a;break;case"copy":case"cut":case"paste":s=xa;break;case"gotpointercapture":case"lostpointercapture":case"pointercancel":case"pointerdown":case"pointermove":case"pointerout":case"pointerover":case"pointerup":s=Ea}var l=0!==(4&t),u=!l&&"scroll"===e,p=l?null!==d?d+"Capture":null:d;l=[];for(var f,h=n;null!==h;){var b=(f=h).stateNode;if(5===f.tag&&null!==b&&(f=b,null!==p&&(null!=(b=Oe(h,p))&&l.push(jn(h,b,f)))),u)break;h=h.return}0<l.length&&(d=new s(d,c,null,a,o),r.push({event:d,listeners:l}))}}if(0===(7&t)){if(s="mouseout"===e||"pointerout"===e,(!(d="mouseover"===e||"pointerover"===e)||0!==(16&t)||!(c=a.relatedTarget||a.fromElement)||!ao(c)&&!c[eo])&&(s||d)&&(d=o.window===o?o:(d=o.ownerDocument)?d.defaultView||d.parentWindow:window,s?(s=n,null!==(c=(c=a.relatedTarget||a.toElement)?ao(c):null)&&(c!==(u=Ze(c))||5!==c.tag&&6!==c.tag)&&(c=null)):(s=null,c=n),s!==c)){if(l=ga,b="onMouseLeave",p="onMouseEnter",h="mouse","pointerout"!==e&&"pointerover"!==e||(l=Ea,b="onPointerLeave",p="onPointerEnter",h="pointer"),u=null==s?d:oo(s),f=null==c?d:oo(c),(d=new l(b,h+"leave",s,a,o)).target=u,d.relatedTarget=f,b=null,ao(o)===n&&((l=new l(p,h+"enter",c,a,o)).target=f,l.relatedTarget=u,b=l),u=b,s&&c)e:{for(p=c,h=0,f=l=s;f;f=Ln(f))h++;for(f=0,b=p;b;b=Ln(b))f++;for(;0<h-f;)l=Ln(l),h--;for(;0<f-h;)p=Ln(p),f--;for(;h--;){if(l===p||null!==p&&l===p.alternate)break e;l=Ln(l),p=Ln(p)}l=null}else l=null;null!==s&&Un(r,d,s,l,!1),null!==c&&null!==u&&Un(r,u,c,l,!0)}if("select"===(s=(d=n?oo(n):window).nodeName&&d.nodeName.toLowerCase())||"input"===s&&"file"===d.type)var g=Qa;else if(Va(d))if(Xa)g=cn;else{g=dn;var m=rn}else(s=d.nodeName)&&"input"===s.toLowerCase()&&("checkbox"===d.type||"radio"===d.type)&&(g=sn);switch(g&&(g=g(e,n))?Ja(r,g,a,o):(m&&m(e,d,n),"focusout"===e&&(m=d._wrapperState)&&m.controlled&&"number"===d.type&&oe(d,"number",d.value)),m=n?oo(n):window,e){case"focusin":(Va(m)||"true"===m.contentEditable)&&(vn=m,wn=n,xn=null);break;case"focusout":xn=wn=vn=null;break;case"mousedown":kn=!0;break;case"contextmenu":case"mouseup":case"dragend":kn=!1,Rn(r,a,o);break;case"selectionchange":if(yn)break;case"keydown":case"keyup":Rn(r,a,o)}var y;if(ja)e:{switch(e){case"compositionstart":var v="onCompositionStart";break e;case"compositionend":v="onCompositionEnd";break e;case"compositionupdate":v="onCompositionUpdate";break e}v=void 0}else Ka?Wa(e,a)&&(v="onCompositionEnd"):"keydown"===e&&229===a.keyCode&&(v="onCompositionStart");v&&(Ua&&"ko"!==a.locale&&(Ka||"onCompositionStart"!==v?"onCompositionEnd"===v&&Ka&&(y=na()):(ta="value"in(ea=o)?ea.value:ea.textContent,Ka=!0)),0<(m=Dn(n,v)).length&&(v=new ka(v,e,null,a,o),r.push({event:v,listeners:m}),y?v.data=y:null!==(y=Ba(a))&&(v.data=y))),(y=La?function(e,t){switch(e){case"compositionend":return Ba(t);case"keypress":return 32!==t.which?null:(Na=!0,Oa);case"textInput":return(e=t.data)===Oa&&Na?null:e;default:return null}}(e,a):function(e,t){if(Ka)return"compositionend"===e||!ja&&Wa(e,t)?(e=na(),aa=ta=ea=null,Ka=!1,e):null;switch(e){case"paste":default:return null;case"keypress":if(!(t.ctrlKey||t.altKey||t.metaKey)||t.ctrlKey&&t.altKey){if(t.char&&1<t.char.length)return t.char;if(t.which)return String.fromCharCode(t.which)}return null;case"compositionend":return Ua&&"ko"!==t.locale?null:t.data}}(e,a))&&(0<(n=Dn(n,"onBeforeInput")).length&&(o=new ka("onBeforeInput","beforeinput",null,a,o),r.push({event:o,listeners:n}),o.data=y))}Cn(r,t)}))}function jn(e,t,a){return{instance:e,listener:t,currentTarget:a}}function Dn(e,t){for(var a=t+"Capture",n=[];null!==e;){var o=e,i=o.stateNode;5===o.tag&&null!==i&&(o=i,null!=(i=Oe(e,a))&&n.unshift(jn(e,i,o)),null!=(i=Oe(e,t))&&n.push(jn(e,i,o))),e=e.return}return n}function Ln(e){if(null===e)return null;do{e=e.return}while(e&&5!==e.tag);return e||null}function Un(e,t,a,n,o){for(var i=t._reactName,r=[];null!==a&&a!==n;){var d=a,s=d.alternate,c=d.stateNode;if(null!==s&&s===n)break;5===d.tag&&null!==c&&(d=c,o?null!=(s=Oe(a,i))&&r.unshift(jn(a,s,d)):o||null!=(s=Oe(a,i))&&r.push(jn(a,s,d))),a=a.return}0!==r.length&&e.push({event:t,listeners:r})}function On(){}var Nn=null,Wn=null;function Bn(e,t){switch(e){case"button":case"input":case"select":case"textarea":return!!t.autoFocus}return!1}function Kn(e,t){return"textarea"===e||"option"===e||"noscript"===e||"string"===typeof t.children||"number"===typeof t.children||"object"===typeof t.dangerouslySetInnerHTML&&null!==t.dangerouslySetInnerHTML&&null!=t.dangerouslySetInnerHTML.__html}var Gn="function"===typeof setTimeout?setTimeout:void 0,Vn="function"===typeof clearTimeout?clearTimeout:void 0;function Jn(e){1===e.nodeType?e.textContent="":9===e.nodeType&&(null!=(e=e.body)&&(e.textContent=""))}function Yn(e){for(;null!=e;e=e.nextSibling){var t=e.nodeType;if(1===t||3===t)break}return e}function qn(e){e=e.previousSibling;for(var t=0;e;){if(8===e.nodeType){var a=e.data;if("$"===a||"$!"===a||"$?"===a){if(0===t)return e;t--}else"/$"===a&&t++}e=e.previousSibling}return null}var Zn=0;var $n=Math.random().toString(36).slice(2),Qn="__reactFiber$"+$n,Xn="__reactProps$"+$n,eo="__reactContainer$"+$n,to="__reactEvents$"+$n;function ao(e){var t=e[Qn];if(t)return t;for(var a=e.parentNode;a;){if(t=a[eo]||a[Qn]){if(a=t.alternate,null!==t.child||null!==a&&null!==a.child)for(e=qn(e);null!==e;){if(a=e[Qn])return a;e=qn(e)}return t}a=(e=a).parentNode}return null}function no(e){return!(e=e[Qn]||e[eo])||5!==e.tag&&6!==e.tag&&13!==e.tag&&3!==e.tag?null:e}function oo(e){if(5===e.tag||6===e.tag)return e.stateNode;throw Error(r(33))}function io(e){return e[Xn]||null}function ro(e){var t=e[to];return void 0===t&&(t=e[to]=new Set),t}var so=[],co=-1;function lo(e){return{current:e}}function uo(e){0>co||(e.current=so[co],so[co]=null,co--)}function po(e,t){co++,so[co]=e.current,e.current=t}var fo={},ho=lo(fo),bo=lo(!1),go=fo;function mo(e,t){var a=e.type.contextTypes;if(!a)return fo;var n=e.stateNode;if(n&&n.__reactInternalMemoizedUnmaskedChildContext===t)return n.__reactInternalMemoizedMaskedChildContext;var o,i={};for(o in a)i[o]=t[o];return n&&((e=e.stateNode).__reactInternalMemoizedUnmaskedChildContext=t,e.__reactInternalMemoizedMaskedChildContext=i),i}function yo(e){return null!==(e=e.childContextTypes)&&void 0!==e}function vo(){uo(bo),uo(ho)}function wo(e,t,a){if(ho.current!==fo)throw Error(r(168));po(ho,t),po(bo,a)}function xo(e,t,a){var n=e.stateNode;if(e=t.childContextTypes,"function"!==typeof n.getChildContext)return a;for(var i in n=n.getChildContext())if(!(i in e))throw Error(r(108,J(t)||"Unknown",i));return o({},a,n)}function ko(e){return e=(e=e.stateNode)&&e.__reactInternalMemoizedMergedChildContext||fo,go=ho.current,po(ho,e),po(bo,bo.current),!0}function Ro(e,t,a){var n=e.stateNode;if(!n)throw Error(r(169));a?(e=xo(e,t,go),n.__reactInternalMemoizedMergedChildContext=e,uo(bo),uo(ho),po(ho,e)):uo(bo),po(bo,a)}var Io=null,Ao=null,Fo=i.unstable_runWithPriority,To=i.unstable_scheduleCallback,So=i.unstable_cancelCallback,Co=i.unstable_shouldYield,Eo=i.unstable_requestPaint,Mo=i.unstable_now,Po=i.unstable_getCurrentPriorityLevel,Ho=i.unstable_ImmediatePriority,_o=i.unstable_UserBlockingPriority,zo=i.unstable_NormalPriority,jo=i.unstable_LowPriority,Do=i.unstable_IdlePriority,Lo={},Uo=void 0!==Eo?Eo:function(){},Oo=null,No=null,Wo=!1,Bo=Mo(),Ko=1e4>Bo?Mo:function(){return Mo()-Bo};function Go(){switch(Po()){case Ho:return 99;case _o:return 98;case zo:return 97;case jo:return 96;case Do:return 95;default:throw Error(r(332))}}function Vo(e){switch(e){case 99:return Ho;case 98:return _o;case 97:return zo;case 96:return jo;case 95:return Do;default:throw Error(r(332))}}function Jo(e,t){return e=Vo(e),Fo(e,t)}function Yo(e,t,a){return e=Vo(e),To(e,t,a)}function qo(){if(null!==No){var e=No;No=null,So(e)}Zo()}function Zo(){if(!Wo&&null!==Oo){Wo=!0;var e=0;try{var t=Oo;Jo(99,(function(){for(;e<t.length;e++){var a=t[e];do{a=a(!0)}while(null!==a)}})),Oo=null}catch(a){throw null!==Oo&&(Oo=Oo.slice(e+1)),To(Ho,qo),a}finally{Wo=!1}}}var $o=x.ReactCurrentBatchConfig;function Qo(e,t){if(e&&e.defaultProps){for(var a in t=o({},t),e=e.defaultProps)void 0===t[a]&&(t[a]=e[a]);return t}return t}var Xo=lo(null),ei=null,ti=null,ai=null;function ni(){ai=ti=ei=null}function oi(e){var t=Xo.current;uo(Xo),e.type._context._currentValue=t}function ii(e,t){for(;null!==e;){var a=e.alternate;if((e.childLanes&t)===t){if(null===a||(a.childLanes&t)===t)break;a.childLanes|=t}else e.childLanes|=t,null!==a&&(a.childLanes|=t);e=e.return}}function ri(e,t){ei=e,ai=ti=null,null!==(e=e.dependencies)&&null!==e.firstContext&&(0!==(e.lanes&t)&&(Dr=!0),e.firstContext=null)}function di(e,t){if(ai!==e&&!1!==t&&0!==t)if("number"===typeof t&&1073741823!==t||(ai=e,t=1073741823),t={context:e,observedBits:t,next:null},null===ti){if(null===ei)throw Error(r(308));ti=t,ei.dependencies={lanes:0,firstContext:t,responders:null}}else ti=ti.next=t;return e._currentValue}var si=!1;function ci(e){e.updateQueue={baseState:e.memoizedState,firstBaseUpdate:null,lastBaseUpdate:null,shared:{pending:null},effects:null}}function li(e,t){e=e.updateQueue,t.updateQueue===e&&(t.updateQueue={baseState:e.baseState,firstBaseUpdate:e.firstBaseUpdate,lastBaseUpdate:e.lastBaseUpdate,shared:e.shared,effects:e.effects})}function ui(e,t){return{eventTime:e,lane:t,tag:0,payload:null,callback:null,next:null}}function pi(e,t){if(null!==(e=e.updateQueue)){var a=(e=e.shared).pending;null===a?t.next=t:(t.next=a.next,a.next=t),e.pending=t}}function fi(e,t){var a=e.updateQueue,n=e.alternate;if(null!==n&&a===(n=n.updateQueue)){var o=null,i=null;if(null!==(a=a.firstBaseUpdate)){do{var r={eventTime:a.eventTime,lane:a.lane,tag:a.tag,payload:a.payload,callback:a.callback,next:null};null===i?o=i=r:i=i.next=r,a=a.next}while(null!==a);null===i?o=i=t:i=i.next=t}else o=i=t;return a={baseState:n.baseState,firstBaseUpdate:o,lastBaseUpdate:i,shared:n.shared,effects:n.effects},void(e.updateQueue=a)}null===(e=a.lastBaseUpdate)?a.firstBaseUpdate=t:e.next=t,a.lastBaseUpdate=t}function hi(e,t,a,n){var i=e.updateQueue;si=!1;var r=i.firstBaseUpdate,d=i.lastBaseUpdate,s=i.shared.pending;if(null!==s){i.shared.pending=null;var c=s,l=c.next;c.next=null,null===d?r=l:d.next=l,d=c;var u=e.alternate;if(null!==u){var p=(u=u.updateQueue).lastBaseUpdate;p!==d&&(null===p?u.firstBaseUpdate=l:p.next=l,u.lastBaseUpdate=c)}}if(null!==r){for(p=i.baseState,d=0,u=l=c=null;;){s=r.lane;var f=r.eventTime;if((n&s)===s){null!==u&&(u=u.next={eventTime:f,lane:0,tag:r.tag,payload:r.payload,callback:r.callback,next:null});e:{var h=e,b=r;switch(s=t,f=a,b.tag){case 1:if("function"===typeof(h=b.payload)){p=h.call(f,p,s);break e}p=h;break e;case 3:h.flags=-4097&h.flags|64;case 0:if(null===(s="function"===typeof(h=b.payload)?h.call(f,p,s):h)||void 0===s)break e;p=o({},p,s);break e;case 2:si=!0}}null!==r.callback&&(e.flags|=32,null===(s=i.effects)?i.effects=[r]:s.push(r))}else f={eventTime:f,lane:s,tag:r.tag,payload:r.payload,callback:r.callback,next:null},null===u?(l=u=f,c=p):u=u.next=f,d|=s;if(null===(r=r.next)){if(null===(s=i.shared.pending))break;r=s.next,s.next=null,i.lastBaseUpdate=s,i.shared.pending=null}}null===u&&(c=p),i.baseState=c,i.firstBaseUpdate=l,i.lastBaseUpdate=u,Nd|=d,e.lanes=d,e.memoizedState=p}}function bi(e,t,a){if(e=t.effects,t.effects=null,null!==e)for(t=0;t<e.length;t++){var n=e[t],o=n.callback;if(null!==o){if(n.callback=null,n=a,"function"!==typeof o)throw Error(r(191,o));o.call(n)}}}var gi=(new n.Component).refs;function mi(e,t,a,n){a=null===(a=a(n,t=e.memoizedState))||void 0===a?t:o({},t,a),e.memoizedState=a,0===e.lanes&&(e.updateQueue.baseState=a)}var yi={isMounted:function(e){return!!(e=e._reactInternals)&&Ze(e)===e},enqueueSetState:function(e,t,a){e=e._reactInternals;var n=ps(),o=fs(e),i=ui(n,o);i.payload=t,void 0!==a&&null!==a&&(i.callback=a),pi(e,i),hs(e,o,n)},enqueueReplaceState:function(e,t,a){e=e._reactInternals;var n=ps(),o=fs(e),i=ui(n,o);i.tag=1,i.payload=t,void 0!==a&&null!==a&&(i.callback=a),pi(e,i),hs(e,o,n)},enqueueForceUpdate:function(e,t){e=e._reactInternals;var a=ps(),n=fs(e),o=ui(a,n);o.tag=2,void 0!==t&&null!==t&&(o.callback=t),pi(e,o),hs(e,n,a)}};function vi(e,t,a,n,o,i,r){return"function"===typeof(e=e.stateNode).shouldComponentUpdate?e.shouldComponentUpdate(n,i,r):!t.prototype||!t.prototype.isPureReactComponent||(!pn(a,n)||!pn(o,i))}function wi(e,t,a){var n=!1,o=fo,i=t.contextType;return"object"===typeof i&&null!==i?i=di(i):(o=yo(t)?go:ho.current,i=(n=null!==(n=t.contextTypes)&&void 0!==n)?mo(e,o):fo),t=new t(a,i),e.memoizedState=null!==t.state&&void 0!==t.state?t.state:null,t.updater=yi,e.stateNode=t,t._reactInternals=e,n&&((e=e.stateNode).__reactInternalMemoizedUnmaskedChildContext=o,e.__reactInternalMemoizedMaskedChildContext=i),t}function xi(e,t,a,n){e=t.state,"function"===typeof t.componentWillReceiveProps&&t.componentWillReceiveProps(a,n),"function"===typeof t.UNSAFE_componentWillReceiveProps&&t.UNSAFE_componentWillReceiveProps(a,n),t.state!==e&&yi.enqueueReplaceState(t,t.state,null)}function ki(e,t,a,n){var o=e.stateNode;o.props=a,o.state=e.memoizedState,o.refs=gi,ci(e);var i=t.contextType;"object"===typeof i&&null!==i?o.context=di(i):(i=yo(t)?go:ho.current,o.context=mo(e,i)),hi(e,a,o,n),o.state=e.memoizedState,"function"===typeof(i=t.getDerivedStateFromProps)&&(mi(e,t,i,a),o.state=e.memoizedState),"function"===typeof t.getDerivedStateFromProps||"function"===typeof o.getSnapshotBeforeUpdate||"function"!==typeof o.UNSAFE_componentWillMount&&"function"!==typeof o.componentWillMount||(t=o.state,"function"===typeof o.componentWillMount&&o.componentWillMount(),"function"===typeof o.UNSAFE_componentWillMount&&o.UNSAFE_componentWillMount(),t!==o.state&&yi.enqueueReplaceState(o,o.state,null),hi(e,a,o,n),o.state=e.memoizedState),"function"===typeof o.componentDidMount&&(e.flags|=4)}var Ri=Array.isArray;function Ii(e,t,a){if(null!==(e=a.ref)&&"function"!==typeof e&&"object"!==typeof e){if(a._owner){if(a=a._owner){if(1!==a.tag)throw Error(r(309));var n=a.stateNode}if(!n)throw Error(r(147,e));var o=""+e;return null!==t&&null!==t.ref&&"function"===typeof t.ref&&t.ref._stringRef===o?t.ref:(t=function(e){var t=n.refs;t===gi&&(t=n.refs={}),null===e?delete t[o]:t[o]=e},t._stringRef=o,t)}if("string"!==typeof e)throw Error(r(284));if(!a._owner)throw Error(r(290,e))}return e}function Ai(e,t){if("textarea"!==e.type)throw Error(r(31,"[object Object]"===Object.prototype.toString.call(t)?"object with keys {"+Object.keys(t).join(", ")+"}":t))}function Fi(e){function t(t,a){if(e){var n=t.lastEffect;null!==n?(n.nextEffect=a,t.lastEffect=a):t.firstEffect=t.lastEffect=a,a.nextEffect=null,a.flags=8}}function a(a,n){if(!e)return null;for(;null!==n;)t(a,n),n=n.sibling;return null}function n(e,t){for(e=new Map;null!==t;)null!==t.key?e.set(t.key,t):e.set(t.index,t),t=t.sibling;return e}function o(e,t){return(e=Vs(e,t)).index=0,e.sibling=null,e}function i(t,a,n){return t.index=n,e?null!==(n=t.alternate)?(n=n.index)<a?(t.flags=2,a):n:(t.flags=2,a):a}function d(t){return e&&null===t.alternate&&(t.flags=2),t}function s(e,t,a,n){return null===t||6!==t.tag?((t=Zs(a,e.mode,n)).return=e,t):((t=o(t,a)).return=e,t)}function c(e,t,a,n){return null!==t&&t.elementType===a.type?((n=o(t,a.props)).ref=Ii(e,t,a),n.return=e,n):((n=Js(a.type,a.key,a.props,null,e.mode,n)).ref=Ii(e,t,a),n.return=e,n)}function l(e,t,a,n){return null===t||4!==t.tag||t.stateNode.containerInfo!==a.containerInfo||t.stateNode.implementation!==a.implementation?((t=$s(a,e.mode,n)).return=e,t):((t=o(t,a.children||[])).return=e,t)}function u(e,t,a,n,i){return null===t||7!==t.tag?((t=Ys(a,e.mode,n,i)).return=e,t):((t=o(t,a)).return=e,t)}function p(e,t,a){if("string"===typeof t||"number"===typeof t)return(t=Zs(""+t,e.mode,a)).return=e,t;if("object"===typeof t&&null!==t){switch(t.$$typeof){case k:return(a=Js(t.type,t.key,t.props,null,e.mode,a)).ref=Ii(e,null,t),a.return=e,a;case R:return(t=$s(t,e.mode,a)).return=e,t}if(Ri(t)||W(t))return(t=Ys(t,e.mode,a,null)).return=e,t;Ai(e,t)}return null}function f(e,t,a,n){var o=null!==t?t.key:null;if("string"===typeof a||"number"===typeof a)return null!==o?null:s(e,t,""+a,n);if("object"===typeof a&&null!==a){switch(a.$$typeof){case k:return a.key===o?a.type===I?u(e,t,a.props.children,n,o):c(e,t,a,n):null;case R:return a.key===o?l(e,t,a,n):null}if(Ri(a)||W(a))return null!==o?null:u(e,t,a,n,null);Ai(e,a)}return null}function h(e,t,a,n,o){if("string"===typeof n||"number"===typeof n)return s(t,e=e.get(a)||null,""+n,o);if("object"===typeof n&&null!==n){switch(n.$$typeof){case k:return e=e.get(null===n.key?a:n.key)||null,n.type===I?u(t,e,n.props.children,o,n.key):c(t,e,n,o);case R:return l(t,e=e.get(null===n.key?a:n.key)||null,n,o)}if(Ri(n)||W(n))return u(t,e=e.get(a)||null,n,o,null);Ai(t,n)}return null}function b(o,r,d,s){for(var c=null,l=null,u=r,b=r=0,g=null;null!==u&&b<d.length;b++){u.index>b?(g=u,u=null):g=u.sibling;var m=f(o,u,d[b],s);if(null===m){null===u&&(u=g);break}e&&u&&null===m.alternate&&t(o,u),r=i(m,r,b),null===l?c=m:l.sibling=m,l=m,u=g}if(b===d.length)return a(o,u),c;if(null===u){for(;b<d.length;b++)null!==(u=p(o,d[b],s))&&(r=i(u,r,b),null===l?c=u:l.sibling=u,l=u);return c}for(u=n(o,u);b<d.length;b++)null!==(g=h(u,o,b,d[b],s))&&(e&&null!==g.alternate&&u.delete(null===g.key?b:g.key),r=i(g,r,b),null===l?c=g:l.sibling=g,l=g);return e&&u.forEach((function(e){return t(o,e)})),c}function g(o,d,s,c){var l=W(s);if("function"!==typeof l)throw Error(r(150));if(null==(s=l.call(s)))throw Error(r(151));for(var u=l=null,b=d,g=d=0,m=null,y=s.next();null!==b&&!y.done;g++,y=s.next()){b.index>g?(m=b,b=null):m=b.sibling;var v=f(o,b,y.value,c);if(null===v){null===b&&(b=m);break}e&&b&&null===v.alternate&&t(o,b),d=i(v,d,g),null===u?l=v:u.sibling=v,u=v,b=m}if(y.done)return a(o,b),l;if(null===b){for(;!y.done;g++,y=s.next())null!==(y=p(o,y.value,c))&&(d=i(y,d,g),null===u?l=y:u.sibling=y,u=y);return l}for(b=n(o,b);!y.done;g++,y=s.next())null!==(y=h(b,o,g,y.value,c))&&(e&&null!==y.alternate&&b.delete(null===y.key?g:y.key),d=i(y,d,g),null===u?l=y:u.sibling=y,u=y);return e&&b.forEach((function(e){return t(o,e)})),l}return function(e,n,i,s){var c="object"===typeof i&&null!==i&&i.type===I&&null===i.key;c&&(i=i.props.children);var l="object"===typeof i&&null!==i;if(l)switch(i.$$typeof){case k:e:{for(l=i.key,c=n;null!==c;){if(c.key===l){if(7===c.tag){if(i.type===I){a(e,c.sibling),(n=o(c,i.props.children)).return=e,e=n;break e}}else if(c.elementType===i.type){a(e,c.sibling),(n=o(c,i.props)).ref=Ii(e,c,i),n.return=e,e=n;break e}a(e,c);break}t(e,c),c=c.sibling}i.type===I?((n=Ys(i.props.children,e.mode,s,i.key)).return=e,e=n):((s=Js(i.type,i.key,i.props,null,e.mode,s)).ref=Ii(e,n,i),s.return=e,e=s)}return d(e);case R:e:{for(c=i.key;null!==n;){if(n.key===c){if(4===n.tag&&n.stateNode.containerInfo===i.containerInfo&&n.stateNode.implementation===i.implementation){a(e,n.sibling),(n=o(n,i.children||[])).return=e,e=n;break e}a(e,n);break}t(e,n),n=n.sibling}(n=$s(i,e.mode,s)).return=e,e=n}return d(e)}if("string"===typeof i||"number"===typeof i)return i=""+i,null!==n&&6===n.tag?(a(e,n.sibling),(n=o(n,i)).return=e,e=n):(a(e,n),(n=Zs(i,e.mode,s)).return=e,e=n),d(e);if(Ri(i))return b(e,n,i,s);if(W(i))return g(e,n,i,s);if(l&&Ai(e,i),"undefined"===typeof i&&!c)switch(e.tag){case 1:case 22:case 0:case 11:case 15:throw Error(r(152,J(e.type)||"Component"))}return a(e,n)}}var Ti=Fi(!0),Si=Fi(!1),Ci={},Ei=lo(Ci),Mi=lo(Ci),Pi=lo(Ci);function Hi(e){if(e===Ci)throw Error(r(174));return e}function _i(e,t){switch(po(Pi,t),po(Mi,e),po(Ei,Ci),e=t.nodeType){case 9:case 11:t=(t=t.documentElement)?t.namespaceURI:he(null,"");break;default:t=he(t=(e=8===e?t.parentNode:t).namespaceURI||null,e=e.tagName)}uo(Ei),po(Ei,t)}function zi(){uo(Ei),uo(Mi),uo(Pi)}function ji(e){Hi(Pi.current);var t=Hi(Ei.current),a=he(t,e.type);t!==a&&(po(Mi,e),po(Ei,a))}function Di(e){Mi.current===e&&(uo(Ei),uo(Mi))}var Li=lo(0);function Ui(e){for(var t=e;null!==t;){if(13===t.tag){var a=t.memoizedState;if(null!==a&&(null===(a=a.dehydrated)||"$?"===a.data||"$!"===a.data))return t}else if(19===t.tag&&void 0!==t.memoizedProps.revealOrder){if(0!==(64&t.flags))return t}else if(null!==t.child){t.child.return=t,t=t.child;continue}if(t===e)break;for(;null===t.sibling;){if(null===t.return||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}return null}var Oi=null,Ni=null,Wi=!1;function Bi(e,t){var a=Ks(5,null,null,0);a.elementType="DELETED",a.type="DELETED",a.stateNode=t,a.return=e,a.flags=8,null!==e.lastEffect?(e.lastEffect.nextEffect=a,e.lastEffect=a):e.firstEffect=e.lastEffect=a}function Ki(e,t){switch(e.tag){case 5:var a=e.type;return null!==(t=1!==t.nodeType||a.toLowerCase()!==t.nodeName.toLowerCase()?null:t)&&(e.stateNode=t,!0);case 6:return null!==(t=""===e.pendingProps||3!==t.nodeType?null:t)&&(e.stateNode=t,!0);default:return!1}}function Gi(e){if(Wi){var t=Ni;if(t){var a=t;if(!Ki(e,t)){if(!(t=Yn(a.nextSibling))||!Ki(e,t))return e.flags=-1025&e.flags|2,Wi=!1,void(Oi=e);Bi(Oi,a)}Oi=e,Ni=Yn(t.firstChild)}else e.flags=-1025&e.flags|2,Wi=!1,Oi=e}}function Vi(e){for(e=e.return;null!==e&&5!==e.tag&&3!==e.tag&&13!==e.tag;)e=e.return;Oi=e}function Ji(e){if(e!==Oi)return!1;if(!Wi)return Vi(e),Wi=!0,!1;var t=e.type;if(5!==e.tag||"head"!==t&&"body"!==t&&!Kn(t,e.memoizedProps))for(t=Ni;t;)Bi(e,t),t=Yn(t.nextSibling);if(Vi(e),13===e.tag){if(!(e=null!==(e=e.memoizedState)?e.dehydrated:null))throw Error(r(317));e:{for(e=e.nextSibling,t=0;e;){if(8===e.nodeType){var a=e.data;if("/$"===a){if(0===t){Ni=Yn(e.nextSibling);break e}t--}else"$"!==a&&"$!"!==a&&"$?"!==a||t++}e=e.nextSibling}Ni=null}}else Ni=Oi?Yn(e.stateNode.nextSibling):null;return!0}function Yi(){Ni=Oi=null,Wi=!1}var qi=[];function Zi(){for(var e=0;e<qi.length;e++)qi[e]._workInProgressVersionPrimary=null;qi.length=0}var $i=x.ReactCurrentDispatcher,Qi=x.ReactCurrentBatchConfig,Xi=0,er=null,tr=null,ar=null,nr=!1,or=!1;function ir(){throw Error(r(321))}function rr(e,t){if(null===t)return!1;for(var a=0;a<t.length&&a<e.length;a++)if(!ln(e[a],t[a]))return!1;return!0}function dr(e,t,a,n,o,i){if(Xi=i,er=t,t.memoizedState=null,t.updateQueue=null,t.lanes=0,$i.current=null===e||null===e.memoizedState?Hr:_r,e=a(n,o),or){i=0;do{if(or=!1,!(25>i))throw Error(r(301));i+=1,ar=tr=null,t.updateQueue=null,$i.current=zr,e=a(n,o)}while(or)}if($i.current=Pr,t=null!==tr&&null!==tr.next,Xi=0,ar=tr=er=null,nr=!1,t)throw Error(r(300));return e}function sr(){var e={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};return null===ar?er.memoizedState=ar=e:ar=ar.next=e,ar}function cr(){if(null===tr){var e=er.alternate;e=null!==e?e.memoizedState:null}else e=tr.next;var t=null===ar?er.memoizedState:ar.next;if(null!==t)ar=t,tr=e;else{if(null===e)throw Error(r(310));e={memoizedState:(tr=e).memoizedState,baseState:tr.baseState,baseQueue:tr.baseQueue,queue:tr.queue,next:null},null===ar?er.memoizedState=ar=e:ar=ar.next=e}return ar}function lr(e,t){return"function"===typeof t?t(e):t}function ur(e){var t=cr(),a=t.queue;if(null===a)throw Error(r(311));a.lastRenderedReducer=e;var n=tr,o=n.baseQueue,i=a.pending;if(null!==i){if(null!==o){var d=o.next;o.next=i.next,i.next=d}n.baseQueue=o=i,a.pending=null}if(null!==o){o=o.next,n=n.baseState;var s=d=i=null,c=o;do{var l=c.lane;if((Xi&l)===l)null!==s&&(s=s.next={lane:0,action:c.action,eagerReducer:c.eagerReducer,eagerState:c.eagerState,next:null}),n=c.eagerReducer===e?c.eagerState:e(n,c.action);else{var u={lane:l,action:c.action,eagerReducer:c.eagerReducer,eagerState:c.eagerState,next:null};null===s?(d=s=u,i=n):s=s.next=u,er.lanes|=l,Nd|=l}c=c.next}while(null!==c&&c!==o);null===s?i=n:s.next=d,ln(n,t.memoizedState)||(Dr=!0),t.memoizedState=n,t.baseState=i,t.baseQueue=s,a.lastRenderedState=n}return[t.memoizedState,a.dispatch]}function pr(e){var t=cr(),a=t.queue;if(null===a)throw Error(r(311));a.lastRenderedReducer=e;var n=a.dispatch,o=a.pending,i=t.memoizedState;if(null!==o){a.pending=null;var d=o=o.next;do{i=e(i,d.action),d=d.next}while(d!==o);ln(i,t.memoizedState)||(Dr=!0),t.memoizedState=i,null===t.baseQueue&&(t.baseState=i),a.lastRenderedState=i}return[i,n]}function fr(e,t,a){var n=t._getVersion;n=n(t._source);var o=t._workInProgressVersionPrimary;if(null!==o?e=o===n:(e=e.mutableReadLanes,(e=(Xi&e)===e)&&(t._workInProgressVersionPrimary=n,qi.push(t))),e)return a(t._source);throw qi.push(t),Error(r(350))}function hr(e,t,a,n){var o=Hd;if(null===o)throw Error(r(349));var i=t._getVersion,d=i(t._source),s=$i.current,c=s.useState((function(){return fr(o,t,a)})),l=c[1],u=c[0];c=ar;var p=e.memoizedState,f=p.refs,h=f.getSnapshot,b=p.source;p=p.subscribe;var g=er;return e.memoizedState={refs:f,source:t,subscribe:n},s.useEffect((function(){f.getSnapshot=a,f.setSnapshot=l;var e=i(t._source);if(!ln(d,e)){e=a(t._source),ln(u,e)||(l(e),e=fs(g),o.mutableReadLanes|=e&o.pendingLanes),e=o.mutableReadLanes,o.entangledLanes|=e;for(var n=o.entanglements,r=e;0<r;){var s=31-Kt(r),c=1<<s;n[s]|=e,r&=~c}}}),[a,t,n]),s.useEffect((function(){return n(t._source,(function(){var e=f.getSnapshot,a=f.setSnapshot;try{a(e(t._source));var n=fs(g);o.mutableReadLanes|=n&o.pendingLanes}catch(i){a((function(){throw i}))}}))}),[t,n]),ln(h,a)&&ln(b,t)&&ln(p,n)||((e={pending:null,dispatch:null,lastRenderedReducer:lr,lastRenderedState:u}).dispatch=l=Mr.bind(null,er,e),c.queue=e,c.baseQueue=null,u=fr(o,t,a),c.memoizedState=c.baseState=u),u}function br(e,t,a){return hr(cr(),e,t,a)}function gr(e){var t=sr();return"function"===typeof e&&(e=e()),t.memoizedState=t.baseState=e,e=(e=t.queue={pending:null,dispatch:null,lastRenderedReducer:lr,lastRenderedState:e}).dispatch=Mr.bind(null,er,e),[t.memoizedState,e]}function mr(e,t,a,n){return e={tag:e,create:t,destroy:a,deps:n,next:null},null===(t=er.updateQueue)?(t={lastEffect:null},er.updateQueue=t,t.lastEffect=e.next=e):null===(a=t.lastEffect)?t.lastEffect=e.next=e:(n=a.next,a.next=e,e.next=n,t.lastEffect=e),e}function yr(e){return e={current:e},sr().memoizedState=e}function vr(){return cr().memoizedState}function wr(e,t,a,n){var o=sr();er.flags|=e,o.memoizedState=mr(1|t,a,void 0,void 0===n?null:n)}function xr(e,t,a,n){var o=cr();n=void 0===n?null:n;var i=void 0;if(null!==tr){var r=tr.memoizedState;if(i=r.destroy,null!==n&&rr(n,r.deps))return void mr(t,a,i,n)}er.flags|=e,o.memoizedState=mr(1|t,a,i,n)}function kr(e,t){return wr(516,4,e,t)}function Rr(e,t){return xr(516,4,e,t)}function Ir(e,t){return xr(4,2,e,t)}function Ar(e,t){return"function"===typeof t?(e=e(),t(e),function(){t(null)}):null!==t&&void 0!==t?(e=e(),t.current=e,function(){t.current=null}):void 0}function Fr(e,t,a){return a=null!==a&&void 0!==a?a.concat([e]):null,xr(4,2,Ar.bind(null,t,e),a)}function Tr(){}function Sr(e,t){var a=cr();t=void 0===t?null:t;var n=a.memoizedState;return null!==n&&null!==t&&rr(t,n[1])?n[0]:(a.memoizedState=[e,t],e)}function Cr(e,t){var a=cr();t=void 0===t?null:t;var n=a.memoizedState;return null!==n&&null!==t&&rr(t,n[1])?n[0]:(e=e(),a.memoizedState=[e,t],e)}function Er(e,t){var a=Go();Jo(98>a?98:a,(function(){e(!0)})),Jo(97<a?97:a,(function(){var a=Qi.transition;Qi.transition=1;try{e(!1),t()}finally{Qi.transition=a}}))}function Mr(e,t,a){var n=ps(),o=fs(e),i={lane:o,action:a,eagerReducer:null,eagerState:null,next:null},r=t.pending;if(null===r?i.next=i:(i.next=r.next,r.next=i),t.pending=i,r=e.alternate,e===er||null!==r&&r===er)or=nr=!0;else{if(0===e.lanes&&(null===r||0===r.lanes)&&null!==(r=t.lastRenderedReducer))try{var d=t.lastRenderedState,s=r(d,a);if(i.eagerReducer=r,i.eagerState=s,ln(s,d))return}catch(c){}hs(e,o,n)}}var Pr={readContext:di,useCallback:ir,useContext:ir,useEffect:ir,useImperativeHandle:ir,useLayoutEffect:ir,useMemo:ir,useReducer:ir,useRef:ir,useState:ir,useDebugValue:ir,useDeferredValue:ir,useTransition:ir,useMutableSource:ir,useOpaqueIdentifier:ir,unstable_isNewReconciler:!1},Hr={readContext:di,useCallback:function(e,t){return sr().memoizedState=[e,void 0===t?null:t],e},useContext:di,useEffect:kr,useImperativeHandle:function(e,t,a){return a=null!==a&&void 0!==a?a.concat([e]):null,wr(4,2,Ar.bind(null,t,e),a)},useLayoutEffect:function(e,t){return wr(4,2,e,t)},useMemo:function(e,t){var a=sr();return t=void 0===t?null:t,e=e(),a.memoizedState=[e,t],e},useReducer:function(e,t,a){var n=sr();return t=void 0!==a?a(t):t,n.memoizedState=n.baseState=t,e=(e=n.queue={pending:null,dispatch:null,lastRenderedReducer:e,lastRenderedState:t}).dispatch=Mr.bind(null,er,e),[n.memoizedState,e]},useRef:yr,useState:gr,useDebugValue:Tr,useDeferredValue:function(e){var t=gr(e),a=t[0],n=t[1];return kr((function(){var t=Qi.transition;Qi.transition=1;try{n(e)}finally{Qi.transition=t}}),[e]),a},useTransition:function(){var e=gr(!1),t=e[0];return yr(e=Er.bind(null,e[1])),[e,t]},useMutableSource:function(e,t,a){var n=sr();return n.memoizedState={refs:{getSnapshot:t,setSnapshot:null},source:e,subscribe:a},hr(n,e,t,a)},useOpaqueIdentifier:function(){if(Wi){var e=!1,t=function(e){return{$$typeof:z,toString:e,valueOf:e}}((function(){throw e||(e=!0,a("r:"+(Zn++).toString(36))),Error(r(355))})),a=gr(t)[1];return 0===(2&er.mode)&&(er.flags|=516,mr(5,(function(){a("r:"+(Zn++).toString(36))}),void 0,null)),t}return gr(t="r:"+(Zn++).toString(36)),t},unstable_isNewReconciler:!1},_r={readContext:di,useCallback:Sr,useContext:di,useEffect:Rr,useImperativeHandle:Fr,useLayoutEffect:Ir,useMemo:Cr,useReducer:ur,useRef:vr,useState:function(){return ur(lr)},useDebugValue:Tr,useDeferredValue:function(e){var t=ur(lr),a=t[0],n=t[1];return Rr((function(){var t=Qi.transition;Qi.transition=1;try{n(e)}finally{Qi.transition=t}}),[e]),a},useTransition:function(){var e=ur(lr)[0];return[vr().current,e]},useMutableSource:br,useOpaqueIdentifier:function(){return ur(lr)[0]},unstable_isNewReconciler:!1},zr={readContext:di,useCallback:Sr,useContext:di,useEffect:Rr,useImperativeHandle:Fr,useLayoutEffect:Ir,useMemo:Cr,useReducer:pr,useRef:vr,useState:function(){return pr(lr)},useDebugValue:Tr,useDeferredValue:function(e){var t=pr(lr),a=t[0],n=t[1];return Rr((function(){var t=Qi.transition;Qi.transition=1;try{n(e)}finally{Qi.transition=t}}),[e]),a},useTransition:function(){var e=pr(lr)[0];return[vr().current,e]},useMutableSource:br,useOpaqueIdentifier:function(){return pr(lr)[0]},unstable_isNewReconciler:!1},jr=x.ReactCurrentOwner,Dr=!1;function Lr(e,t,a,n){t.child=null===e?Si(t,null,a,n):Ti(t,e.child,a,n)}function Ur(e,t,a,n,o){a=a.render;var i=t.ref;return ri(t,o),n=dr(e,t,a,n,i,o),null===e||Dr?(t.flags|=1,Lr(e,t,n,o),t.child):(t.updateQueue=e.updateQueue,t.flags&=-517,e.lanes&=~o,id(e,t,o))}function Or(e,t,a,n,o,i){if(null===e){var r=a.type;return"function"!==typeof r||Gs(r)||void 0!==r.defaultProps||null!==a.compare||void 0!==a.defaultProps?((e=Js(a.type,null,n,t,t.mode,i)).ref=t.ref,e.return=t,t.child=e):(t.tag=15,t.type=r,Nr(e,t,r,n,o,i))}return r=e.child,0===(o&i)&&(o=r.memoizedProps,(a=null!==(a=a.compare)?a:pn)(o,n)&&e.ref===t.ref)?id(e,t,i):(t.flags|=1,(e=Vs(r,n)).ref=t.ref,e.return=t,t.child=e)}function Nr(e,t,a,n,o,i){if(null!==e&&pn(e.memoizedProps,n)&&e.ref===t.ref){if(Dr=!1,0===(i&o))return t.lanes=e.lanes,id(e,t,i);0!==(16384&e.flags)&&(Dr=!0)}return Kr(e,t,a,n,i)}function Wr(e,t,a){var n=t.pendingProps,o=n.children,i=null!==e?e.memoizedState:null;if("hidden"===n.mode||"unstable-defer-without-hiding"===n.mode)if(0===(4&t.mode))t.memoizedState={baseLanes:0},ks(t,a);else{if(0===(1073741824&a))return e=null!==i?i.baseLanes|a:a,t.lanes=t.childLanes=1073741824,t.memoizedState={baseLanes:e},ks(t,e),null;t.memoizedState={baseLanes:0},ks(t,null!==i?i.baseLanes:a)}else null!==i?(n=i.baseLanes|a,t.memoizedState=null):n=a,ks(t,n);return Lr(e,t,o,a),t.child}function Br(e,t){var a=t.ref;(null===e&&null!==a||null!==e&&e.ref!==a)&&(t.flags|=128)}function Kr(e,t,a,n,o){var i=yo(a)?go:ho.current;return i=mo(t,i),ri(t,o),a=dr(e,t,a,n,i,o),null===e||Dr?(t.flags|=1,Lr(e,t,a,o),t.child):(t.updateQueue=e.updateQueue,t.flags&=-517,e.lanes&=~o,id(e,t,o))}function Gr(e,t,a,n,o){if(yo(a)){var i=!0;ko(t)}else i=!1;if(ri(t,o),null===t.stateNode)null!==e&&(e.alternate=null,t.alternate=null,t.flags|=2),wi(t,a,n),ki(t,a,n,o),n=!0;else if(null===e){var r=t.stateNode,d=t.memoizedProps;r.props=d;var s=r.context,c=a.contextType;"object"===typeof c&&null!==c?c=di(c):c=mo(t,c=yo(a)?go:ho.current);var l=a.getDerivedStateFromProps,u="function"===typeof l||"function"===typeof r.getSnapshotBeforeUpdate;u||"function"!==typeof r.UNSAFE_componentWillReceiveProps&&"function"!==typeof r.componentWillReceiveProps||(d!==n||s!==c)&&xi(t,r,n,c),si=!1;var p=t.memoizedState;r.state=p,hi(t,n,r,o),s=t.memoizedState,d!==n||p!==s||bo.current||si?("function"===typeof l&&(mi(t,a,l,n),s=t.memoizedState),(d=si||vi(t,a,d,n,p,s,c))?(u||"function"!==typeof r.UNSAFE_componentWillMount&&"function"!==typeof r.componentWillMount||("function"===typeof r.componentWillMount&&r.componentWillMount(),"function"===typeof r.UNSAFE_componentWillMount&&r.UNSAFE_componentWillMount()),"function"===typeof r.componentDidMount&&(t.flags|=4)):("function"===typeof r.componentDidMount&&(t.flags|=4),t.memoizedProps=n,t.memoizedState=s),r.props=n,r.state=s,r.context=c,n=d):("function"===typeof r.componentDidMount&&(t.flags|=4),n=!1)}else{r=t.stateNode,li(e,t),d=t.memoizedProps,c=t.type===t.elementType?d:Qo(t.type,d),r.props=c,u=t.pendingProps,p=r.context,"object"===typeof(s=a.contextType)&&null!==s?s=di(s):s=mo(t,s=yo(a)?go:ho.current);var f=a.getDerivedStateFromProps;(l="function"===typeof f||"function"===typeof r.getSnapshotBeforeUpdate)||"function"!==typeof r.UNSAFE_componentWillReceiveProps&&"function"!==typeof r.componentWillReceiveProps||(d!==u||p!==s)&&xi(t,r,n,s),si=!1,p=t.memoizedState,r.state=p,hi(t,n,r,o);var h=t.memoizedState;d!==u||p!==h||bo.current||si?("function"===typeof f&&(mi(t,a,f,n),h=t.memoizedState),(c=si||vi(t,a,c,n,p,h,s))?(l||"function"!==typeof r.UNSAFE_componentWillUpdate&&"function"!==typeof r.componentWillUpdate||("function"===typeof r.componentWillUpdate&&r.componentWillUpdate(n,h,s),"function"===typeof r.UNSAFE_componentWillUpdate&&r.UNSAFE_componentWillUpdate(n,h,s)),"function"===typeof r.componentDidUpdate&&(t.flags|=4),"function"===typeof r.getSnapshotBeforeUpdate&&(t.flags|=256)):("function"!==typeof r.componentDidUpdate||d===e.memoizedProps&&p===e.memoizedState||(t.flags|=4),"function"!==typeof r.getSnapshotBeforeUpdate||d===e.memoizedProps&&p===e.memoizedState||(t.flags|=256),t.memoizedProps=n,t.memoizedState=h),r.props=n,r.state=h,r.context=s,n=c):("function"!==typeof r.componentDidUpdate||d===e.memoizedProps&&p===e.memoizedState||(t.flags|=4),"function"!==typeof r.getSnapshotBeforeUpdate||d===e.memoizedProps&&p===e.memoizedState||(t.flags|=256),n=!1)}return Vr(e,t,a,n,i,o)}function Vr(e,t,a,n,o,i){Br(e,t);var r=0!==(64&t.flags);if(!n&&!r)return o&&Ro(t,a,!1),id(e,t,i);n=t.stateNode,jr.current=t;var d=r&&"function"!==typeof a.getDerivedStateFromError?null:n.render();return t.flags|=1,null!==e&&r?(t.child=Ti(t,e.child,null,i),t.child=Ti(t,null,d,i)):Lr(e,t,d,i),t.memoizedState=n.state,o&&Ro(t,a,!0),t.child}function Jr(e){var t=e.stateNode;t.pendingContext?wo(0,t.pendingContext,t.pendingContext!==t.context):t.context&&wo(0,t.context,!1),_i(e,t.containerInfo)}var Yr,qr,Zr,$r={dehydrated:null,retryLane:0};function Qr(e,t,a){var n,o=t.pendingProps,i=Li.current,r=!1;return(n=0!==(64&t.flags))||(n=(null===e||null!==e.memoizedState)&&0!==(2&i)),n?(r=!0,t.flags&=-65):null!==e&&null===e.memoizedState||void 0===o.fallback||!0===o.unstable_avoidThisFallback||(i|=1),po(Li,1&i),null===e?(void 0!==o.fallback&&Gi(t),e=o.children,i=o.fallback,r?(e=Xr(t,e,i,a),t.child.memoizedState={baseLanes:a},t.memoizedState=$r,e):"number"===typeof o.unstable_expectedLoadTime?(e=Xr(t,e,i,a),t.child.memoizedState={baseLanes:a},t.memoizedState=$r,t.lanes=33554432,e):((a=qs({mode:"visible",children:e},t.mode,a,null)).return=t,t.child=a)):(e.memoizedState,r?(o=td(e,t,o.children,o.fallback,a),r=t.child,i=e.child.memoizedState,r.memoizedState=null===i?{baseLanes:a}:{baseLanes:i.baseLanes|a},r.childLanes=e.childLanes&~a,t.memoizedState=$r,o):(a=ed(e,t,o.children,a),t.memoizedState=null,a))}function Xr(e,t,a,n){var o=e.mode,i=e.child;return t={mode:"hidden",children:t},0===(2&o)&&null!==i?(i.childLanes=0,i.pendingProps=t):i=qs(t,o,0,null),a=Ys(a,o,n,null),i.return=e,a.return=e,i.sibling=a,e.child=i,a}function ed(e,t,a,n){var o=e.child;return e=o.sibling,a=Vs(o,{mode:"visible",children:a}),0===(2&t.mode)&&(a.lanes=n),a.return=t,a.sibling=null,null!==e&&(e.nextEffect=null,e.flags=8,t.firstEffect=t.lastEffect=e),t.child=a}function td(e,t,a,n,o){var i=t.mode,r=e.child;e=r.sibling;var d={mode:"hidden",children:a};return 0===(2&i)&&t.child!==r?((a=t.child).childLanes=0,a.pendingProps=d,null!==(r=a.lastEffect)?(t.firstEffect=a.firstEffect,t.lastEffect=r,r.nextEffect=null):t.firstEffect=t.lastEffect=null):a=Vs(r,d),null!==e?n=Vs(e,n):(n=Ys(n,i,o,null)).flags|=2,n.return=t,a.return=t,a.sibling=n,t.child=a,n}function ad(e,t){e.lanes|=t;var a=e.alternate;null!==a&&(a.lanes|=t),ii(e.return,t)}function nd(e,t,a,n,o,i){var r=e.memoizedState;null===r?e.memoizedState={isBackwards:t,rendering:null,renderingStartTime:0,last:n,tail:a,tailMode:o,lastEffect:i}:(r.isBackwards=t,r.rendering=null,r.renderingStartTime=0,r.last=n,r.tail=a,r.tailMode=o,r.lastEffect=i)}function od(e,t,a){var n=t.pendingProps,o=n.revealOrder,i=n.tail;if(Lr(e,t,n.children,a),0!==(2&(n=Li.current)))n=1&n|2,t.flags|=64;else{if(null!==e&&0!==(64&e.flags))e:for(e=t.child;null!==e;){if(13===e.tag)null!==e.memoizedState&&ad(e,a);else if(19===e.tag)ad(e,a);else if(null!==e.child){e.child.return=e,e=e.child;continue}if(e===t)break e;for(;null===e.sibling;){if(null===e.return||e.return===t)break e;e=e.return}e.sibling.return=e.return,e=e.sibling}n&=1}if(po(Li,n),0===(2&t.mode))t.memoizedState=null;else switch(o){case"forwards":for(a=t.child,o=null;null!==a;)null!==(e=a.alternate)&&null===Ui(e)&&(o=a),a=a.sibling;null===(a=o)?(o=t.child,t.child=null):(o=a.sibling,a.sibling=null),nd(t,!1,o,a,i,t.lastEffect);break;case"backwards":for(a=null,o=t.child,t.child=null;null!==o;){if(null!==(e=o.alternate)&&null===Ui(e)){t.child=o;break}e=o.sibling,o.sibling=a,a=o,o=e}nd(t,!0,a,null,i,t.lastEffect);break;case"together":nd(t,!1,null,null,void 0,t.lastEffect);break;default:t.memoizedState=null}return t.child}function id(e,t,a){if(null!==e&&(t.dependencies=e.dependencies),Nd|=t.lanes,0!==(a&t.childLanes)){if(null!==e&&t.child!==e.child)throw Error(r(153));if(null!==t.child){for(a=Vs(e=t.child,e.pendingProps),t.child=a,a.return=t;null!==e.sibling;)e=e.sibling,(a=a.sibling=Vs(e,e.pendingProps)).return=t;a.sibling=null}return t.child}return null}function rd(e,t){if(!Wi)switch(e.tailMode){case"hidden":t=e.tail;for(var a=null;null!==t;)null!==t.alternate&&(a=t),t=t.sibling;null===a?e.tail=null:a.sibling=null;break;case"collapsed":a=e.tail;for(var n=null;null!==a;)null!==a.alternate&&(n=a),a=a.sibling;null===n?t||null===e.tail?e.tail=null:e.tail.sibling=null:n.sibling=null}}function dd(e,t,a){var n=t.pendingProps;switch(t.tag){case 2:case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return null;case 1:case 17:return yo(t.type)&&vo(),null;case 3:return zi(),uo(bo),uo(ho),Zi(),(n=t.stateNode).pendingContext&&(n.context=n.pendingContext,n.pendingContext=null),null!==e&&null!==e.child||(Ji(t)?t.flags|=4:n.hydrate||(t.flags|=256)),null;case 5:Di(t);var i=Hi(Pi.current);if(a=t.type,null!==e&&null!=t.stateNode)qr(e,t,a,n),e.ref!==t.ref&&(t.flags|=128);else{if(!n){if(null===t.stateNode)throw Error(r(166));return null}if(e=Hi(Ei.current),Ji(t)){n=t.stateNode,a=t.type;var d=t.memoizedProps;switch(n[Qn]=t,n[Xn]=d,a){case"dialog":En("cancel",n),En("close",n);break;case"iframe":case"object":case"embed":En("load",n);break;case"video":case"audio":for(e=0;e<Fn.length;e++)En(Fn[e],n);break;case"source":En("error",n);break;case"img":case"image":case"link":En("error",n),En("load",n);break;case"details":En("toggle",n);break;case"input":ee(n,d),En("invalid",n);break;case"select":n._wrapperState={wasMultiple:!!d.multiple},En("invalid",n);break;case"textarea":se(n,d),En("invalid",n)}for(var c in Ie(a,d),e=null,d)d.hasOwnProperty(c)&&(i=d[c],"children"===c?"string"===typeof i?n.textContent!==i&&(e=["children",i]):"number"===typeof i&&n.textContent!==""+i&&(e=["children",""+i]):s.hasOwnProperty(c)&&null!=i&&"onScroll"===c&&En("scroll",n));switch(a){case"input":Z(n),ne(n,d,!0);break;case"textarea":Z(n),le(n);break;case"select":case"option":break;default:"function"===typeof d.onClick&&(n.onclick=On)}n=e,t.updateQueue=n,null!==n&&(t.flags|=4)}else{switch(c=9===i.nodeType?i:i.ownerDocument,e===ue&&(e=fe(a)),e===ue?"script"===a?((e=c.createElement("div")).innerHTML="<script><\/script>",e=e.removeChild(e.firstChild)):"string"===typeof n.is?e=c.createElement(a,{is:n.is}):(e=c.createElement(a),"select"===a&&(c=e,n.multiple?c.multiple=!0:n.size&&(c.size=n.size))):e=c.createElementNS(e,a),e[Qn]=t,e[Xn]=n,Yr(e,t),t.stateNode=e,c=Ae(a,n),a){case"dialog":En("cancel",e),En("close",e),i=n;break;case"iframe":case"object":case"embed":En("load",e),i=n;break;case"video":case"audio":for(i=0;i<Fn.length;i++)En(Fn[i],e);i=n;break;case"source":En("error",e),i=n;break;case"img":case"image":case"link":En("error",e),En("load",e),i=n;break;case"details":En("toggle",e),i=n;break;case"input":ee(e,n),i=X(e,n),En("invalid",e);break;case"option":i=ie(e,n);break;case"select":e._wrapperState={wasMultiple:!!n.multiple},i=o({},n,{value:void 0}),En("invalid",e);break;case"textarea":se(e,n),i=de(e,n),En("invalid",e);break;default:i=n}Ie(a,i);var l=i;for(d in l)if(l.hasOwnProperty(d)){var u=l[d];"style"===d?ke(e,u):"dangerouslySetInnerHTML"===d?null!=(u=u?u.__html:void 0)&&me(e,u):"children"===d?"string"===typeof u?("textarea"!==a||""!==u)&&ye(e,u):"number"===typeof u&&ye(e,""+u):"suppressContentEditableWarning"!==d&&"suppressHydrationWarning"!==d&&"autoFocus"!==d&&(s.hasOwnProperty(d)?null!=u&&"onScroll"===d&&En("scroll",e):null!=u&&w(e,d,u,c))}switch(a){case"input":Z(e),ne(e,n,!1);break;case"textarea":Z(e),le(e);break;case"option":null!=n.value&&e.setAttribute("value",""+Y(n.value));break;case"select":e.multiple=!!n.multiple,null!=(d=n.value)?re(e,!!n.multiple,d,!1):null!=n.defaultValue&&re(e,!!n.multiple,n.defaultValue,!0);break;default:"function"===typeof i.onClick&&(e.onclick=On)}Bn(a,n)&&(t.flags|=4)}null!==t.ref&&(t.flags|=128)}return null;case 6:if(e&&null!=t.stateNode)Zr(0,t,e.memoizedProps,n);else{if("string"!==typeof n&&null===t.stateNode)throw Error(r(166));a=Hi(Pi.current),Hi(Ei.current),Ji(t)?(n=t.stateNode,a=t.memoizedProps,n[Qn]=t,n.nodeValue!==a&&(t.flags|=4)):((n=(9===a.nodeType?a:a.ownerDocument).createTextNode(n))[Qn]=t,t.stateNode=n)}return null;case 13:return uo(Li),n=t.memoizedState,0!==(64&t.flags)?(t.lanes=a,t):(n=null!==n,a=!1,null===e?void 0!==t.memoizedProps.fallback&&Ji(t):a=null!==e.memoizedState,n&&!a&&0!==(2&t.mode)&&(null===e&&!0!==t.memoizedProps.unstable_avoidThisFallback||0!==(1&Li.current)?0===Ld&&(Ld=3):(0!==Ld&&3!==Ld||(Ld=4),null===Hd||0===(134217727&Nd)&&0===(134217727&Wd)||ys(Hd,zd))),(n||a)&&(t.flags|=4),null);case 4:return zi(),null===e&&Pn(t.stateNode.containerInfo),null;case 10:return oi(t),null;case 19:if(uo(Li),null===(n=t.memoizedState))return null;if(d=0!==(64&t.flags),null===(c=n.rendering))if(d)rd(n,!1);else{if(0!==Ld||null!==e&&0!==(64&e.flags))for(e=t.child;null!==e;){if(null!==(c=Ui(e))){for(t.flags|=64,rd(n,!1),null!==(d=c.updateQueue)&&(t.updateQueue=d,t.flags|=4),null===n.lastEffect&&(t.firstEffect=null),t.lastEffect=n.lastEffect,n=a,a=t.child;null!==a;)e=n,(d=a).flags&=2,d.nextEffect=null,d.firstEffect=null,d.lastEffect=null,null===(c=d.alternate)?(d.childLanes=0,d.lanes=e,d.child=null,d.memoizedProps=null,d.memoizedState=null,d.updateQueue=null,d.dependencies=null,d.stateNode=null):(d.childLanes=c.childLanes,d.lanes=c.lanes,d.child=c.child,d.memoizedProps=c.memoizedProps,d.memoizedState=c.memoizedState,d.updateQueue=c.updateQueue,d.type=c.type,e=c.dependencies,d.dependencies=null===e?null:{lanes:e.lanes,firstContext:e.firstContext}),a=a.sibling;return po(Li,1&Li.current|2),t.child}e=e.sibling}null!==n.tail&&Ko()>Vd&&(t.flags|=64,d=!0,rd(n,!1),t.lanes=33554432)}else{if(!d)if(null!==(e=Ui(c))){if(t.flags|=64,d=!0,null!==(a=e.updateQueue)&&(t.updateQueue=a,t.flags|=4),rd(n,!0),null===n.tail&&"hidden"===n.tailMode&&!c.alternate&&!Wi)return null!==(t=t.lastEffect=n.lastEffect)&&(t.nextEffect=null),null}else 2*Ko()-n.renderingStartTime>Vd&&1073741824!==a&&(t.flags|=64,d=!0,rd(n,!1),t.lanes=33554432);n.isBackwards?(c.sibling=t.child,t.child=c):(null!==(a=n.last)?a.sibling=c:t.child=c,n.last=c)}return null!==n.tail?(a=n.tail,n.rendering=a,n.tail=a.sibling,n.lastEffect=t.lastEffect,n.renderingStartTime=Ko(),a.sibling=null,t=Li.current,po(Li,d?1&t|2:1&t),a):null;case 23:case 24:return Rs(),null!==e&&null!==e.memoizedState!==(null!==t.memoizedState)&&"unstable-defer-without-hiding"!==n.mode&&(t.flags|=4),null}throw Error(r(156,t.tag))}function sd(e){switch(e.tag){case 1:yo(e.type)&&vo();var t=e.flags;return 4096&t?(e.flags=-4097&t|64,e):null;case 3:if(zi(),uo(bo),uo(ho),Zi(),0!==(64&(t=e.flags)))throw Error(r(285));return e.flags=-4097&t|64,e;case 5:return Di(e),null;case 13:return uo(Li),4096&(t=e.flags)?(e.flags=-4097&t|64,e):null;case 19:return uo(Li),null;case 4:return zi(),null;case 10:return oi(e),null;case 23:case 24:return Rs(),null;default:return null}}function cd(e,t){try{var a="",n=t;do{a+=V(n),n=n.return}while(n);var o=a}catch(i){o="\nError generating stack: "+i.message+"\n"+i.stack}return{value:e,source:t,stack:o}}function ld(e,t){try{console.error(t.value)}catch(a){setTimeout((function(){throw a}))}}Yr=function(e,t){for(var a=t.child;null!==a;){if(5===a.tag||6===a.tag)e.appendChild(a.stateNode);else if(4!==a.tag&&null!==a.child){a.child.return=a,a=a.child;continue}if(a===t)break;for(;null===a.sibling;){if(null===a.return||a.return===t)return;a=a.return}a.sibling.return=a.return,a=a.sibling}},qr=function(e,t,a,n){var i=e.memoizedProps;if(i!==n){e=t.stateNode,Hi(Ei.current);var r,d=null;switch(a){case"input":i=X(e,i),n=X(e,n),d=[];break;case"option":i=ie(e,i),n=ie(e,n),d=[];break;case"select":i=o({},i,{value:void 0}),n=o({},n,{value:void 0}),d=[];break;case"textarea":i=de(e,i),n=de(e,n),d=[];break;default:"function"!==typeof i.onClick&&"function"===typeof n.onClick&&(e.onclick=On)}for(u in Ie(a,n),a=null,i)if(!n.hasOwnProperty(u)&&i.hasOwnProperty(u)&&null!=i[u])if("style"===u){var c=i[u];for(r in c)c.hasOwnProperty(r)&&(a||(a={}),a[r]="")}else"dangerouslySetInnerHTML"!==u&&"children"!==u&&"suppressContentEditableWarning"!==u&&"suppressHydrationWarning"!==u&&"autoFocus"!==u&&(s.hasOwnProperty(u)?d||(d=[]):(d=d||[]).push(u,null));for(u in n){var l=n[u];if(c=null!=i?i[u]:void 0,n.hasOwnProperty(u)&&l!==c&&(null!=l||null!=c))if("style"===u)if(c){for(r in c)!c.hasOwnProperty(r)||l&&l.hasOwnProperty(r)||(a||(a={}),a[r]="");for(r in l)l.hasOwnProperty(r)&&c[r]!==l[r]&&(a||(a={}),a[r]=l[r])}else a||(d||(d=[]),d.push(u,a)),a=l;else"dangerouslySetInnerHTML"===u?(l=l?l.__html:void 0,c=c?c.__html:void 0,null!=l&&c!==l&&(d=d||[]).push(u,l)):"children"===u?"string"!==typeof l&&"number"!==typeof l||(d=d||[]).push(u,""+l):"suppressContentEditableWarning"!==u&&"suppressHydrationWarning"!==u&&(s.hasOwnProperty(u)?(null!=l&&"onScroll"===u&&En("scroll",e),d||c===l||(d=[])):"object"===typeof l&&null!==l&&l.$$typeof===z?l.toString():(d=d||[]).push(u,l))}a&&(d=d||[]).push("style",a);var u=d;(t.updateQueue=u)&&(t.flags|=4)}},Zr=function(e,t,a,n){a!==n&&(t.flags|=4)};var ud="function"===typeof WeakMap?WeakMap:Map;function pd(e,t,a){(a=ui(-1,a)).tag=3,a.payload={element:null};var n=t.value;return a.callback=function(){Zd||(Zd=!0,$d=n),ld(0,t)},a}function fd(e,t,a){(a=ui(-1,a)).tag=3;var n=e.type.getDerivedStateFromError;if("function"===typeof n){var o=t.value;a.payload=function(){return ld(0,t),n(o)}}var i=e.stateNode;return null!==i&&"function"===typeof i.componentDidCatch&&(a.callback=function(){"function"!==typeof n&&(null===Qd?Qd=new Set([this]):Qd.add(this),ld(0,t));var e=t.stack;this.componentDidCatch(t.value,{componentStack:null!==e?e:""})}),a}var hd="function"===typeof WeakSet?WeakSet:Set;function bd(e){var t=e.ref;if(null!==t)if("function"===typeof t)try{t(null)}catch(a){Os(e,a)}else t.current=null}function gd(e,t){switch(t.tag){case 0:case 11:case 15:case 22:case 5:case 6:case 4:case 17:return;case 1:if(256&t.flags&&null!==e){var a=e.memoizedProps,n=e.memoizedState;t=(e=t.stateNode).getSnapshotBeforeUpdate(t.elementType===t.type?a:Qo(t.type,a),n),e.__reactInternalSnapshotBeforeUpdate=t}return;case 3:return void(256&t.flags&&Jn(t.stateNode.containerInfo))}throw Error(r(163))}function md(e,t,a){switch(a.tag){case 0:case 11:case 15:case 22:if(null!==(t=null!==(t=a.updateQueue)?t.lastEffect:null)){e=t=t.next;do{if(3===(3&e.tag)){var n=e.create;e.destroy=n()}e=e.next}while(e!==t)}if(null!==(t=null!==(t=a.updateQueue)?t.lastEffect:null)){e=t=t.next;do{var o=e;n=o.next,0!==(4&(o=o.tag))&&0!==(1&o)&&(Ds(a,e),js(a,e)),e=n}while(e!==t)}return;case 1:return e=a.stateNode,4&a.flags&&(null===t?e.componentDidMount():(n=a.elementType===a.type?t.memoizedProps:Qo(a.type,t.memoizedProps),e.componentDidUpdate(n,t.memoizedState,e.__reactInternalSnapshotBeforeUpdate))),void(null!==(t=a.updateQueue)&&bi(a,t,e));case 3:if(null!==(t=a.updateQueue)){if(e=null,null!==a.child)switch(a.child.tag){case 5:case 1:e=a.child.stateNode}bi(a,t,e)}return;case 5:return e=a.stateNode,void(null===t&&4&a.flags&&Bn(a.type,a.memoizedProps)&&e.focus());case 6:case 4:case 12:case 19:case 17:case 20:case 21:case 23:case 24:return;case 13:return void(null===a.memoizedState&&(a=a.alternate,null!==a&&(a=a.memoizedState,null!==a&&(a=a.dehydrated,null!==a&&kt(a)))))}throw Error(r(163))}function yd(e,t){for(var a=e;;){if(5===a.tag){var n=a.stateNode;if(t)"function"===typeof(n=n.style).setProperty?n.setProperty("display","none","important"):n.display="none";else{n=a.stateNode;var o=a.memoizedProps.style;o=void 0!==o&&null!==o&&o.hasOwnProperty("display")?o.display:null,n.style.display=xe("display",o)}}else if(6===a.tag)a.stateNode.nodeValue=t?"":a.memoizedProps;else if((23!==a.tag&&24!==a.tag||null===a.memoizedState||a===e)&&null!==a.child){a.child.return=a,a=a.child;continue}if(a===e)break;for(;null===a.sibling;){if(null===a.return||a.return===e)return;a=a.return}a.sibling.return=a.return,a=a.sibling}}function vd(e,t){if(Ao&&"function"===typeof Ao.onCommitFiberUnmount)try{Ao.onCommitFiberUnmount(Io,t)}catch(i){}switch(t.tag){case 0:case 11:case 14:case 15:case 22:if(null!==(e=t.updateQueue)&&null!==(e=e.lastEffect)){var a=e=e.next;do{var n=a,o=n.destroy;if(n=n.tag,void 0!==o)if(0!==(4&n))Ds(t,a);else{n=t;try{o()}catch(i){Os(n,i)}}a=a.next}while(a!==e)}break;case 1:if(bd(t),"function"===typeof(e=t.stateNode).componentWillUnmount)try{e.props=t.memoizedProps,e.state=t.memoizedState,e.componentWillUnmount()}catch(i){Os(t,i)}break;case 5:bd(t);break;case 4:Ad(e,t)}}function wd(e){e.alternate=null,e.child=null,e.dependencies=null,e.firstEffect=null,e.lastEffect=null,e.memoizedProps=null,e.memoizedState=null,e.pendingProps=null,e.return=null,e.updateQueue=null}function xd(e){return 5===e.tag||3===e.tag||4===e.tag}function kd(e){e:{for(var t=e.return;null!==t;){if(xd(t))break e;t=t.return}throw Error(r(160))}var a=t;switch(t=a.stateNode,a.tag){case 5:var n=!1;break;case 3:case 4:t=t.containerInfo,n=!0;break;default:throw Error(r(161))}16&a.flags&&(ye(t,""),a.flags&=-17);e:t:for(a=e;;){for(;null===a.sibling;){if(null===a.return||xd(a.return)){a=null;break e}a=a.return}for(a.sibling.return=a.return,a=a.sibling;5!==a.tag&&6!==a.tag&&18!==a.tag;){if(2&a.flags)continue t;if(null===a.child||4===a.tag)continue t;a.child.return=a,a=a.child}if(!(2&a.flags)){a=a.stateNode;break e}}n?Rd(e,a,t):Id(e,a,t)}function Rd(e,t,a){var n=e.tag,o=5===n||6===n;if(o)e=o?e.stateNode:e.stateNode.instance,t?8===a.nodeType?a.parentNode.insertBefore(e,t):a.insertBefore(e,t):(8===a.nodeType?(t=a.parentNode).insertBefore(e,a):(t=a).appendChild(e),null!==(a=a._reactRootContainer)&&void 0!==a||null!==t.onclick||(t.onclick=On));else if(4!==n&&null!==(e=e.child))for(Rd(e,t,a),e=e.sibling;null!==e;)Rd(e,t,a),e=e.sibling}function Id(e,t,a){var n=e.tag,o=5===n||6===n;if(o)e=o?e.stateNode:e.stateNode.instance,t?a.insertBefore(e,t):a.appendChild(e);else if(4!==n&&null!==(e=e.child))for(Id(e,t,a),e=e.sibling;null!==e;)Id(e,t,a),e=e.sibling}function Ad(e,t){for(var a,n,o=t,i=!1;;){if(!i){i=o.return;e:for(;;){if(null===i)throw Error(r(160));switch(a=i.stateNode,i.tag){case 5:n=!1;break e;case 3:case 4:a=a.containerInfo,n=!0;break e}i=i.return}i=!0}if(5===o.tag||6===o.tag){e:for(var d=e,s=o,c=s;;)if(vd(d,c),null!==c.child&&4!==c.tag)c.child.return=c,c=c.child;else{if(c===s)break e;for(;null===c.sibling;){if(null===c.return||c.return===s)break e;c=c.return}c.sibling.return=c.return,c=c.sibling}n?(d=a,s=o.stateNode,8===d.nodeType?d.parentNode.removeChild(s):d.removeChild(s)):a.removeChild(o.stateNode)}else if(4===o.tag){if(null!==o.child){a=o.stateNode.containerInfo,n=!0,o.child.return=o,o=o.child;continue}}else if(vd(e,o),null!==o.child){o.child.return=o,o=o.child;continue}if(o===t)break;for(;null===o.sibling;){if(null===o.return||o.return===t)return;4===(o=o.return).tag&&(i=!1)}o.sibling.return=o.return,o=o.sibling}}function Fd(e,t){switch(t.tag){case 0:case 11:case 14:case 15:case 22:var a=t.updateQueue;if(null!==(a=null!==a?a.lastEffect:null)){var n=a=a.next;do{3===(3&n.tag)&&(e=n.destroy,n.destroy=void 0,void 0!==e&&e()),n=n.next}while(n!==a)}return;case 1:case 12:case 17:return;case 5:if(null!=(a=t.stateNode)){n=t.memoizedProps;var o=null!==e?e.memoizedProps:n;e=t.type;var i=t.updateQueue;if(t.updateQueue=null,null!==i){for(a[Xn]=n,"input"===e&&"radio"===n.type&&null!=n.name&&te(a,n),Ae(e,o),t=Ae(e,n),o=0;o<i.length;o+=2){var d=i[o],s=i[o+1];"style"===d?ke(a,s):"dangerouslySetInnerHTML"===d?me(a,s):"children"===d?ye(a,s):w(a,d,s,t)}switch(e){case"input":ae(a,n);break;case"textarea":ce(a,n);break;case"select":e=a._wrapperState.wasMultiple,a._wrapperState.wasMultiple=!!n.multiple,null!=(i=n.value)?re(a,!!n.multiple,i,!1):e!==!!n.multiple&&(null!=n.defaultValue?re(a,!!n.multiple,n.defaultValue,!0):re(a,!!n.multiple,n.multiple?[]:"",!1))}}}return;case 6:if(null===t.stateNode)throw Error(r(162));return void(t.stateNode.nodeValue=t.memoizedProps);case 3:return void((a=t.stateNode).hydrate&&(a.hydrate=!1,kt(a.containerInfo)));case 13:return null!==t.memoizedState&&(Gd=Ko(),yd(t.child,!0)),void Td(t);case 19:return void Td(t);case 23:case 24:return void yd(t,null!==t.memoizedState)}throw Error(r(163))}function Td(e){var t=e.updateQueue;if(null!==t){e.updateQueue=null;var a=e.stateNode;null===a&&(a=e.stateNode=new hd),t.forEach((function(t){var n=Ws.bind(null,e,t);a.has(t)||(a.add(t),t.then(n,n))}))}}function Sd(e,t){return null!==e&&(null===(e=e.memoizedState)||null!==e.dehydrated)&&(null!==(t=t.memoizedState)&&null===t.dehydrated)}var Cd=Math.ceil,Ed=x.ReactCurrentDispatcher,Md=x.ReactCurrentOwner,Pd=0,Hd=null,_d=null,zd=0,jd=0,Dd=lo(0),Ld=0,Ud=null,Od=0,Nd=0,Wd=0,Bd=0,Kd=null,Gd=0,Vd=1/0;function Jd(){Vd=Ko()+500}var Yd,qd=null,Zd=!1,$d=null,Qd=null,Xd=!1,es=null,ts=90,as=[],ns=[],os=null,is=0,rs=null,ds=-1,ss=0,cs=0,ls=null,us=!1;function ps(){return 0!==(48&Pd)?Ko():-1!==ds?ds:ds=Ko()}function fs(e){if(0===(2&(e=e.mode)))return 1;if(0===(4&e))return 99===Go()?1:2;if(0===ss&&(ss=Od),0!==$o.transition){0!==cs&&(cs=null!==Kd?Kd.pendingLanes:0),e=ss;var t=4186112&~cs;return 0===(t&=-t)&&(0===(t=(e=4186112&~e)&-e)&&(t=8192)),t}return e=Go(),0!==(4&Pd)&&98===e?e=Ot(12,ss):e=Ot(e=function(e){switch(e){case 99:return 15;case 98:return 10;case 97:case 96:return 8;case 95:return 2;default:return 0}}(e),ss),e}function hs(e,t,a){if(50<is)throw is=0,rs=null,Error(r(185));if(null===(e=bs(e,t)))return null;Bt(e,t,a),e===Hd&&(Wd|=t,4===Ld&&ys(e,zd));var n=Go();1===t?0!==(8&Pd)&&0===(48&Pd)?vs(e):(gs(e,a),0===Pd&&(Jd(),qo())):(0===(4&Pd)||98!==n&&99!==n||(null===os?os=new Set([e]):os.add(e)),gs(e,a)),Kd=e}function bs(e,t){e.lanes|=t;var a=e.alternate;for(null!==a&&(a.lanes|=t),a=e,e=e.return;null!==e;)e.childLanes|=t,null!==(a=e.alternate)&&(a.childLanes|=t),a=e,e=e.return;return 3===a.tag?a.stateNode:null}function gs(e,t){for(var a=e.callbackNode,n=e.suspendedLanes,o=e.pingedLanes,i=e.expirationTimes,d=e.pendingLanes;0<d;){var s=31-Kt(d),c=1<<s,l=i[s];if(-1===l){if(0===(c&n)||0!==(c&o)){l=t,Dt(c);var u=jt;i[s]=10<=u?l+250:6<=u?l+5e3:-1}}else l<=t&&(e.expiredLanes|=c);d&=~c}if(n=Lt(e,e===Hd?zd:0),t=jt,0===n)null!==a&&(a!==Lo&&So(a),e.callbackNode=null,e.callbackPriority=0);else{if(null!==a){if(e.callbackPriority===t)return;a!==Lo&&So(a)}15===t?(a=vs.bind(null,e),null===Oo?(Oo=[a],No=To(Ho,Zo)):Oo.push(a),a=Lo):14===t?a=Yo(99,vs.bind(null,e)):(a=function(e){switch(e){case 15:case 14:return 99;case 13:case 12:case 11:case 10:return 98;case 9:case 8:case 7:case 6:case 4:case 5:return 97;case 3:case 2:case 1:return 95;case 0:return 90;default:throw Error(r(358,e))}}(t),a=Yo(a,ms.bind(null,e))),e.callbackPriority=t,e.callbackNode=a}}function ms(e){if(ds=-1,cs=ss=0,0!==(48&Pd))throw Error(r(327));var t=e.callbackNode;if(zs()&&e.callbackNode!==t)return null;var a=Lt(e,e===Hd?zd:0);if(0===a)return null;var n=a,o=Pd;Pd|=16;var i=Fs();for(Hd===e&&zd===n||(Jd(),Is(e,n));;)try{Cs();break}catch(s){As(e,s)}if(ni(),Ed.current=i,Pd=o,null!==_d?n=0:(Hd=null,zd=0,n=Ld),0!==(Od&Wd))Is(e,0);else if(0!==n){if(2===n&&(Pd|=64,e.hydrate&&(e.hydrate=!1,Jn(e.containerInfo)),0!==(a=Ut(e))&&(n=Ts(e,a))),1===n)throw t=Ud,Is(e,0),ys(e,a),gs(e,Ko()),t;switch(e.finishedWork=e.current.alternate,e.finishedLanes=a,n){case 0:case 1:throw Error(r(345));case 2:case 5:Ps(e);break;case 3:if(ys(e,a),(62914560&a)===a&&10<(n=Gd+500-Ko())){if(0!==Lt(e,0))break;if(((o=e.suspendedLanes)&a)!==a){ps(),e.pingedLanes|=e.suspendedLanes&o;break}e.timeoutHandle=Gn(Ps.bind(null,e),n);break}Ps(e);break;case 4:if(ys(e,a),(4186112&a)===a)break;for(n=e.eventTimes,o=-1;0<a;){var d=31-Kt(a);i=1<<d,(d=n[d])>o&&(o=d),a&=~i}if(a=o,10<(a=(120>(a=Ko()-a)?120:480>a?480:1080>a?1080:1920>a?1920:3e3>a?3e3:4320>a?4320:1960*Cd(a/1960))-a)){e.timeoutHandle=Gn(Ps.bind(null,e),a);break}Ps(e);break;default:throw Error(r(329))}}return gs(e,Ko()),e.callbackNode===t?ms.bind(null,e):null}function ys(e,t){for(t&=~Bd,t&=~Wd,e.suspendedLanes|=t,e.pingedLanes&=~t,e=e.expirationTimes;0<t;){var a=31-Kt(t),n=1<<a;e[a]=-1,t&=~n}}function vs(e){if(0!==(48&Pd))throw Error(r(327));if(zs(),e===Hd&&0!==(e.expiredLanes&zd)){var t=zd,a=Ts(e,t);0!==(Od&Wd)&&(a=Ts(e,t=Lt(e,t)))}else a=Ts(e,t=Lt(e,0));if(0!==e.tag&&2===a&&(Pd|=64,e.hydrate&&(e.hydrate=!1,Jn(e.containerInfo)),0!==(t=Ut(e))&&(a=Ts(e,t))),1===a)throw a=Ud,Is(e,0),ys(e,t),gs(e,Ko()),a;return e.finishedWork=e.current.alternate,e.finishedLanes=t,Ps(e),gs(e,Ko()),null}function ws(e,t){var a=Pd;Pd|=1;try{return e(t)}finally{0===(Pd=a)&&(Jd(),qo())}}function xs(e,t){var a=Pd;Pd&=-2,Pd|=8;try{return e(t)}finally{0===(Pd=a)&&(Jd(),qo())}}function ks(e,t){po(Dd,jd),jd|=t,Od|=t}function Rs(){jd=Dd.current,uo(Dd)}function Is(e,t){e.finishedWork=null,e.finishedLanes=0;var a=e.timeoutHandle;if(-1!==a&&(e.timeoutHandle=-1,Vn(a)),null!==_d)for(a=_d.return;null!==a;){var n=a;switch(n.tag){case 1:null!==(n=n.type.childContextTypes)&&void 0!==n&&vo();break;case 3:zi(),uo(bo),uo(ho),Zi();break;case 5:Di(n);break;case 4:zi();break;case 13:case 19:uo(Li);break;case 10:oi(n);break;case 23:case 24:Rs()}a=a.return}Hd=e,_d=Vs(e.current,null),zd=jd=Od=t,Ld=0,Ud=null,Bd=Wd=Nd=0}function As(e,t){for(;;){var a=_d;try{if(ni(),$i.current=Pr,nr){for(var n=er.memoizedState;null!==n;){var o=n.queue;null!==o&&(o.pending=null),n=n.next}nr=!1}if(Xi=0,ar=tr=er=null,or=!1,Md.current=null,null===a||null===a.return){Ld=1,Ud=t,_d=null;break}e:{var i=e,r=a.return,d=a,s=t;if(t=zd,d.flags|=2048,d.firstEffect=d.lastEffect=null,null!==s&&"object"===typeof s&&"function"===typeof s.then){var c=s;if(0===(2&d.mode)){var l=d.alternate;l?(d.updateQueue=l.updateQueue,d.memoizedState=l.memoizedState,d.lanes=l.lanes):(d.updateQueue=null,d.memoizedState=null)}var u=0!==(1&Li.current),p=r;do{var f;if(f=13===p.tag){var h=p.memoizedState;if(null!==h)f=null!==h.dehydrated;else{var b=p.memoizedProps;f=void 0!==b.fallback&&(!0!==b.unstable_avoidThisFallback||!u)}}if(f){var g=p.updateQueue;if(null===g){var m=new Set;m.add(c),p.updateQueue=m}else g.add(c);if(0===(2&p.mode)){if(p.flags|=64,d.flags|=16384,d.flags&=-2981,1===d.tag)if(null===d.alternate)d.tag=17;else{var y=ui(-1,1);y.tag=2,pi(d,y)}d.lanes|=1;break e}s=void 0,d=t;var v=i.pingCache;if(null===v?(v=i.pingCache=new ud,s=new Set,v.set(c,s)):void 0===(s=v.get(c))&&(s=new Set,v.set(c,s)),!s.has(d)){s.add(d);var w=Ns.bind(null,i,c,d);c.then(w,w)}p.flags|=4096,p.lanes=t;break e}p=p.return}while(null!==p);s=Error((J(d.type)||"A React component")+" suspended while rendering, but no fallback UI was specified.\n\nAdd a <Suspense fallback=...> component higher in the tree to provide a loading indicator or placeholder to display.")}5!==Ld&&(Ld=2),s=cd(s,d),p=r;do{switch(p.tag){case 3:i=s,p.flags|=4096,t&=-t,p.lanes|=t,fi(p,pd(0,i,t));break e;case 1:i=s;var x=p.type,k=p.stateNode;if(0===(64&p.flags)&&("function"===typeof x.getDerivedStateFromError||null!==k&&"function"===typeof k.componentDidCatch&&(null===Qd||!Qd.has(k)))){p.flags|=4096,t&=-t,p.lanes|=t,fi(p,fd(p,i,t));break e}}p=p.return}while(null!==p)}Ms(a)}catch(R){t=R,_d===a&&null!==a&&(_d=a=a.return);continue}break}}function Fs(){var e=Ed.current;return Ed.current=Pr,null===e?Pr:e}function Ts(e,t){var a=Pd;Pd|=16;var n=Fs();for(Hd===e&&zd===t||Is(e,t);;)try{Ss();break}catch(o){As(e,o)}if(ni(),Pd=a,Ed.current=n,null!==_d)throw Error(r(261));return Hd=null,zd=0,Ld}function Ss(){for(;null!==_d;)Es(_d)}function Cs(){for(;null!==_d&&!Co();)Es(_d)}function Es(e){var t=Yd(e.alternate,e,jd);e.memoizedProps=e.pendingProps,null===t?Ms(e):_d=t,Md.current=null}function Ms(e){var t=e;do{var a=t.alternate;if(e=t.return,0===(2048&t.flags)){if(null!==(a=dd(a,t,jd)))return void(_d=a);if(24!==(a=t).tag&&23!==a.tag||null===a.memoizedState||0!==(1073741824&jd)||0===(4&a.mode)){for(var n=0,o=a.child;null!==o;)n|=o.lanes|o.childLanes,o=o.sibling;a.childLanes=n}null!==e&&0===(2048&e.flags)&&(null===e.firstEffect&&(e.firstEffect=t.firstEffect),null!==t.lastEffect&&(null!==e.lastEffect&&(e.lastEffect.nextEffect=t.firstEffect),e.lastEffect=t.lastEffect),1<t.flags&&(null!==e.lastEffect?e.lastEffect.nextEffect=t:e.firstEffect=t,e.lastEffect=t))}else{if(null!==(a=sd(t)))return a.flags&=2047,void(_d=a);null!==e&&(e.firstEffect=e.lastEffect=null,e.flags|=2048)}if(null!==(t=t.sibling))return void(_d=t);_d=t=e}while(null!==t);0===Ld&&(Ld=5)}function Ps(e){var t=Go();return Jo(99,Hs.bind(null,e,t)),null}function Hs(e,t){do{zs()}while(null!==es);if(0!==(48&Pd))throw Error(r(327));var a=e.finishedWork;if(null===a)return null;if(e.finishedWork=null,e.finishedLanes=0,a===e.current)throw Error(r(177));e.callbackNode=null;var n=a.lanes|a.childLanes,o=n,i=e.pendingLanes&~o;e.pendingLanes=o,e.suspendedLanes=0,e.pingedLanes=0,e.expiredLanes&=o,e.mutableReadLanes&=o,e.entangledLanes&=o,o=e.entanglements;for(var d=e.eventTimes,s=e.expirationTimes;0<i;){var c=31-Kt(i),l=1<<c;o[c]=0,d[c]=-1,s[c]=-1,i&=~l}if(null!==os&&0===(24&n)&&os.has(e)&&os.delete(e),e===Hd&&(_d=Hd=null,zd=0),1<a.flags?null!==a.lastEffect?(a.lastEffect.nextEffect=a,n=a.firstEffect):n=a:n=a.firstEffect,null!==n){if(o=Pd,Pd|=32,Md.current=null,Nn=qt,mn(d=gn())){if("selectionStart"in d)s={start:d.selectionStart,end:d.selectionEnd};else e:if(s=(s=d.ownerDocument)&&s.defaultView||window,(l=s.getSelection&&s.getSelection())&&0!==l.rangeCount){s=l.anchorNode,i=l.anchorOffset,c=l.focusNode,l=l.focusOffset;try{s.nodeType,c.nodeType}catch(F){s=null;break e}var u=0,p=-1,f=-1,h=0,b=0,g=d,m=null;t:for(;;){for(var y;g!==s||0!==i&&3!==g.nodeType||(p=u+i),g!==c||0!==l&&3!==g.nodeType||(f=u+l),3===g.nodeType&&(u+=g.nodeValue.length),null!==(y=g.firstChild);)m=g,g=y;for(;;){if(g===d)break t;if(m===s&&++h===i&&(p=u),m===c&&++b===l&&(f=u),null!==(y=g.nextSibling))break;m=(g=m).parentNode}g=y}s=-1===p||-1===f?null:{start:p,end:f}}else s=null;s=s||{start:0,end:0}}else s=null;Wn={focusedElem:d,selectionRange:s},qt=!1,ls=null,us=!1,qd=n;do{try{_s()}catch(F){if(null===qd)throw Error(r(330));Os(qd,F),qd=qd.nextEffect}}while(null!==qd);ls=null,qd=n;do{try{for(d=e;null!==qd;){var v=qd.flags;if(16&v&&ye(qd.stateNode,""),128&v){var w=qd.alternate;if(null!==w){var x=w.ref;null!==x&&("function"===typeof x?x(null):x.current=null)}}switch(1038&v){case 2:kd(qd),qd.flags&=-3;break;case 6:kd(qd),qd.flags&=-3,Fd(qd.alternate,qd);break;case 1024:qd.flags&=-1025;break;case 1028:qd.flags&=-1025,Fd(qd.alternate,qd);break;case 4:Fd(qd.alternate,qd);break;case 8:Ad(d,s=qd);var k=s.alternate;wd(s),null!==k&&wd(k)}qd=qd.nextEffect}}catch(F){if(null===qd)throw Error(r(330));Os(qd,F),qd=qd.nextEffect}}while(null!==qd);if(x=Wn,w=gn(),v=x.focusedElem,d=x.selectionRange,w!==v&&v&&v.ownerDocument&&bn(v.ownerDocument.documentElement,v)){null!==d&&mn(v)&&(w=d.start,void 0===(x=d.end)&&(x=w),"selectionStart"in v?(v.selectionStart=w,v.selectionEnd=Math.min(x,v.value.length)):(x=(w=v.ownerDocument||document)&&w.defaultView||window).getSelection&&(x=x.getSelection(),s=v.textContent.length,k=Math.min(d.start,s),d=void 0===d.end?k:Math.min(d.end,s),!x.extend&&k>d&&(s=d,d=k,k=s),s=hn(v,k),i=hn(v,d),s&&i&&(1!==x.rangeCount||x.anchorNode!==s.node||x.anchorOffset!==s.offset||x.focusNode!==i.node||x.focusOffset!==i.offset)&&((w=w.createRange()).setStart(s.node,s.offset),x.removeAllRanges(),k>d?(x.addRange(w),x.extend(i.node,i.offset)):(w.setEnd(i.node,i.offset),x.addRange(w))))),w=[];for(x=v;x=x.parentNode;)1===x.nodeType&&w.push({element:x,left:x.scrollLeft,top:x.scrollTop});for("function"===typeof v.focus&&v.focus(),v=0;v<w.length;v++)(x=w[v]).element.scrollLeft=x.left,x.element.scrollTop=x.top}qt=!!Nn,Wn=Nn=null,e.current=a,qd=n;do{try{for(v=e;null!==qd;){var R=qd.flags;if(36&R&&md(v,qd.alternate,qd),128&R){w=void 0;var I=qd.ref;if(null!==I){var A=qd.stateNode;qd.tag,w=A,"function"===typeof I?I(w):I.current=w}}qd=qd.nextEffect}}catch(F){if(null===qd)throw Error(r(330));Os(qd,F),qd=qd.nextEffect}}while(null!==qd);qd=null,Uo(),Pd=o}else e.current=a;if(Xd)Xd=!1,es=e,ts=t;else for(qd=n;null!==qd;)t=qd.nextEffect,qd.nextEffect=null,8&qd.flags&&((R=qd).sibling=null,R.stateNode=null),qd=t;if(0===(n=e.pendingLanes)&&(Qd=null),1===n?e===rs?is++:(is=0,rs=e):is=0,a=a.stateNode,Ao&&"function"===typeof Ao.onCommitFiberRoot)try{Ao.onCommitFiberRoot(Io,a,void 0,64===(64&a.current.flags))}catch(F){}if(gs(e,Ko()),Zd)throw Zd=!1,e=$d,$d=null,e;return 0!==(8&Pd)||qo(),null}function _s(){for(;null!==qd;){var e=qd.alternate;us||null===ls||(0!==(8&qd.flags)?et(qd,ls)&&(us=!0):13===qd.tag&&Sd(e,qd)&&et(qd,ls)&&(us=!0));var t=qd.flags;0!==(256&t)&&gd(e,qd),0===(512&t)||Xd||(Xd=!0,Yo(97,(function(){return zs(),null}))),qd=qd.nextEffect}}function zs(){if(90!==ts){var e=97<ts?97:ts;return ts=90,Jo(e,Ls)}return!1}function js(e,t){as.push(t,e),Xd||(Xd=!0,Yo(97,(function(){return zs(),null})))}function Ds(e,t){ns.push(t,e),Xd||(Xd=!0,Yo(97,(function(){return zs(),null})))}function Ls(){if(null===es)return!1;var e=es;if(es=null,0!==(48&Pd))throw Error(r(331));var t=Pd;Pd|=32;var a=ns;ns=[];for(var n=0;n<a.length;n+=2){var o=a[n],i=a[n+1],d=o.destroy;if(o.destroy=void 0,"function"===typeof d)try{d()}catch(c){if(null===i)throw Error(r(330));Os(i,c)}}for(a=as,as=[],n=0;n<a.length;n+=2){o=a[n],i=a[n+1];try{var s=o.create;o.destroy=s()}catch(c){if(null===i)throw Error(r(330));Os(i,c)}}for(s=e.current.firstEffect;null!==s;)e=s.nextEffect,s.nextEffect=null,8&s.flags&&(s.sibling=null,s.stateNode=null),s=e;return Pd=t,qo(),!0}function Us(e,t,a){pi(e,t=pd(0,t=cd(a,t),1)),t=ps(),null!==(e=bs(e,1))&&(Bt(e,1,t),gs(e,t))}function Os(e,t){if(3===e.tag)Us(e,e,t);else for(var a=e.return;null!==a;){if(3===a.tag){Us(a,e,t);break}if(1===a.tag){var n=a.stateNode;if("function"===typeof a.type.getDerivedStateFromError||"function"===typeof n.componentDidCatch&&(null===Qd||!Qd.has(n))){var o=fd(a,e=cd(t,e),1);if(pi(a,o),o=ps(),null!==(a=bs(a,1)))Bt(a,1,o),gs(a,o);else if("function"===typeof n.componentDidCatch&&(null===Qd||!Qd.has(n)))try{n.componentDidCatch(t,e)}catch(i){}break}}a=a.return}}function Ns(e,t,a){var n=e.pingCache;null!==n&&n.delete(t),t=ps(),e.pingedLanes|=e.suspendedLanes&a,Hd===e&&(zd&a)===a&&(4===Ld||3===Ld&&(62914560&zd)===zd&&500>Ko()-Gd?Is(e,0):Bd|=a),gs(e,t)}function Ws(e,t){var a=e.stateNode;null!==a&&a.delete(t),0===(t=0)&&(0===(2&(t=e.mode))?t=1:0===(4&t)?t=99===Go()?1:2:(0===ss&&(ss=Od),0===(t=Nt(62914560&~ss))&&(t=4194304))),a=ps(),null!==(e=bs(e,t))&&(Bt(e,t,a),gs(e,a))}function Bs(e,t,a,n){this.tag=e,this.key=a,this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null,this.index=0,this.ref=null,this.pendingProps=t,this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null,this.mode=n,this.flags=0,this.lastEffect=this.firstEffect=this.nextEffect=null,this.childLanes=this.lanes=0,this.alternate=null}function Ks(e,t,a,n){return new Bs(e,t,a,n)}function Gs(e){return!(!(e=e.prototype)||!e.isReactComponent)}function Vs(e,t){var a=e.alternate;return null===a?((a=Ks(e.tag,t,e.key,e.mode)).elementType=e.elementType,a.type=e.type,a.stateNode=e.stateNode,a.alternate=e,e.alternate=a):(a.pendingProps=t,a.type=e.type,a.flags=0,a.nextEffect=null,a.firstEffect=null,a.lastEffect=null),a.childLanes=e.childLanes,a.lanes=e.lanes,a.child=e.child,a.memoizedProps=e.memoizedProps,a.memoizedState=e.memoizedState,a.updateQueue=e.updateQueue,t=e.dependencies,a.dependencies=null===t?null:{lanes:t.lanes,firstContext:t.firstContext},a.sibling=e.sibling,a.index=e.index,a.ref=e.ref,a}function Js(e,t,a,n,o,i){var d=2;if(n=e,"function"===typeof e)Gs(e)&&(d=1);else if("string"===typeof e)d=5;else e:switch(e){case I:return Ys(a.children,o,i,t);case j:d=8,o|=16;break;case A:d=8,o|=1;break;case F:return(e=Ks(12,a,t,8|o)).elementType=F,e.type=F,e.lanes=i,e;case E:return(e=Ks(13,a,t,o)).type=E,e.elementType=E,e.lanes=i,e;case M:return(e=Ks(19,a,t,o)).elementType=M,e.lanes=i,e;case D:return qs(a,o,i,t);case L:return(e=Ks(24,a,t,o)).elementType=L,e.lanes=i,e;default:if("object"===typeof e&&null!==e)switch(e.$$typeof){case T:d=10;break e;case S:d=9;break e;case C:d=11;break e;case P:d=14;break e;case H:d=16,n=null;break e;case _:d=22;break e}throw Error(r(130,null==e?e:typeof e,""))}return(t=Ks(d,a,t,o)).elementType=e,t.type=n,t.lanes=i,t}function Ys(e,t,a,n){return(e=Ks(7,e,n,t)).lanes=a,e}function qs(e,t,a,n){return(e=Ks(23,e,n,t)).elementType=D,e.lanes=a,e}function Zs(e,t,a){return(e=Ks(6,e,null,t)).lanes=a,e}function $s(e,t,a){return(t=Ks(4,null!==e.children?e.children:[],e.key,t)).lanes=a,t.stateNode={containerInfo:e.containerInfo,pendingChildren:null,implementation:e.implementation},t}function Qs(e,t,a){this.tag=t,this.containerInfo=e,this.finishedWork=this.pingCache=this.current=this.pendingChildren=null,this.timeoutHandle=-1,this.pendingContext=this.context=null,this.hydrate=a,this.callbackNode=null,this.callbackPriority=0,this.eventTimes=Wt(0),this.expirationTimes=Wt(-1),this.entangledLanes=this.finishedLanes=this.mutableReadLanes=this.expiredLanes=this.pingedLanes=this.suspendedLanes=this.pendingLanes=0,this.entanglements=Wt(0),this.mutableSourceEagerHydrationData=null}function Xs(e,t,a){var n=3<arguments.length&&void 0!==arguments[3]?arguments[3]:null;return{$$typeof:R,key:null==n?null:""+n,children:e,containerInfo:t,implementation:a}}function ec(e,t,a,n){var o=t.current,i=ps(),d=fs(o);e:if(a){t:{if(Ze(a=a._reactInternals)!==a||1!==a.tag)throw Error(r(170));var s=a;do{switch(s.tag){case 3:s=s.stateNode.context;break t;case 1:if(yo(s.type)){s=s.stateNode.__reactInternalMemoizedMergedChildContext;break t}}s=s.return}while(null!==s);throw Error(r(171))}if(1===a.tag){var c=a.type;if(yo(c)){a=xo(a,c,s);break e}}a=s}else a=fo;return null===t.context?t.context=a:t.pendingContext=a,(t=ui(i,d)).payload={element:e},null!==(n=void 0===n?null:n)&&(t.callback=n),pi(o,t),hs(o,d,i),d}function tc(e){return(e=e.current).child?(e.child.tag,e.child.stateNode):null}function ac(e,t){if(null!==(e=e.memoizedState)&&null!==e.dehydrated){var a=e.retryLane;e.retryLane=0!==a&&a<t?a:t}}function nc(e,t){ac(e,t),(e=e.alternate)&&ac(e,t)}function oc(e,t,a){var n=null!=a&&null!=a.hydrationOptions&&a.hydrationOptions.mutableSources||null;if(a=new Qs(e,t,null!=a&&!0===a.hydrate),t=Ks(3,null,null,2===t?7:1===t?3:0),a.current=t,t.stateNode=a,ci(t),e[eo]=a.current,Pn(8===e.nodeType?e.parentNode:e),n)for(e=0;e<n.length;e++){var o=(t=n[e])._getVersion;o=o(t._source),null==a.mutableSourceEagerHydrationData?a.mutableSourceEagerHydrationData=[t,o]:a.mutableSourceEagerHydrationData.push(t,o)}this._internalRoot=a}function ic(e){return!(!e||1!==e.nodeType&&9!==e.nodeType&&11!==e.nodeType&&(8!==e.nodeType||" react-mount-point-unstable "!==e.nodeValue))}function rc(e,t,a,n,o){var i=a._reactRootContainer;if(i){var r=i._internalRoot;if("function"===typeof o){var d=o;o=function(){var e=tc(r);d.call(e)}}ec(t,r,e,o)}else{if(i=a._reactRootContainer=function(e,t){if(t||(t=!(!(t=e?9===e.nodeType?e.documentElement:e.firstChild:null)||1!==t.nodeType||!t.hasAttribute("data-reactroot"))),!t)for(var a;a=e.lastChild;)e.removeChild(a);return new oc(e,0,t?{hydrate:!0}:void 0)}(a,n),r=i._internalRoot,"function"===typeof o){var s=o;o=function(){var e=tc(r);s.call(e)}}xs((function(){ec(t,r,e,o)}))}return tc(r)}function dc(e,t){var a=2<arguments.length&&void 0!==arguments[2]?arguments[2]:null;if(!ic(t))throw Error(r(200));return Xs(e,t,null,a)}Yd=function(e,t,a){var n=t.lanes;if(null!==e)if(e.memoizedProps!==t.pendingProps||bo.current)Dr=!0;else{if(0===(a&n)){switch(Dr=!1,t.tag){case 3:Jr(t),Yi();break;case 5:ji(t);break;case 1:yo(t.type)&&ko(t);break;case 4:_i(t,t.stateNode.containerInfo);break;case 10:n=t.memoizedProps.value;var o=t.type._context;po(Xo,o._currentValue),o._currentValue=n;break;case 13:if(null!==t.memoizedState)return 0!==(a&t.child.childLanes)?Qr(e,t,a):(po(Li,1&Li.current),null!==(t=id(e,t,a))?t.sibling:null);po(Li,1&Li.current);break;case 19:if(n=0!==(a&t.childLanes),0!==(64&e.flags)){if(n)return od(e,t,a);t.flags|=64}if(null!==(o=t.memoizedState)&&(o.rendering=null,o.tail=null,o.lastEffect=null),po(Li,Li.current),n)break;return null;case 23:case 24:return t.lanes=0,Wr(e,t,a)}return id(e,t,a)}Dr=0!==(16384&e.flags)}else Dr=!1;switch(t.lanes=0,t.tag){case 2:if(n=t.type,null!==e&&(e.alternate=null,t.alternate=null,t.flags|=2),e=t.pendingProps,o=mo(t,ho.current),ri(t,a),o=dr(null,t,n,e,o,a),t.flags|=1,"object"===typeof o&&null!==o&&"function"===typeof o.render&&void 0===o.$$typeof){if(t.tag=1,t.memoizedState=null,t.updateQueue=null,yo(n)){var i=!0;ko(t)}else i=!1;t.memoizedState=null!==o.state&&void 0!==o.state?o.state:null,ci(t);var d=n.getDerivedStateFromProps;"function"===typeof d&&mi(t,n,d,e),o.updater=yi,t.stateNode=o,o._reactInternals=t,ki(t,n,e,a),t=Vr(null,t,n,!0,i,a)}else t.tag=0,Lr(null,t,o,a),t=t.child;return t;case 16:o=t.elementType;e:{switch(null!==e&&(e.alternate=null,t.alternate=null,t.flags|=2),e=t.pendingProps,o=(i=o._init)(o._payload),t.type=o,i=t.tag=function(e){if("function"===typeof e)return Gs(e)?1:0;if(void 0!==e&&null!==e){if((e=e.$$typeof)===C)return 11;if(e===P)return 14}return 2}(o),e=Qo(o,e),i){case 0:t=Kr(null,t,o,e,a);break e;case 1:t=Gr(null,t,o,e,a);break e;case 11:t=Ur(null,t,o,e,a);break e;case 14:t=Or(null,t,o,Qo(o.type,e),n,a);break e}throw Error(r(306,o,""))}return t;case 0:return n=t.type,o=t.pendingProps,Kr(e,t,n,o=t.elementType===n?o:Qo(n,o),a);case 1:return n=t.type,o=t.pendingProps,Gr(e,t,n,o=t.elementType===n?o:Qo(n,o),a);case 3:if(Jr(t),n=t.updateQueue,null===e||null===n)throw Error(r(282));if(n=t.pendingProps,o=null!==(o=t.memoizedState)?o.element:null,li(e,t),hi(t,n,null,a),(n=t.memoizedState.element)===o)Yi(),t=id(e,t,a);else{if((i=(o=t.stateNode).hydrate)&&(Ni=Yn(t.stateNode.containerInfo.firstChild),Oi=t,i=Wi=!0),i){if(null!=(e=o.mutableSourceEagerHydrationData))for(o=0;o<e.length;o+=2)(i=e[o])._workInProgressVersionPrimary=e[o+1],qi.push(i);for(a=Si(t,null,n,a),t.child=a;a;)a.flags=-3&a.flags|1024,a=a.sibling}else Lr(e,t,n,a),Yi();t=t.child}return t;case 5:return ji(t),null===e&&Gi(t),n=t.type,o=t.pendingProps,i=null!==e?e.memoizedProps:null,d=o.children,Kn(n,o)?d=null:null!==i&&Kn(n,i)&&(t.flags|=16),Br(e,t),Lr(e,t,d,a),t.child;case 6:return null===e&&Gi(t),null;case 13:return Qr(e,t,a);case 4:return _i(t,t.stateNode.containerInfo),n=t.pendingProps,null===e?t.child=Ti(t,null,n,a):Lr(e,t,n,a),t.child;case 11:return n=t.type,o=t.pendingProps,Ur(e,t,n,o=t.elementType===n?o:Qo(n,o),a);case 7:return Lr(e,t,t.pendingProps,a),t.child;case 8:case 12:return Lr(e,t,t.pendingProps.children,a),t.child;case 10:e:{n=t.type._context,o=t.pendingProps,d=t.memoizedProps,i=o.value;var s=t.type._context;if(po(Xo,s._currentValue),s._currentValue=i,null!==d)if(s=d.value,0===(i=ln(s,i)?0:0|("function"===typeof n._calculateChangedBits?n._calculateChangedBits(s,i):1073741823))){if(d.children===o.children&&!bo.current){t=id(e,t,a);break e}}else for(null!==(s=t.child)&&(s.return=t);null!==s;){var c=s.dependencies;if(null!==c){d=s.child;for(var l=c.firstContext;null!==l;){if(l.context===n&&0!==(l.observedBits&i)){1===s.tag&&((l=ui(-1,a&-a)).tag=2,pi(s,l)),s.lanes|=a,null!==(l=s.alternate)&&(l.lanes|=a),ii(s.return,a),c.lanes|=a;break}l=l.next}}else d=10===s.tag&&s.type===t.type?null:s.child;if(null!==d)d.return=s;else for(d=s;null!==d;){if(d===t){d=null;break}if(null!==(s=d.sibling)){s.return=d.return,d=s;break}d=d.return}s=d}Lr(e,t,o.children,a),t=t.child}return t;case 9:return o=t.type,n=(i=t.pendingProps).children,ri(t,a),n=n(o=di(o,i.unstable_observedBits)),t.flags|=1,Lr(e,t,n,a),t.child;case 14:return i=Qo(o=t.type,t.pendingProps),Or(e,t,o,i=Qo(o.type,i),n,a);case 15:return Nr(e,t,t.type,t.pendingProps,n,a);case 17:return n=t.type,o=t.pendingProps,o=t.elementType===n?o:Qo(n,o),null!==e&&(e.alternate=null,t.alternate=null,t.flags|=2),t.tag=1,yo(n)?(e=!0,ko(t)):e=!1,ri(t,a),wi(t,n,o),ki(t,n,o,a),Vr(null,t,n,!0,e,a);case 19:return od(e,t,a);case 23:case 24:return Wr(e,t,a)}throw Error(r(156,t.tag))},oc.prototype.render=function(e){ec(e,this._internalRoot,null,null)},oc.prototype.unmount=function(){var e=this._internalRoot,t=e.containerInfo;ec(null,e,null,(function(){t[eo]=null}))},tt=function(e){13===e.tag&&(hs(e,4,ps()),nc(e,4))},at=function(e){13===e.tag&&(hs(e,67108864,ps()),nc(e,67108864))},nt=function(e){if(13===e.tag){var t=ps(),a=fs(e);hs(e,a,t),nc(e,a)}},ot=function(e,t){return t()},Te=function(e,t,a){switch(t){case"input":if(ae(e,a),t=a.name,"radio"===a.type&&null!=t){for(a=e;a.parentNode;)a=a.parentNode;for(a=a.querySelectorAll("input[name="+JSON.stringify(""+t)+'][type="radio"]'),t=0;t<a.length;t++){var n=a[t];if(n!==e&&n.form===e.form){var o=io(n);if(!o)throw Error(r(90));$(n),ae(n,o)}}}break;case"textarea":ce(e,a);break;case"select":null!=(t=a.value)&&re(e,!!a.multiple,t,!1)}},He=ws,_e=function(e,t,a,n,o){var i=Pd;Pd|=4;try{return Jo(98,e.bind(null,t,a,n,o))}finally{0===(Pd=i)&&(Jd(),qo())}},ze=function(){0===(49&Pd)&&(function(){if(null!==os){var e=os;os=null,e.forEach((function(e){e.expiredLanes|=24&e.pendingLanes,gs(e,Ko())}))}qo()}(),zs())},je=function(e,t){var a=Pd;Pd|=2;try{return e(t)}finally{0===(Pd=a)&&(Jd(),qo())}};var sc={Events:[no,oo,io,Me,Pe,zs,{current:!1}]},cc={findFiberByHostInstance:ao,bundleType:0,version:"17.0.2",rendererPackageName:"react-dom"},lc={bundleType:cc.bundleType,version:cc.version,rendererPackageName:cc.rendererPackageName,rendererConfig:cc.rendererConfig,overrideHookState:null,overrideHookStateDeletePath:null,overrideHookStateRenamePath:null,overrideProps:null,overridePropsDeletePath:null,overridePropsRenamePath:null,setSuspenseHandler:null,scheduleUpdate:null,currentDispatcherRef:x.ReactCurrentDispatcher,findHostInstanceByFiber:function(e){return null===(e=Xe(e))?null:e.stateNode},findFiberByHostInstance:cc.findFiberByHostInstance||function(){return null},findHostInstancesForRefresh:null,scheduleRefresh:null,scheduleRoot:null,setRefreshHandler:null,getCurrentFiber:null};if("undefined"!==typeof __REACT_DEVTOOLS_GLOBAL_HOOK__){var uc=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(!uc.isDisabled&&uc.supportsFiber)try{Io=uc.inject(lc),Ao=uc}catch(ge){}}t.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=sc,t.createPortal=dc,t.findDOMNode=function(e){if(null==e)return null;if(1===e.nodeType)return e;var t=e._reactInternals;if(void 0===t){if("function"===typeof e.render)throw Error(r(188));throw Error(r(268,Object.keys(e)))}return e=null===(e=Xe(t))?null:e.stateNode},t.flushSync=function(e,t){var a=Pd;if(0!==(48&a))return e(t);Pd|=1;try{if(e)return Jo(99,e.bind(null,t))}finally{Pd=a,qo()}},t.hydrate=function(e,t,a){if(!ic(t))throw Error(r(200));return rc(null,e,t,!0,a)},t.render=function(e,t,a){if(!ic(t))throw Error(r(200));return rc(null,e,t,!1,a)},t.unmountComponentAtNode=function(e){if(!ic(e))throw Error(r(40));return!!e._reactRootContainer&&(xs((function(){rc(null,null,e,!1,(function(){e._reactRootContainer=null,e[eo]=null}))})),!0)},t.unstable_batchedUpdates=ws,t.unstable_createPortal=function(e,t){return dc(e,t,2<arguments.length&&void 0!==arguments[2]?arguments[2]:null)},t.unstable_renderSubtreeIntoContainer=function(e,t,a,n){if(!ic(a))throw Error(r(200));if(null==e||void 0===e._reactInternals)throw Error(r(38));return rc(e,t,a,!1,n)},t.version="17.0.2"},164:function(e,t,a){"use strict";!function e(){if("undefined"!==typeof __REACT_DEVTOOLS_GLOBAL_HOOK__&&"function"===typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE)try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(e)}catch(t){console.error(t)}}(),e.exports=a(463)},374:function(e,t,a){"use strict";a(725);var n=a(791),o=60103;if(t.Fragment=60107,"function"===typeof Symbol&&Symbol.for){var i=Symbol.for;o=i("react.element"),t.Fragment=i("react.fragment")}var r=n.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED.ReactCurrentOwner,d=Object.prototype.hasOwnProperty,s={key:!0,ref:!0,__self:!0,__source:!0};function c(e,t,a){var n,i={},c=null,l=null;for(n in void 0!==a&&(c=""+a),void 0!==t.key&&(c=""+t.key),void 0!==t.ref&&(l=t.ref),t)d.call(t,n)&&!s.hasOwnProperty(n)&&(i[n]=t[n]);if(e&&e.defaultProps)for(n in t=e.defaultProps)void 0===i[n]&&(i[n]=t[n]);return{$$typeof:o,type:e,key:c,ref:l,props:i,_owner:r.current}}t.jsx=c,t.jsxs=c},117:function(e,t,a){"use strict";var n=a(725),o=60103,i=60106;t.Fragment=60107,t.StrictMode=60108,t.Profiler=60114;var r=60109,d=60110,s=60112;t.Suspense=60113;var c=60115,l=60116;if("function"===typeof Symbol&&Symbol.for){var u=Symbol.for;o=u("react.element"),i=u("react.portal"),t.Fragment=u("react.fragment"),t.StrictMode=u("react.strict_mode"),t.Profiler=u("react.profiler"),r=u("react.provider"),d=u("react.context"),s=u("react.forward_ref"),t.Suspense=u("react.suspense"),c=u("react.memo"),l=u("react.lazy")}var p="function"===typeof Symbol&&Symbol.iterator;function f(e){for(var t="https://reactjs.org/docs/error-decoder.html?invariant="+e,a=1;a<arguments.length;a++)t+="&args[]="+encodeURIComponent(arguments[a]);return"Minified React error #"+e+"; visit "+t+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}var h={isMounted:function(){return!1},enqueueForceUpdate:function(){},enqueueReplaceState:function(){},enqueueSetState:function(){}},b={};function g(e,t,a){this.props=e,this.context=t,this.refs=b,this.updater=a||h}function m(){}function y(e,t,a){this.props=e,this.context=t,this.refs=b,this.updater=a||h}g.prototype.isReactComponent={},g.prototype.setState=function(e,t){if("object"!==typeof e&&"function"!==typeof e&&null!=e)throw Error(f(85));this.updater.enqueueSetState(this,e,t,"setState")},g.prototype.forceUpdate=function(e){this.updater.enqueueForceUpdate(this,e,"forceUpdate")},m.prototype=g.prototype;var v=y.prototype=new m;v.constructor=y,n(v,g.prototype),v.isPureReactComponent=!0;var w={current:null},x=Object.prototype.hasOwnProperty,k={key:!0,ref:!0,__self:!0,__source:!0};function R(e,t,a){var n,i={},r=null,d=null;if(null!=t)for(n in void 0!==t.ref&&(d=t.ref),void 0!==t.key&&(r=""+t.key),t)x.call(t,n)&&!k.hasOwnProperty(n)&&(i[n]=t[n]);var s=arguments.length-2;if(1===s)i.children=a;else if(1<s){for(var c=Array(s),l=0;l<s;l++)c[l]=arguments[l+2];i.children=c}if(e&&e.defaultProps)for(n in s=e.defaultProps)void 0===i[n]&&(i[n]=s[n]);return{$$typeof:o,type:e,key:r,ref:d,props:i,_owner:w.current}}function I(e){return"object"===typeof e&&null!==e&&e.$$typeof===o}var A=/\/+/g;function F(e,t){return"object"===typeof e&&null!==e&&null!=e.key?function(e){var t={"=":"=0",":":"=2"};return"$"+e.replace(/[=:]/g,(function(e){return t[e]}))}(""+e.key):t.toString(36)}function T(e,t,a,n,r){var d=typeof e;"undefined"!==d&&"boolean"!==d||(e=null);var s=!1;if(null===e)s=!0;else switch(d){case"string":case"number":s=!0;break;case"object":switch(e.$$typeof){case o:case i:s=!0}}if(s)return r=r(s=e),e=""===n?"."+F(s,0):n,Array.isArray(r)?(a="",null!=e&&(a=e.replace(A,"$&/")+"/"),T(r,t,a,"",(function(e){return e}))):null!=r&&(I(r)&&(r=function(e,t){return{$$typeof:o,type:e.type,key:t,ref:e.ref,props:e.props,_owner:e._owner}}(r,a+(!r.key||s&&s.key===r.key?"":(""+r.key).replace(A,"$&/")+"/")+e)),t.push(r)),1;if(s=0,n=""===n?".":n+":",Array.isArray(e))for(var c=0;c<e.length;c++){var l=n+F(d=e[c],c);s+=T(d,t,a,l,r)}else if(l=function(e){return null===e||"object"!==typeof e?null:"function"===typeof(e=p&&e[p]||e["@@iterator"])?e:null}(e),"function"===typeof l)for(e=l.call(e),c=0;!(d=e.next()).done;)s+=T(d=d.value,t,a,l=n+F(d,c++),r);else if("object"===d)throw t=""+e,Error(f(31,"[object Object]"===t?"object with keys {"+Object.keys(e).join(", ")+"}":t));return s}function S(e,t,a){if(null==e)return e;var n=[],o=0;return T(e,n,"","",(function(e){return t.call(a,e,o++)})),n}function C(e){if(-1===e._status){var t=e._result;t=t(),e._status=0,e._result=t,t.then((function(t){0===e._status&&(t=t.default,e._status=1,e._result=t)}),(function(t){0===e._status&&(e._status=2,e._result=t)}))}if(1===e._status)return e._result;throw e._result}var E={current:null};function M(){var e=E.current;if(null===e)throw Error(f(321));return e}var P={ReactCurrentDispatcher:E,ReactCurrentBatchConfig:{transition:0},ReactCurrentOwner:w,IsSomeRendererActing:{current:!1},assign:n};t.Children={map:S,forEach:function(e,t,a){S(e,(function(){t.apply(this,arguments)}),a)},count:function(e){var t=0;return S(e,(function(){t++})),t},toArray:function(e){return S(e,(function(e){return e}))||[]},only:function(e){if(!I(e))throw Error(f(143));return e}},t.Component=g,t.PureComponent=y,t.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=P,t.cloneElement=function(e,t,a){if(null===e||void 0===e)throw Error(f(267,e));var i=n({},e.props),r=e.key,d=e.ref,s=e._owner;if(null!=t){if(void 0!==t.ref&&(d=t.ref,s=w.current),void 0!==t.key&&(r=""+t.key),e.type&&e.type.defaultProps)var c=e.type.defaultProps;for(l in t)x.call(t,l)&&!k.hasOwnProperty(l)&&(i[l]=void 0===t[l]&&void 0!==c?c[l]:t[l])}var l=arguments.length-2;if(1===l)i.children=a;else if(1<l){c=Array(l);for(var u=0;u<l;u++)c[u]=arguments[u+2];i.children=c}return{$$typeof:o,type:e.type,key:r,ref:d,props:i,_owner:s}},t.createContext=function(e,t){return void 0===t&&(t=null),(e={$$typeof:d,_calculateChangedBits:t,_currentValue:e,_currentValue2:e,_threadCount:0,Provider:null,Consumer:null}).Provider={$$typeof:r,_context:e},e.Consumer=e},t.createElement=R,t.createFactory=function(e){var t=R.bind(null,e);return t.type=e,t},t.createRef=function(){return{current:null}},t.forwardRef=function(e){return{$$typeof:s,render:e}},t.isValidElement=I,t.lazy=function(e){return{$$typeof:l,_payload:{_status:-1,_result:e},_init:C}},t.memo=function(e,t){return{$$typeof:c,type:e,compare:void 0===t?null:t}},t.useCallback=function(e,t){return M().useCallback(e,t)},t.useContext=function(e,t){return M().useContext(e,t)},t.useDebugValue=function(){},t.useEffect=function(e,t){return M().useEffect(e,t)},t.useImperativeHandle=function(e,t,a){return M().useImperativeHandle(e,t,a)},t.useLayoutEffect=function(e,t){return M().useLayoutEffect(e,t)},t.useMemo=function(e,t){return M().useMemo(e,t)},t.useReducer=function(e,t,a){return M().useReducer(e,t,a)},t.useRef=function(e){return M().useRef(e)},t.useState=function(e){return M().useState(e)},t.version="17.0.2"},791:function(e,t,a){"use strict";e.exports=a(117)},184:function(e,t,a){"use strict";e.exports=a(374)},813:function(e,t){"use strict";var a,n,o,i;if("object"===typeof performance&&"function"===typeof performance.now){var r=performance;t.unstable_now=function(){return r.now()}}else{var d=Date,s=d.now();t.unstable_now=function(){return d.now()-s}}if("undefined"===typeof window||"function"!==typeof MessageChannel){var c=null,l=null,u=function e(){if(null!==c)try{var a=t.unstable_now();c(!0,a),c=null}catch(n){throw setTimeout(e,0),n}};a=function(e){null!==c?setTimeout(a,0,e):(c=e,setTimeout(u,0))},n=function(e,t){l=setTimeout(e,t)},o=function(){clearTimeout(l)},t.unstable_shouldYield=function(){return!1},i=t.unstable_forceFrameRate=function(){}}else{var p=window.setTimeout,f=window.clearTimeout;if("undefined"!==typeof console){var h=window.cancelAnimationFrame;"function"!==typeof window.requestAnimationFrame&&console.error("This browser doesn't support requestAnimationFrame. Make sure that you load a polyfill in older browsers. https://reactjs.org/link/react-polyfills"),"function"!==typeof h&&console.error("This browser doesn't support cancelAnimationFrame. Make sure that you load a polyfill in older browsers. https://reactjs.org/link/react-polyfills")}var b=!1,g=null,m=-1,y=5,v=0;t.unstable_shouldYield=function(){return t.unstable_now()>=v},i=function(){},t.unstable_forceFrameRate=function(e){0>e||125<e?console.error("forceFrameRate takes a positive int between 0 and 125, forcing frame rates higher than 125 fps is not supported"):y=0<e?Math.floor(1e3/e):5};var w=new MessageChannel,x=w.port2;w.port1.onmessage=function(){if(null!==g){var e=t.unstable_now();v=e+y;try{g(!0,e)?x.postMessage(null):(b=!1,g=null)}catch(a){throw x.postMessage(null),a}}else b=!1},a=function(e){g=e,b||(b=!0,x.postMessage(null))},n=function(e,a){m=p((function(){e(t.unstable_now())}),a)},o=function(){f(m),m=-1}}function k(e,t){var a=e.length;e.push(t);e:for(;;){var n=a-1>>>1,o=e[n];if(!(void 0!==o&&0<A(o,t)))break e;e[n]=t,e[a]=o,a=n}}function R(e){return void 0===(e=e[0])?null:e}function I(e){var t=e[0];if(void 0!==t){var a=e.pop();if(a!==t){e[0]=a;e:for(var n=0,o=e.length;n<o;){var i=2*(n+1)-1,r=e[i],d=i+1,s=e[d];if(void 0!==r&&0>A(r,a))void 0!==s&&0>A(s,r)?(e[n]=s,e[d]=a,n=d):(e[n]=r,e[i]=a,n=i);else{if(!(void 0!==s&&0>A(s,a)))break e;e[n]=s,e[d]=a,n=d}}}return t}return null}function A(e,t){var a=e.sortIndex-t.sortIndex;return 0!==a?a:e.id-t.id}var F=[],T=[],S=1,C=null,E=3,M=!1,P=!1,H=!1;function _(e){for(var t=R(T);null!==t;){if(null===t.callback)I(T);else{if(!(t.startTime<=e))break;I(T),t.sortIndex=t.expirationTime,k(F,t)}t=R(T)}}function z(e){if(H=!1,_(e),!P)if(null!==R(F))P=!0,a(j);else{var t=R(T);null!==t&&n(z,t.startTime-e)}}function j(e,a){P=!1,H&&(H=!1,o()),M=!0;var i=E;try{for(_(a),C=R(F);null!==C&&(!(C.expirationTime>a)||e&&!t.unstable_shouldYield());){var r=C.callback;if("function"===typeof r){C.callback=null,E=C.priorityLevel;var d=r(C.expirationTime<=a);a=t.unstable_now(),"function"===typeof d?C.callback=d:C===R(F)&&I(F),_(a)}else I(F);C=R(F)}if(null!==C)var s=!0;else{var c=R(T);null!==c&&n(z,c.startTime-a),s=!1}return s}finally{C=null,E=i,M=!1}}var D=i;t.unstable_IdlePriority=5,t.unstable_ImmediatePriority=1,t.unstable_LowPriority=4,t.unstable_NormalPriority=3,t.unstable_Profiling=null,t.unstable_UserBlockingPriority=2,t.unstable_cancelCallback=function(e){e.callback=null},t.unstable_continueExecution=function(){P||M||(P=!0,a(j))},t.unstable_getCurrentPriorityLevel=function(){return E},t.unstable_getFirstCallbackNode=function(){return R(F)},t.unstable_next=function(e){switch(E){case 1:case 2:case 3:var t=3;break;default:t=E}var a=E;E=t;try{return e()}finally{E=a}},t.unstable_pauseExecution=function(){},t.unstable_requestPaint=D,t.unstable_runWithPriority=function(e,t){switch(e){case 1:case 2:case 3:case 4:case 5:break;default:e=3}var a=E;E=e;try{return t()}finally{E=a}},t.unstable_scheduleCallback=function(e,i,r){var d=t.unstable_now();switch("object"===typeof r&&null!==r?r="number"===typeof(r=r.delay)&&0<r?d+r:d:r=d,e){case 1:var s=-1;break;case 2:s=250;break;case 5:s=1073741823;break;case 4:s=1e4;break;default:s=5e3}return e={id:S++,callback:i,priorityLevel:e,startTime:r,expirationTime:s=r+s,sortIndex:-1},r>d?(e.sortIndex=r,k(T,e),null===R(F)&&e===R(T)&&(H?o():H=!0,n(z,r-d))):(e.sortIndex=s,k(F,e),P||M||(P=!0,a(j))),e},t.unstable_wrapCallback=function(e){var t=E;return function(){var a=E;E=t;try{return e.apply(this,arguments)}finally{E=a}}}},296:function(e,t,a){"use strict";e.exports=a(813)}},t={};function a(n){var o=t[n];if(void 0!==o)return o.exports;var i=t[n]={id:n,loaded:!1,exports:{}};return e[n].call(i.exports,i,i.exports,a),i.loaded=!0,i.exports}a.n=function(e){var t=e&&e.__esModule?function(){return e.default}:function(){return e};return a.d(t,{a:t}),t},a.d=function(e,t){for(var n in t)a.o(t,n)&&!a.o(e,n)&&Object.defineProperty(e,n,{enumerable:!0,get:t[n]})},a.g=function(){if("object"===typeof globalThis)return globalThis;try{return this||new Function("return this")()}catch(e){if("object"===typeof window)return window}}(),a.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)},a.nmd=function(e){return e.paths=[],e.children||(e.children=[]),e},function(){"use strict";var e=a(791),t=a(164);function n(e,t){(null==t||t>e.length)&&(t=e.length);for(var a=0,n=new Array(t);a<t;a++)n[a]=e[a];return n}function o(e){return function(e){if(Array.isArray(e))return n(e)}(e)||function(e){if("undefined"!==typeof Symbol&&null!=e[Symbol.iterator]||null!=e["@@iterator"])return Array.from(e)}(e)||function(e,t){if(e){if("string"===typeof e)return n(e,t);var a=Object.prototype.toString.call(e).slice(8,-1);return"Object"===a&&e.constructor&&(a=e.constructor.name),"Map"===a||"Set"===a?Array.from(e):"Arguments"===a||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(a)?n(e,t):void 0}}(e)||function(){throw new TypeError("Invalid attempt to spread non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}function i(e,t){if(!(e instanceof t))throw new TypeError("Cannot call a class as a function")}function r(e,t){for(var a=0;a<t.length;a++){var n=t[a];n.enumerable=n.enumerable||!1,n.configurable=!0,"value"in n&&(n.writable=!0),Object.defineProperty(e,n.key,n)}}function d(e,t,a){return t&&r(e.prototype,t),a&&r(e,a),Object.defineProperty(e,"prototype",{writable:!1}),e}function s(e){if(void 0===e)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return e}function c(e,t){return c=Object.setPrototypeOf||function(e,t){return e.__proto__=t,e},c(e,t)}function l(e,t){if("function"!==typeof t&&null!==t)throw new TypeError("Super expression must either be null or a function");e.prototype=Object.create(t&&t.prototype,{constructor:{value:e,writable:!0,configurable:!0}}),Object.defineProperty(e,"prototype",{writable:!1}),t&&c(e,t)}function u(e){return u=Object.setPrototypeOf?Object.getPrototypeOf:function(e){return e.__proto__||Object.getPrototypeOf(e)},u(e)}function p(e){return p="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e},p(e)}function f(e,t){if(t&&("object"===p(t)||"function"===typeof t))return t;if(void 0!==t)throw new TypeError("Derived constructors may only return object or undefined");return s(e)}function h(e){var t=function(){if("undefined"===typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if("function"===typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Reflect.construct(Boolean,[],(function(){}))),!0}catch(e){return!1}}();return function(){var a,n=u(e);if(t){var o=u(this).constructor;a=Reflect.construct(n,arguments,o)}else a=n.apply(this,arguments);return f(this,a)}}var b=a(763),g=a.n(b),m=JSON.parse('[{"type":"article","key":"zhang2020augmented","title":"Augmented Reality in Robot Programming","author":"Zhang, Fengxin and Lai, Chow Yin and Simic, Milan and Ding, Songlin","journal":"Procedia Computer Science","volume":"176","pages":"1221--1230","year":"2020","publisher":"Elsevier","authors":["Fengxin Zhang","Chow Yin Lai","Milan Simic","Songlin Ding"],"doi":"10.1016/j.procs.2020.09.119","dblp":"conf/kes/ZhangLSD20","venue":"KES","abstract":null,"paperId":"272c82358f9b6e5dea24e5e513aa52547be39e2e","paperTitle":"Augmented reality in robot programming","paperUrl":"https://www.semanticscholar.org/paper/272c82358f9b6e5dea24e5e513aa52547be39e2e","images":["https://d3i71xaburhd42.cloudfront.net/700201aa1586b5a646effda1389367677aed397d/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/700201aa1586b5a646effda1389367677aed397d/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/700201aa1586b5a646effda1389367677aed397d/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/700201aa1586b5a646effda1389367677aed397d/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/700201aa1586b5a646effda1389367677aed397d/7-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/700201aa1586b5a646effda1389367677aed397d/8-Figure8-1.png"],"pdf":"https://discovery.ucl.ac.uk/id/eprint/10114000/7/Lai_1-s2.0-S1877050920320196-main.pdf"},{"type":"article","key":"wen2014hand","title":"Hand Gesture Guided Robot-Assisted Surgery Based on a Direct Augmented Reality Interface","author":"Wen, Rong and Tay, Wei-Liang and Nguyen, Binh P and Chng, Chin-Boon and Chui, Chee-Kong","journal":"Computer methods and programs in biomedicine","volume":"116","number":"2","pages":"68--80","year":"2014","publisher":"Elsevier","authors":["Rong Wen","Wei-Liang Tay","Binh P Nguyen","Chin-Boon Chng","Chee-Kong Chui"],"doi":"10.1016/j.cmpb.2013.12.018","dblp":"journals/cmpb/WenTNCC14","venue":"Comput. Methods Programs Biomed.","abstract":null,"paperId":"ef2e561ca2ea0beeed66c5d535d5de8f29c90cfe","paperTitle":"Hand gesture guided robot-assisted surgery based on a direct augmented reality interface","paperUrl":"https://www.semanticscholar.org/paper/ef2e561ca2ea0beeed66c5d535d5de8f29c90cfe","images":["https://d3i71xaburhd42.cloudfront.net/ef2e561ca2ea0beeed66c5d535d5de8f29c90cfe/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/ef2e561ca2ea0beeed66c5d535d5de8f29c90cfe/12-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/ef2e561ca2ea0beeed66c5d535d5de8f29c90cfe/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/ef2e561ca2ea0beeed66c5d535d5de8f29c90cfe/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/ef2e561ca2ea0beeed66c5d535d5de8f29c90cfe/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/ef2e561ca2ea0beeed66c5d535d5de8f29c90cfe/8-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/ef2e561ca2ea0beeed66c5d535d5de8f29c90cfe/8-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/ef2e561ca2ea0beeed66c5d535d5de8f29c90cfe/10-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/ef2e561ca2ea0beeed66c5d535d5de8f29c90cfe/10-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/ef2e561ca2ea0beeed66c5d535d5de8f29c90cfe/11-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/ef2e561ca2ea0beeed66c5d535d5de8f29c90cfe/11-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/ef2e561ca2ea0beeed66c5d535d5de8f29c90cfe/11-Figure14-1.png","https://d3i71xaburhd42.cloudfront.net/ef2e561ca2ea0beeed66c5d535d5de8f29c90cfe/12-Figure15-1.png"],"pdf":"http://www.researchgate.net/profile/Binh_Nguyen35/publication/259805819_Hand_gesture_guided_robot-assisted_surgery_based_on_a_direct_augmented_reality_interface/links/546329000cf2837efdb02a63.pdf"},{"type":"inproceedings","key":"wang2004augmented","title":"Augmented Reality Provision in Robotically Assisted Minimally Invasive Surgery","author":"Wang, DA and Bello, Fernando and Darzi, Ara","booktitle":"International Congress Series","volume":"1268","pages":"527--532","year":"2004","organization":"Elsevier","authors":["DA Wang","Fernando Bello","Ara Darzi"],"doi":"10.1016/J.ICS.2004.03.057","dblp":"conf/cars/WangBD04","venue":"CARS","abstract":null,"paperId":"28ee31c98fab55f765cd1b0b7a2572ee1f2a3256","paperTitle":"Augmented reality provision in robotically assisted minimally invasive surgery","paperUrl":"https://www.semanticscholar.org/paper/28ee31c98fab55f765cd1b0b7a2572ee1f2a3256","images":[],"pdf":null},{"type":"article","key":"samei2020partial","title":"A Partial Augmented Reality System with Live Ultrasound and Registered Preoperative MRI for Guiding Robot-Assisted Radical Prostatectomy","author":"Samei, Golnoosh and Tsang, Keith and Kesch, Claudia and Lobo, Julio and Hor, Soheil and Mohareri, Omid and Chang, Silvia and Goldenberg, S Larry and Black, Peter C and Salcudean, Septimiu","journal":"Medical image analysis","volume":"60","pages":"101588","year":"2020","publisher":"Elsevier","authors":["Golnoosh Samei","Keith Tsang","Claudia Kesch","Julio Lobo","Soheil Hor","Omid Mohareri","Silvia Chang","S Larry Goldenberg","Peter C Black","Septimiu Salcudean"],"doi":"10.1016/j.media.2019.101588","dblp":"journals/mia/SameiTKLHMCGBS20","venue":"Medical Image Anal.","abstract":null,"paperId":"d49cfdc3aed3d4bf44915696a79ea32b35b9422c","paperTitle":"A partial augmented reality system with live ultrasound and registered preoperative MRI for guiding robot-assisted radical prostatectomy","paperUrl":"https://www.semanticscholar.org/paper/d49cfdc3aed3d4bf44915696a79ea32b35b9422c","images":[],"pdf":"https://doi.org/10.1016/j.media.2019.101588"},{"type":"article","key":"chen2020multichannel","title":"A Multichannel Human-Swarm Robot Interaction System in Augmented Reality","author":"Chen, Mingxuan and Zhang, Ping and Wu, Zebo and Chen, Xiaodan","journal":"Virtual Reality & Intelligent Hardware","volume":"2","number":"6","pages":"518--533","year":"2020","publisher":"Elsevier","authors":["Mingxuan Chen","Ping Zhang","Zebo Wu","Xiaodan Chen"],"doi":"10.1016/j.vrih.2020.05.006","dblp":"journals/vrih/ChenZWC20","venue":"Virtual Real. Intell. Hardw.","abstract":null,"paperId":"b586cbd5b2f9e7520d81a6282aedafd021aaed71","paperTitle":"A multichannel human-swarm robot interaction system in augmented reality","paperUrl":"https://www.semanticscholar.org/paper/b586cbd5b2f9e7520d81a6282aedafd021aaed71","images":[],"pdf":null},{"type":"article","key":"pan2021augmented","title":"Augmented Reality-Based Robot Teleoperation System Using RGB-D Imaging and Attitude Teaching Device","author":"Pan, Yong and Chen, Chengjun and Li, Dongnian and Zhao, Zhengxu and Hong, Jun","journal":"Robotics and Computer-Integrated Manufacturing","volume":"71","pages":"102167","year":"2021","publisher":"Elsevier","authors":["Yong Pan","Chengjun Chen","Dongnian Li","Zhengxu Zhao","Jun Hong"],"doi":"10.1016/J.RCIM.2021.102167","dblp":"journals/rcim/PanCLZH21","venue":"Robotics Comput. Integr. Manuf.","abstract":null,"paperId":"543e6ed58bb55a018a99486794d36964f1629f9b","paperTitle":"Augmented reality-based robot teleoperation system using RGB-D imaging and attitude teaching device","paperUrl":"https://www.semanticscholar.org/paper/543e6ed58bb55a018a99486794d36964f1629f9b","images":[],"pdf":"https://doi.org/10.1016/j.rcim.2021.102167"},{"type":"article","key":"su2009augmented","title":"Augmented Reality during Robot-Assisted Laparoscopic Partial Nephrectomy: Toward Real-Time 3D-CT to Stereoscopic Video Registration","author":"Su, Li-Ming and Vagvolgyi, Balazs P and Agarwal, Rahul and Reiley, Carol E and Taylor, Russell H and Hager, Gregory D","journal":"Urology","volume":"73","number":"4","pages":"896--900","year":"2009","publisher":"Elsevier","authors":["Li-Ming Su","Balazs P Vagvolgyi","Rahul Agarwal","Carol E Reiley","Russell H Taylor","Gregory D Hager"],"doi":"10.1016/j.urology.2008.11.040","venue":"Urology","abstract":null,"paperId":"cc65a78d143d8a5bab8961b6d66db47964127108","paperTitle":"Augmented reality during robot-assisted laparoscopic partial nephrectomy: toward real-time 3D-CT to stereoscopic video registration.","paperUrl":"https://www.semanticscholar.org/paper/cc65a78d143d8a5bab8961b6d66db47964127108","images":[],"pdf":"https://doi.org/10.1016/j.urology.2008.11.040"},{"type":"article","key":"marin2002augmented","title":"Augmented Reality to Teleoperate a Robot through the Web","author":"Mar\'in, Ra\'ul and Sanz, Pedro J","journal":"IFAC Proceedings Volumes","volume":"35","number":"1","pages":"161--165","year":"2002","publisher":"Elsevier","authors":["Ra\'ul Mar\'in","Pedro J Sanz"],"doi":"10.3182/20020721-6-ES-1901.00933","venue":"","abstract":"Abstract The system consists of a multirobot architecture that gives access to both, an educational and industrial robot, through the Internet, by using advanced multimedia and distributed programming tools like Java, Java3D and CORBA. The main effort is focused on the user interface that allows specifying a task into a 3D model and sending commands to the real robot once the operator approves them. This kind of interaction has the ability to consume very little bandwidth in order to perform a remote manipulation through the Internet. The user interface offers Augmented and Virtual Reality possibilities to the users.","paperId":"e749587cf6223a5be17fe36e63842125f01401bc","paperTitle":"Augmented reality to teleoperate a robot through the web","paperUrl":"https://www.semanticscholar.org/paper/e749587cf6223a5be17fe36e63842125f01401bc","images":["https://d3i71xaburhd42.cloudfront.net/e749587cf6223a5be17fe36e63842125f01401bc/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/e749587cf6223a5be17fe36e63842125f01401bc/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/e749587cf6223a5be17fe36e63842125f01401bc/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/e749587cf6223a5be17fe36e63842125f01401bc/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/e749587cf6223a5be17fe36e63842125f01401bc/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/e749587cf6223a5be17fe36e63842125f01401bc/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/e749587cf6223a5be17fe36e63842125f01401bc/4-Figure7-1.png"],"pdf":"http://folk.ntnu.no/skoge/prost/proceedings/ifac2002/data/content/02640/2640.pdf"},{"type":"incollection","key":"kolagunda2018mixed","title":"A Mixed Reality Guidance System for Robot Assisted Laparoscopic Radical Prostatectomy","author":"Kolagunda, Abhishek and Sorensen, Scott and Mehralivand, Sherif and Saponaro, Philip and Treible, Wayne and Turkbey, Baris and Pinto, Peter and Choyke, Peter and Kambhamettu, Chandra","booktitle":"OR 2.0 Context-Aware Operating Theaters, Computer Assisted Robotic Endoscopy, Clinical Image-Based Procedures, and Skin Image Analysis","pages":"164--174","year":"2018","publisher":"Springer","authors":["Abhishek Kolagunda","Scott Sorensen","Sherif Mehralivand","Philip Saponaro","Wayne Treible","Baris Turkbey","Peter Pinto","Peter Choyke","Chandra Kambhamettu"],"doi":"10.1007/978-3-030-01201-4_18","dblp":"conf/miccai/KolagundaSMSTTP18","venue":"OR 2.0/CARE/CLIP/ISIC@MICCAI","abstract":"Robotic surgery with preoperative imaging data for planning have become increasingly common for surgical treatment of patients. For surgeons using robotic surgical platforms, maintaining spatial awareness of the anatomical structures in the surgical area is key for good outcomes. We propose a Mixed Reality system which allows surgeons to visualize and interact with aligned anatomical models extracted from preoperative imagery as well as the in vivo imagery from the stereo laparoscope. To develop this system, we have employed techniques to 3D reconstruct stereo laparoscope images, model 3D shape of the anatomical structures from preoperative MRI stack and align the two 3D surfaces. The application we have developed allows surgeons to visualize occluded and obscured organ boundaries as well as other important anatomy that is not visible through the laparoscope alone, facilitating better spatial awareness during surgery. The system was deployed in 9 robot assisted laparoscopic prostatectomy procedures as part of a feasibility study.","paperId":"5d124ba4b930373c8ef99e38d412944b26782b7f","paperTitle":"A Mixed Reality Guidance System for Robot Assisted Laparoscopic Radical Prostatectomy","paperUrl":"https://www.semanticscholar.org/paper/5d124ba4b930373c8ef99e38d412944b26782b7f","images":[],"pdf":null},{"type":"inproceedings","key":"sgouros2001generation","title":"Generation and Implementation of Mixed-Reality, Narrative Performances Involving Robotic Actors","author":"Sgouros, Nikitas M and Kousidou, Sophia","booktitle":"International Conference on Virtual Storytelling","pages":"69--78","year":"2001","organization":"Springer","authors":["Nikitas M Sgouros","Sophia Kousidou"],"doi":"10.1007/3-540-45420-9_9","dblp":"conf/storytelling/SgourosK01","venue":"International Conference on Virtual Storytelling","abstract":"Recent advances in robotics and multimedia technologies have created new possibilities for staging narrative performances involving robotic actors. However, the implementation of these types of events is complicated by the lack of appropriate direction and execution environments that can deal with the complexity of these productions. This paper seeks to address this problem by describing CHOROS, a Java-based environment for authoring, direction and control of narrative performances. In particular, CHOROS allows the story author to annotate the performance script with stage directions. Furthermore, the environment offers to the performance director an augmented reality interface for planning the behavior of the actors. Finally, the system uses vision-based tracking methods and behavior-based control for adjusting the behavior of the robotic actors according to the director instructions during the performance.","paperId":"7e54b403389187d38c4ee8a3460128554aaa24ec","paperTitle":"Generation and Implementation of Mixed-Reality, Narrative Performances Involving Robotic Actors","paperUrl":"https://www.semanticscholar.org/paper/7e54b403389187d38c4ee8a3460128554aaa24ec","images":[],"pdf":"http://www.gbv.de/dms/bowker/toc/9783540426110.pdf"},{"type":"article","key":"huang2021augmented","title":"Augmented Reality-Based Autostereoscopic Surgical Visualization System for Telesurgery","author":"Huang, Tianqi and Li, Ruiyang and Li, Yangxi and Zhang, Xinran and Liao, Hongen","journal":"International Journal of Computer Assisted Radiology and Surgery","volume":"16","number":"11","pages":"1985--1997","year":"2021","publisher":"Springer","authors":["Tianqi Huang","Ruiyang Li","Yangxi Li","Xinran Zhang","Hongen Liao"],"doi":"10.1007/s11548-021-02463-5","dblp":"journals/cars/HuangLLZL21","venue":"International Journal of Computer Assisted Radiology and Surgery","abstract":"The visualization of remote surgical scenes is the key to realizing the remote operation of surgical robots. However, current non-endoscopic surgical robot systems lack an effective visualization tool to offer sufficient surgical scene information and depth perception. We propose a novel autostereoscopic surgical visualization system integrating 3D intraoperative scene reconstruction, autostereoscopic 3D display, and augmented reality-based image fusion. The preoperative organ structure and the intraoperative surface point cloud are obtained from medical imaging and the RGB-D camera, respectively, and aligned by an automatic marker-free intraoperative registration algorithm. After registration, preoperative meshes with precalculated illumination and intraoperative textured point cloud are blended in real time. Finally, the fused image is shown on a 3D autostereoscopic display device to achieve depth perception. A prototype of the autostereoscopic surgical visualization system was built. The system had a horizontal image resolution of 1.31 mm, a vertical image resolution of 0.82 mm, an average rendering rate of 33.1 FPS, an average registration rate of 20.5 FPS, and average registration errors of approximately 3 mm. A telesurgical robot prototype based on 3D autostereoscopic display was built. The quantitative evaluation experiments showed that our system achieved similar operational accuracy (1.79\u2009\xb1\u20090.87 mm) as the conventional system (1.95\u2009\xb1\u20090.71 mm), while having advantages in terms of completion time (with 34.11% reduction) and path length (with 35.87% reduction). Post-experimental questionnaires indicated that the system was user-friendly for novices and experts. We propose a 3D surgical visualization system with augmented instruction and depth perception for telesurgery. The qualitative and quantitative evaluation results illustrate the accuracy and efficiency of the proposed system. Therefore, it shows great prospects in robotic surgery and telesurgery.","paperId":"aa9a91591c657c050f6d15ee6d058e8037153ad3","paperTitle":"Augmented reality-based autostereoscopic surgical visualization system for telesurgery","paperUrl":"https://www.semanticscholar.org/paper/aa9a91591c657c050f6d15ee6d058e8037153ad3","images":["https://d3i71xaburhd42.cloudfront.net/aa9a91591c657c050f6d15ee6d058e8037153ad3/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/aa9a91591c657c050f6d15ee6d058e8037153ad3/5-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/aa9a91591c657c050f6d15ee6d058e8037153ad3/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/aa9a91591c657c050f6d15ee6d058e8037153ad3/7-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/aa9a91591c657c050f6d15ee6d058e8037153ad3/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/aa9a91591c657c050f6d15ee6d058e8037153ad3/8-Table3-1.png","https://d3i71xaburhd42.cloudfront.net/aa9a91591c657c050f6d15ee6d058e8037153ad3/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/aa9a91591c657c050f6d15ee6d058e8037153ad3/10-Table4-1.png","https://d3i71xaburhd42.cloudfront.net/aa9a91591c657c050f6d15ee6d058e8037153ad3/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/aa9a91591c657c050f6d15ee6d058e8037153ad3/10-Table5-1.png","https://d3i71xaburhd42.cloudfront.net/aa9a91591c657c050f6d15ee6d058e8037153ad3/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/aa9a91591c657c050f6d15ee6d058e8037153ad3/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/aa9a91591c657c050f6d15ee6d058e8037153ad3/7-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/aa9a91591c657c050f6d15ee6d058e8037153ad3/7-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/aa9a91591c657c050f6d15ee6d058e8037153ad3/8-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/aa9a91591c657c050f6d15ee6d058e8037153ad3/9-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/aa9a91591c657c050f6d15ee6d058e8037153ad3/10-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/aa9a91591c657c050f6d15ee6d058e8037153ad3/10-Figure13-1.png"],"pdf":"https://www.ncbi.nlm.nih.gov/pubmed/34363583"},{"type":"article","key":"tanzi2021real","title":"Real-Time Deep Learning Semantic Segmentation during Intra-Operative Surgery for 3D Augmented Reality Assistance","author":"Tanzi, Leonardo and Piazzolla, Pietro and Porpiglia, Francesco and Vezzetti, Enrico","journal":"International Journal of Computer Assisted Radiology and Surgery","volume":"16","number":"9","pages":"1435--1445","year":"2021","publisher":"Springer","authors":["Leonardo Tanzi","Pietro Piazzolla","Francesco Porpiglia","Enrico Vezzetti"],"doi":"10.1007/s11548-021-02432-y","dblp":"journals/cars/TanziPPV21","venue":"International Journal of Computer Assisted Radiology and Surgery","abstract":"The current study aimed to propose a Deep Learning (DL) and Augmented Reality (AR) based solution for a in-vivo robot-assisted radical prostatectomy (RARP), to improve the precision of a published work from our group. We implemented a two-steps automatic system to align a 3D virtual ad-hoc model of a patient\u2019s organ with its 2D endoscopic image, to assist surgeons during the procedure. This approach was carried out using a Convolutional Neural Network (CNN) based structure for semantic segmentation and a subsequent elaboration of the obtained output, which produced the needed parameters for attaching the 3D model. We used a dataset obtained from 5 endoscopic videos (A, B, C, D, E), selected and tagged by our team\u2019s specialists. We then evaluated the most performing couple of segmentation architecture and neural network and tested the overlay performances. U-Net stood out as the most effecting architectures for segmentation. ResNet and MobileNet obtained similar Intersection over Unit (IoU) results but MobileNet was able to elaborate almost twice operations per seconds. This segmentation technique outperformed the results from the former work, obtaining an average IoU for the catheter of 0.894 (\u03c3 = 0.076) compared to 0.339 (\u03c3 = 0.195). This modifications lead to an improvement also in the 3D overlay performances, in particular in the Euclidean Distance between the predicted and actual model\u2019s anchor point, from 12.569 (\u03c3= 4.456) to 4.160 (\u03c3 = 1.448) and in the Geodesic Distance between the predicted and actual model\u2019s rotations, from 0.266 (\u03c3 = 0.131) to 0.169 (\u03c3 = 0.073). This work is a further step through the adoption of DL and AR in the surgery domain. In future works, we will overcome the limits of this approach and finally improve every step of the surgical procedure.","paperId":"11deb1d973d5ae885230a7be10c4246200681e21","paperTitle":"Real-time deep learning semantic segmentation during intra-operative surgery for 3D augmented reality assistance","paperUrl":"https://www.semanticscholar.org/paper/11deb1d973d5ae885230a7be10c4246200681e21","images":["https://d3i71xaburhd42.cloudfront.net/11deb1d973d5ae885230a7be10c4246200681e21/4-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/11deb1d973d5ae885230a7be10c4246200681e21/6-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/11deb1d973d5ae885230a7be10c4246200681e21/5-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/11deb1d973d5ae885230a7be10c4246200681e21/7-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/11deb1d973d5ae885230a7be10c4246200681e21/6-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/11deb1d973d5ae885230a7be10c4246200681e21/7-Table3-1.png","https://d3i71xaburhd42.cloudfront.net/11deb1d973d5ae885230a7be10c4246200681e21/7-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/11deb1d973d5ae885230a7be10c4246200681e21/8-Table4-1.png","https://d3i71xaburhd42.cloudfront.net/11deb1d973d5ae885230a7be10c4246200681e21/8-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/11deb1d973d5ae885230a7be10c4246200681e21/9-Figure6-1.png"],"pdf":"https://link.springer.com/content/pdf/10.1007/s11548-021-02432-y.pdf"},{"type":"article","key":"kalia2021preclinical","title":"Preclinical Evaluation of a Markerless, Real-Time, Augmented Reality Guidance System for Robot-Assisted Radical Prostatectomy","author":"Kalia, Megha and Avinash, Apeksha and Navab, Nassir and Salcudean, Septimiu","journal":"International Journal of Computer Assisted Radiology and Surgery","pages":"1--8","year":"2021","publisher":"Springer","authors":["Megha Kalia","Apeksha Avinash","Nassir Navab","Septimiu Salcudean"],"doi":"10.1007/s11548-021-02419-9","dblp":"journals/cars/KaliaANS21","venue":"International Journal of Computer Assisted Radiology and Surgery","abstract":"Intra-operative augmented reality (AR) during surgery can mitigate incomplete cancer removal by overlaying the anatomical boundaries extracted from medical imaging data onto the camera image. In this paper, we present the first such completely markerless AR guidance system for robot-assisted laparoscopic radical prostatectomy (RALRP) that transforms medical data from transrectal ultrasound (TRUS) to endoscope camera image. Moreover, we reduce the total number of transformations by combining the hand\u2013eye and camera calibrations in a single step. Our proposed solution requires two transformations: TRUS to robot, DVTTRUS\\\\documentclass[12pt]{minimal} \\\\usepackage{amsmath} \\\\usepackage{wasysym} \\\\usepackage{amsfonts} \\\\usepackage{amssymb} \\\\usepackage{amsbsy} \\\\usepackage{mathrsfs} \\\\usepackage{upgreek} \\\\setlength{\\\\oddsidemargin}{-69pt} \\\\begin{document}$$^\\\\mathrm{DV}T_\\\\mathrm{TRUS}$$\\\\end{document}, and camera projection matrix, M\\\\documentclass[12pt]{minimal} \\\\usepackage{amsmath} \\\\usepackage{wasysym} \\\\usepackage{amsfonts} \\\\usepackage{amssymb} \\\\usepackage{amsbsy} \\\\usepackage{mathrsfs} \\\\usepackage{upgreek} \\\\setlength{\\\\oddsidemargin}{-69pt} \\\\begin{document}$$\\\\mathbf{M} $$\\\\end{document} (i.e., the transformation from endoscope to camera image frame). DVTTRUS\\\\documentclass[12pt]{minimal} \\\\usepackage{amsmath} \\\\usepackage{wasysym} \\\\usepackage{amsfonts} \\\\usepackage{amssymb} \\\\usepackage{amsbsy} \\\\usepackage{mathrsfs} \\\\usepackage{upgreek} \\\\setlength{\\\\oddsidemargin}{-69pt} \\\\begin{document}$$^\\\\mathrm{DV}T_\\\\mathrm{TRUS}$$\\\\end{document} is estimated by the method proposed in Mohareri et al. (in J Urol 193(1):302\u2013312, 2015). M\\\\documentclass[12pt]{minimal} \\\\usepackage{amsmath} \\\\usepackage{wasysym} \\\\usepackage{amsfonts} \\\\usepackage{amssymb} \\\\usepackage{amsbsy} \\\\usepackage{mathrsfs} \\\\usepackage{upgreek} \\\\setlength{\\\\oddsidemargin}{-69pt} \\\\begin{document}$$\\\\mathbf{M} $$\\\\end{document} is estimated by selecting corresponding 3D-2D data points in the endoscope and the image coordinate frame, respectively, by using a CAD model of the surgical instrument and a preoperative camera intrinsic matrix with an assumption of a projective camera. The parameters are estimated using Levenberg\u2013Marquardt algorithm. Overall mean re-projection errors (MRE) are reported using simulated and real data using a water bath. We show that M\\\\documentclass[12pt]{minimal} \\\\usepackage{amsmath} \\\\usepackage{wasysym} \\\\usepackage{amsfonts} \\\\usepackage{amssymb} \\\\usepackage{amsbsy} \\\\usepackage{mathrsfs} \\\\usepackage{upgreek} \\\\setlength{\\\\oddsidemargin}{-69pt} \\\\begin{document}$$\\\\mathbf{M} $$\\\\end{document} can be re-estimated if the focus is changed during surgery. Using simulated data, we received an overall MRE in the range of 11.69\u201313.32 pixels for monoscopic and stereo left and right cameras. For the water bath experiment, the overall MRE is in the range of 26.04\u201330.59 pixels for monoscopic and stereo cameras. The overall system error from TRUS to camera world frame is 4.05 mm. Details of the procedure are given in supplementary material. We demonstrate a markerless AR guidance system for RALRP that does not need calibration markers and thus has the capability to re-estimate the camera projection matrix if it changes during surgery, e.g., due to a focus change.","paperId":"3ad25dc76fc199101ff4b39c8276f4d502042277","paperTitle":"Preclinical evaluation of a markerless, real-time, augmented reality guidance system for robot-assisted radical prostatectomy","paperUrl":"https://www.semanticscholar.org/paper/3ad25dc76fc199101ff4b39c8276f4d502042277","images":["https://d3i71xaburhd42.cloudfront.net/df700de11c6e08f7d12ef801ef02890dcebd9cb3/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/df700de11c6e08f7d12ef801ef02890dcebd9cb3/5-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/df700de11c6e08f7d12ef801ef02890dcebd9cb3/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/df700de11c6e08f7d12ef801ef02890dcebd9cb3/6-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/df700de11c6e08f7d12ef801ef02890dcebd9cb3/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/df700de11c6e08f7d12ef801ef02890dcebd9cb3/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/df700de11c6e08f7d12ef801ef02890dcebd9cb3/6-Figure5-1.png"],"pdf":"https://www.ncbi.nlm.nih.gov/pubmed/34076803"},{"type":"article","key":"kalia2020evaluation","title":"Evaluation of a Marker-Less, Intra-Operative, Augmented Reality Guidance System for Robot-Assisted Laparoscopic Radical Prostatectomy","author":"Kalia, Megha and Mathur, Prateek and Tsang, Keith and Black, Peter and Navab, Nassir and Salcudean, Septimiu","journal":"International Journal of Computer Assisted Radiology and Surgery","volume":"15","pages":"1225--1233","year":"2020","publisher":"Springer","authors":["Megha Kalia","Prateek Mathur","Keith Tsang","Peter Black","Nassir Navab","Septimiu Salcudean"],"doi":"10.1007/s11548-020-02181-4","dblp":"journals/cars/KaliaMTBNS20","venue":"International Journal of Computer Assisted Radiology and Surgery","abstract":"Purpose Robot-assisted laparoscopic radical prostatectomy (RALRP) using the da Vinci surgical robot is a common treatment for organ-confined prostate cancer. Augmented reality (AR) can help during RALRP by showing the surgeon the location of anatomical structures and tumors from preoperative imaging. Previously, we proposed hand-eye and camera intrinsic matrix estimation procedures that can be carried out with conventional instruments within the patient during surgery, take <\xa03\xa0min to perform, and fit seamlessly in the existing surgical workflow. In this paper, we describe and evaluate a complete AR guidance system for RALRP and quantify its accuracy. Methods Our AR system requires three transformations: the transrectal ultrasound (TRUS) to da Vinci transformation, the camera intrinsic matrix, and the hand-eye transformation. For evaluation, a 3D-printed cross-wire was visualized in TRUS and stereo endoscope in a water bath. Manually triangulated cross-wire points from stereo images were used as ground truth to evaluate overall TRE between these points and points transformed from TRUS to camera. Results After transforming the ground-truth points from the TRUS to the camera coordinate frame, the mean target registration error (TRE) (SD) was $$4.56\\\\pm 1.57$$ 4.56 \xb1 1.57 \xa0mm. The mean TREs (SD) in the x -, y -, and z -directions are $$1.93\\\\pm 1.26$$ 1.93 \xb1 1.26 mm, $$2.04\\\\pm 1.37$$ 2.04 \xb1 1.37 mm, and $$2.94\\\\pm 1.84$$ 2.94 \xb1 1.84 mm, respectively. Conclusions We describe and evaluate a complete AR guidance system for RALRP which can augment preoperative data to endoscope camera image, after a deformable magnetic resonance image to TRUS registration step. The streamlined procedures with current surgical workflow and low TRE demonstrate the compatibility and readiness of the system for clinical translation. A detailed sensitivity study remains part of future work.","paperId":"d4c1a0f0d681d687a9e52634f935c01e25c51028","paperTitle":"Evaluation of a marker-less, intra-operative, augmented reality guidance system for robot-assisted laparoscopic radical prostatectomy","paperUrl":"https://www.semanticscholar.org/paper/d4c1a0f0d681d687a9e52634f935c01e25c51028","images":["https://d3i71xaburhd42.cloudfront.net/d4c1a0f0d681d687a9e52634f935c01e25c51028/4-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/d4c1a0f0d681d687a9e52634f935c01e25c51028/7-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/d4c1a0f0d681d687a9e52634f935c01e25c51028/5-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/d4c1a0f0d681d687a9e52634f935c01e25c51028/6-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/d4c1a0f0d681d687a9e52634f935c01e25c51028/7-Figure4-1.png"],"pdf":"https://www.ncbi.nlm.nih.gov/pubmed/32500450"},{"type":"article","key":"shen2020transrectal","title":"Transrectal Ultrasound Image-Based Real-Time Augmented Reality Guidance in Robot-Assisted Laparoscopic Rectal Surgery: A Proof-of-Concept Study","author":"Shen, Jun and Zemiti, Nabil and Taoum, Christophe and Aiche, Guillaume and Dillenseger, Jean-Louis and Rouanet, Philippe and Poignet, Philippe","journal":"International journal of computer assisted radiology and surgery","volume":"15","number":"3","pages":"531--543","year":"2020","publisher":"Springer","authors":["Jun Shen","Nabil Zemiti","Christophe Taoum","Guillaume Aiche","Jean-Louis Dillenseger","Philippe Rouanet","Philippe Poignet"],"doi":"10.1007/s11548-019-02100-2","dblp":"journals/cars/ShenZTADRP20","venue":"International Journal of Computer Assisted Radiology and Surgery","abstract":"Purpose Surgical treatments for low-rectal cancer require careful considerations due to the low location of cancer in rectums. Successful surgical outcomes highly depend on surgeons\u2019 ability to determine clear distal margins of rectal tumors. This is a challenge for surgeons in robot-assisted laparoscopic surgery, since tumors are often concealed in rectums and robotic surgical instruments do not provide tactile feedback for tissue diagnosis in real time. This paper presents the development and evaluation of an intraoperative ultrasound-based augmented reality framework for surgical guidance in robot-assisted rectal surgery. Methods Framework implementation consists in calibrating the transrectal ultrasound (TRUS) and the endoscopic camera (hand-eye calibration), generating a virtual model and registering it to the endoscopic image via optical tracking, and displaying the augmented view on a head-mounted display. An experimental validation setup is designed to evaluate the framework. Results The evaluation process yields a mean error of 0.9\xa0mm for the TRUS calibration, a maximum error of 0.51\xa0mm for the hand-eye calibration of endoscopic cameras, and a maximum RMS error of 0.8\xa0mm for the whole framework. In the experiment with a rectum phantom, our framework guides the surgeon to accurately localize the simulated tumor and the distal resection margin. Conclusions This framework is developed with our clinical partner, based on actual clinical conditions. The experimental protocol and the high level of accuracy show the feasibility of seamlessly integrating this framework within the surgical workflow.","paperId":"d9508d7e05570cf4cc23b81a76f4eedbdae9e1ac","paperTitle":"Transrectal ultrasound image-based real-time augmented reality guidance in robot-assisted laparoscopic rectal surgery: a proof-of-concept study","paperUrl":"https://www.semanticscholar.org/paper/d9508d7e05570cf4cc23b81a76f4eedbdae9e1ac","images":["https://d3i71xaburhd42.cloudfront.net/d9508d7e05570cf4cc23b81a76f4eedbdae9e1ac/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/d9508d7e05570cf4cc23b81a76f4eedbdae9e1ac/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/d9508d7e05570cf4cc23b81a76f4eedbdae9e1ac/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/d9508d7e05570cf4cc23b81a76f4eedbdae9e1ac/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/d9508d7e05570cf4cc23b81a76f4eedbdae9e1ac/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/d9508d7e05570cf4cc23b81a76f4eedbdae9e1ac/7-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/d9508d7e05570cf4cc23b81a76f4eedbdae9e1ac/9-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/d9508d7e05570cf4cc23b81a76f4eedbdae9e1ac/10-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/d9508d7e05570cf4cc23b81a76f4eedbdae9e1ac/11-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/d9508d7e05570cf4cc23b81a76f4eedbdae9e1ac/12-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/d9508d7e05570cf4cc23b81a76f4eedbdae9e1ac/12-Figure13-1.png"],"pdf":"https://hal-lirmm.ccsd.cnrs.fr/lirmm-02381958/file/Shen_IJCARS2020.pdf"},{"type":"inproceedings","key":"grijalva2019landmark","title":"Landmark-Based Virtual Path Estimation for Assisted UAV FPV Tele-Operation with Augmented Reality","author":"Grijalva, Santiago and Aguilar, Wilbert G","booktitle":"International Conference on Intelligent Robotics and Applications","pages":"688--700","year":"2019","organization":"Springer","authors":["Santiago Grijalva","Wilbert G Aguilar"],"doi":"10.1007/978-3-030-27529-7_58","dblp":"conf/icira/GrijalvaA19","venue":"ICIRA","abstract":"In this paper we proposed an Assisted UAV Tele-Operation System, specifically for FPV navigation based on Artificial Landmarks in obstacle free environments. The system estimates the optimal path through landmarks and traces an artificial route to be followed. Path recognition uses color space and morphological transformation such as eroding and dilating to reduce noise due to different lighting environments. Once path is recognized ORB detector is used for getting a set of the most representative pixels coordinates, this is done for each ROI (Region of Interest) in the camera image. Later, the median of each pixel coordinate in the specific ROI is considered for interpolation needed to trace the route. Parrot\u2019s drone Bebop 2 was used for the purpose of this study as it has a fisheye lens camera that allows us to face downwards to detect the landmarks.","paperId":"d454e03ef45ec23418c1272a7d164ac3b170c124","paperTitle":"Landmark-Based Virtual Path Estimation for Assisted UAV FPV Tele-Operation with Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/d454e03ef45ec23418c1272a7d164ac3b170c124","images":[],"pdf":null},{"type":"inproceedings","key":"kot2017application","title":"Application of Augmented Reality in Mobile Robot Teleoperation","author":"Kot, Tom\'a\u0161 and Nov\'ak, Petr and Babjak, J\'an","booktitle":"International Workshop on Modelling and Simulation for Autonomous Systems","pages":"223--236","year":"2017","organization":"Springer","authors":["Tom\'a\u0161 Kot","Petr Nov\'ak","J\'an Babjak"],"doi":"10.1007/978-3-319-76072-8_16","dblp":"conf/mesas/KotNB17","venue":"MESAS","abstract":"The paper deals with the utilisation of augmented reality on the operator control panel of a teleoperated mobile robot. The work is a continuation of the previous research, which resulted in the creation and successful practical implementation of a virtual operator station based on the virtual reality head-mounted display (HMD) Oculus Rift. The new approach suggests using the new Microsoft Hololens augmented reality headset to add the virtual control elements directly to the real world, which has some very important benefits \u2013 especially the fact that the operator is not visually isolated from his surroundings. The device is introduced in the beginning of the article and then follows the description of all tasks required to create the augmented reality operator station. Mentioned are also other possible ways of using augmented reality to assist the operator of a mobile robot.","paperId":"95548333f2e56cb7aed0ca4a998de9fcbdf38674","paperTitle":"Application of Augmented Reality in Mobile Robot Teleoperation","paperUrl":"https://www.semanticscholar.org/paper/95548333f2e56cb7aed0ca4a998de9fcbdf38674","images":[],"pdf":null},{"type":"incollection","key":"lera2014augmented","title":"Augmented Reality in Robotic Assistance for the Elderly","author":"Lera, Francisco J and Rodr\'iguez, V\'ictor and Rodr\'iguez, Carlos and Matell\'an, Vicente","booktitle":"International technology robotics applications","pages":"3--11","year":"2014","publisher":"Springer","authors":["Francisco J Lera","V\'ictor Rodr\'iguez","Carlos Rodr\'iguez","Vicente Matell\'an"],"doi":"10.1007/978-3-319-02332-8_1","venue":"","abstract":"The basis of this research was to create a platform for social interaction based on augmented reality, ready to be deployed in the homes of elderly people. Two main components are presented: the first one is an affordable robot platform built from TurtleBot robot. The second one is the underlying software system built on top of ROS, in charge of the interaction interface and the user tasks, called MYRA. The purpose of this study is to test the platform and the augmented reality in real environments and how it can be used effectively and without complications by elderly people. With this goal in mind we prepared our platform to be able to do two different tasks: a generic assistance system, and a drug dose control system. Both were tested in a real environment.","paperId":"c1410171a4589fc09b76313d9d2d91755833e734","paperTitle":"Augmented Reality in Robotic Assistance for the Elderly","paperUrl":"https://www.semanticscholar.org/paper/c1410171a4589fc09b76313d9d2d91755833e734","images":[],"pdf":null},{"type":"incollection","key":"amir2013uncertainty","title":"Uncertainty-Encoded Augmented Reality for Robot-Assisted Partial Nephrectomy: A Phantom Study","author":"Amir-Khalili, Alborz and Nosrati, Masoud S and Peyrat, Jean-Marc and Hamarneh, Ghassan and Abugharbieh, Rafeef","booktitle":"Augmented Reality Environments for Medical Imaging and Computer-Assisted Interventions","pages":"182--191","year":"2013","publisher":"Springer","authors":["Alborz Amir-Khalili","Masoud S Nosrati","Jean-Marc Peyrat","Ghassan Hamarneh","Rafeef Abugharbieh"],"doi":"10.1007/978-3-642-40843-4_20","dblp":"conf/miccai/Amir-KhaliliNPHA13","venue":"AE-CAI","abstract":"In most robot-assisted surgical interventions, multimodal fusion of pre- and intra-operative data is highly valuable, affording the surgeon a more comprehensive understanding of the surgical scene observed through the stereo endoscopic camera. More specifically, in the case of partial nephrectomy, fusing pre-operative segmentations of kidney and tumor with the stereo endoscopic view can guide tumor localization and the identification of resection margins. However, the surgeons are often unable to reliably assess the levels of trust they can bestow on what is overlaid on the screen. In this paper, we present the proof-of-concept of an uncertainty-encoded augmented reality framework and novel visualizations of the uncertainties derived from the pre-operative CT segmentation onto the surgeon\u2019s stereo endoscopic view. To verify its clinical potential, the proposed method is applied to an ex vivo lamb kidney. The results are contrasted to different visualization solutions based on crisp segmentation demonstrating that our method provides valuable additional information that can help the surgeon during the resection planning.","paperId":"eccb7fbeed566bb39abbf797f5fa4e0d60041903","paperTitle":"Uncertainty-Encoded Augmented Reality for Robot-Assisted Partial Nephrectomy: A Phantom Study","paperUrl":"https://www.semanticscholar.org/paper/eccb7fbeed566bb39abbf797f5fa4e0d60041903","images":["https://d3i71xaburhd42.cloudfront.net/eccb7fbeed566bb39abbf797f5fa4e0d60041903/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/eccb7fbeed566bb39abbf797f5fa4e0d60041903/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/eccb7fbeed566bb39abbf797f5fa4e0d60041903/6-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/eccb7fbeed566bb39abbf797f5fa4e0d60041903/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/eccb7fbeed566bb39abbf797f5fa4e0d60041903/8-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/eccb7fbeed566bb39abbf797f5fa4e0d60041903/9-Figure6-1.png"],"pdf":"http://www.cs.sfu.ca/~hamarneh/ecopy/miccai_miar2013.pdf"},{"type":"article","key":"volonte2011augmented","title":"Augmented Reality and Image Overlay Navigation with OsiriX in Laparoscopic and Robotic Surgery: Not Only a Matter of Fashion","author":"Volont\'e, Francesco and Pugin, Fran\xe7ois and Bucher, Pascal and Sugimoto, Maki and Ratib, Osman and Morel, Philippe","journal":"Journal of Hepato-biliary-pancreatic Sciences","volume":"18","number":"4","pages":"506--509","year":"2011","publisher":"Wiley Online Library","authors":["Francesco Volont\'e","Fran\xe7ois Pugin","Pascal Bucher","Maki Sugimoto","Osman Ratib","Philippe Morel"],"doi":"10.1007/s00534-011-0385-6","venue":"Journal of hepato-biliary-pancreatic sciences","abstract":"BackgroundNew technologies can considerably improve preoperative planning, enhance the surgeon\u2019s skill and simplify the approach to complex procedures. Augmented reality techniques, robot assisted operations and computer assisted navigation tools will become increasingly important in surgery and in residents\u2019 education.MethodsWe obtained 3D reconstructions from simple spiral computed tomography (CT) slides using OsiriX, an open source processing software package dedicated to DICOM images. These images were then projected on the patient\'s body with a beamer fixed to the operating table to enhance spatial perception during surgical intervention (augmented reality).ResultsChanging a window\'s deepness level allowed the surgeon to navigate through the patient\'s anatomy, highlighting regions of interest and marked pathologies. We used image overlay navigation for laparoscopic operations such cholecystectomy, abdominal exploration, distal pancreas resection and robotic liver resection.ConclusionsAugmented reality techniques will transform the behaviour of surgeons, making surgical interventions easier, faster and probably safer. These new techniques will also renew methods of surgical teaching, facilitating transmission of knowledge and skill to young surgeons.","paperId":"78c9d7d5bb46e604a9ddca428a632c281bdd9fbc","paperTitle":"Augmented reality and image overlay navigation with OsiriX in laparoscopic and robotic surgery: not only a matter of fashion","paperUrl":"https://www.semanticscholar.org/paper/78c9d7d5bb46e604a9ddca428a632c281bdd9fbc","images":[],"pdf":"https://www.ncbi.nlm.nih.gov/pubmed/21487758"},{"type":"inproceedings","key":"lerotic2007pq","title":"Pq-Space Based Non-Photorealistic Rendering for Augmented Reality","author":"Lerotic, Mirna and Chung, Adrian J and Mylonas, George and Yang, Guang-Zhong","booktitle":"International Conference on Medical Image Computing and Computer-Assisted Intervention","pages":"102--109","year":"2007","organization":"Springer","authors":["Mirna Lerotic","Adrian J Chung","George Mylonas","Guang-Zhong Yang"],"doi":"10.1007/978-3-540-75759-7_13","dblp":"conf/miccai/LeroticCMY07","venue":"MICCAI","abstract":"The increasing use of robotic assisted minimally invasive surgery (MIS) provides an ideal environment for using Augmented Reality (AR) for performing image guided surgery. Seamless synthesis of AR depends on a number of factors relating to the way in which virtual objects appear and visually interact with a real environment. Traditional overlaid AR approaches generally suffer from a loss of depth perception. This paper presents a new AR method for robotic assisted MIS, which uses a novel pq-space based non-photorealistic rendering technique for providing see-through vision of the embedded virtual object whilst maintaining salient anatomical details of the exposed anatomical surface. Experimental results with both phantom and in vivo lung lobectomy data demonstrate the visual realism achieved for the proposed method and its accuracy in providing high fidelity AR depth perception.","paperId":"d97aac257457261f726ddc38b1bc82de1ab93016","paperTitle":"pq-space Based Non-Photorealistic Rendering for Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/d97aac257457261f726ddc38b1bc82de1ab93016","images":["https://d3i71xaburhd42.cloudfront.net/d97aac257457261f726ddc38b1bc82de1ab93016/4-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/d97aac257457261f726ddc38b1bc82de1ab93016/7-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/d97aac257457261f726ddc38b1bc82de1ab93016/5-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/d97aac257457261f726ddc38b1bc82de1ab93016/6-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/d97aac257457261f726ddc38b1bc82de1ab93016/7-Figure4-1.png"],"pdf":"https://link.springer.com/content/pdf/10.1007%2F978-3-540-75759-7_13.pdf"},{"type":"inproceedings","key":"rodriguez2004enhancing","title":"Enhancing a Telerobotics Java Tool with Augmented Reality","author":"Rodriguez, Nancy and Pulido, Luis Jose and Jessel, Jean-Pierre","booktitle":"International Symposium and School on Advancex Distributed Systems","pages":"9--18","year":"2004","organization":"Springer","authors":["Nancy Rodriguez","Luis Jose Pulido","Jean-Pierre Jessel"],"doi":"10.1007/978-3-540-25958-9_2","dblp":"conf/issads/RodriguezPJ04","venue":"ISSADS","abstract":"This paper describes the integration of an Augmented Reality service into our telerobotics system ASSET. ASSET is a teleoperation tool written in Java offering the services of simulation, 3D visualization, devices management and Java3D/VRML2.0 models loading. ASSET allows the definition of behaviors for each simulation object, and hence, entities sharing the same environment can have different degrees of autonomy. The Augmented Reality service that we have integrated uses the Java binding of the ARToolkit in order to allow operators and autonomous robots to gather information about the mission. Information points are represented in the real world by visual patterns, which trigger actions to be executed by the robot or activate virtual objects display when recognized by the Augmented Reality Service.","paperId":"2f24252c3bdc23af4eb8b5115dc22a9ce1e03312","paperTitle":"Enhancing a Telerobotics Java Tool with Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/2f24252c3bdc23af4eb8b5115dc22a9ce1e03312","images":["https://d3i71xaburhd42.cloudfront.net/2f24252c3bdc23af4eb8b5115dc22a9ce1e03312/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/2f24252c3bdc23af4eb8b5115dc22a9ce1e03312/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/2f24252c3bdc23af4eb8b5115dc22a9ce1e03312/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/2f24252c3bdc23af4eb8b5115dc22a9ce1e03312/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/2f24252c3bdc23af4eb8b5115dc22a9ce1e03312/8-Figure5-1.png"],"pdf":"http://www2.lirmm.fr/~rodrigue/pdf/isaads04.pdf"},{"type":"article","key":"piardi2019arena","title":"ARENA\u2014Augmented Reality to Enhanced Experimentation in Smart Warehouses","author":"Piardi, Luis and Kalempa, Vivian Cremer and Limeira, Marcelo and de Oliveira, Andr\'e Schneider and Leit~ao, Paulo","journal":"Sensors","volume":"19","number":"19","pages":"4308","year":"2019","publisher":"Multidisciplinary Digital Publishing Institute","authors":["Luis Piardi","Vivian Cremer Kalempa","Marcelo Limeira","Andr\'e Schneider de Oliveira","Paulo Leit~ao"],"doi":"10.3390/s19194308","dblp":"journals/sensors/PiardiKLOA19","venue":"Sensors","abstract":"The current industrial scenario demands advances that depend on expensive and sophisticated solutions. Augmented Reality (AR) can complement, with virtual elements, the real world. Faced with this features, an AR experience can meet the demand for prototype testing and new solutions, predicting problems and failures that may only exist in real situations. This work presents an environment for experimentation of advanced behaviors in smart factories, allowing experimentation with multi-robot systems (MRS), interconnected, cooperative, and interacting with virtual elements. The concept of ARENA introduces a novel approach to realistic and immersive experimentation in industrial environments, aiming to evaluate new technologies aligned with the Industry 4.0. The proposed method consists of a small-scale warehouse, inspired in a real scenario characterized in this paper, managing by a group of autonomous forklifts, fully interconnected, which are embodied by a swarm of tiny robots developed and prepared to operate in the small scale scenario. The AR is employed to enhance the capabilities of swarm robots, allowing box handling and virtual forklifts. Virtual laser range finders (LRF) are specially designed as segmentation of a global RGB-D camera, to improve robot perception, allowing obstacle avoidance and environment mapping. This infrastructure enables the evaluation of new strategies to improve manufacturing productivity, without compromising the production by automation faults.","paperId":"ac70977492f7c2cb1aa14292b2a67469616b136b","paperTitle":"ARENA\u2014Augmented Reality to Enhanced Experimentation in Smart Warehouses","paperUrl":"https://www.semanticscholar.org/paper/ac70977492f7c2cb1aa14292b2a67469616b136b","images":["https://d3i71xaburhd42.cloudfront.net/f246dea966e6442223ce70d0c46822dec43d30ef/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/f246dea966e6442223ce70d0c46822dec43d30ef/7-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/f246dea966e6442223ce70d0c46822dec43d30ef/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/f246dea966e6442223ce70d0c46822dec43d30ef/13-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/f246dea966e6442223ce70d0c46822dec43d30ef/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/f246dea966e6442223ce70d0c46822dec43d30ef/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/f246dea966e6442223ce70d0c46822dec43d30ef/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/f246dea966e6442223ce70d0c46822dec43d30ef/7-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/f246dea966e6442223ce70d0c46822dec43d30ef/8-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/f246dea966e6442223ce70d0c46822dec43d30ef/9-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/f246dea966e6442223ce70d0c46822dec43d30ef/10-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/f246dea966e6442223ce70d0c46822dec43d30ef/11-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/f246dea966e6442223ce70d0c46822dec43d30ef/11-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/f246dea966e6442223ce70d0c46822dec43d30ef/12-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/f246dea966e6442223ce70d0c46822dec43d30ef/12-Figure13-1.png"],"pdf":null},{"type":"article","key":"khatib2021human","title":"Human-Robot Contactless Collaboration with Mixed Reality Interface","author":"Khatib, Maram and Al Khudir, Khaled and De Luca, Alessandro","journal":"Robotics and Computer-Integrated Manufacturing","volume":"67","pages":"102030","year":"2021","publisher":"Elsevier","authors":["Maram Khatib","Khaled Al Khudir","Alessandro De Luca"],"doi":"10.1016/j.rcim.2020.102030","dblp":"journals/rcim/KhatibKL21","venue":"Robotics Comput. Integr. Manuf.","abstract":null,"paperId":"a36f0b7b7cef46789be0cddf616903b576a118a4","paperTitle":"Human-robot contactless collaboration with mixed reality interface","paperUrl":"https://www.semanticscholar.org/paper/a36f0b7b7cef46789be0cddf616903b576a118a4","images":["https://d3i71xaburhd42.cloudfront.net/efc549b05d69603a742ae7e56a4921b13d6808df/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/efc549b05d69603a742ae7e56a4921b13d6808df/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/efc549b05d69603a742ae7e56a4921b13d6808df/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/efc549b05d69603a742ae7e56a4921b13d6808df/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/efc549b05d69603a742ae7e56a4921b13d6808df/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/efc549b05d69603a742ae7e56a4921b13d6808df/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/efc549b05d69603a742ae7e56a4921b13d6808df/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/efc549b05d69603a742ae7e56a4921b13d6808df/6-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/efc549b05d69603a742ae7e56a4921b13d6808df/8-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/efc549b05d69603a742ae7e56a4921b13d6808df/8-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/efc549b05d69603a742ae7e56a4921b13d6808df/9-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/efc549b05d69603a742ae7e56a4921b13d6808df/9-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/efc549b05d69603a742ae7e56a4921b13d6808df/10-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/efc549b05d69603a742ae7e56a4921b13d6808df/10-Figure14-1.png","https://d3i71xaburhd42.cloudfront.net/efc549b05d69603a742ae7e56a4921b13d6808df/11-Figure15-1.png"],"pdf":"https://iris.uniroma1.it/retrieve/handle/11573/1434612/1542340/Khatib_Postprint_Human-robot_2020.pdf"},{"type":"article","key":"sauer2009towards","title":"Towards a Predictive Mixed Reality User Interface for Mobile Robot Teleoperation","author":"Sauer, Markus and Hess, Martin and Schilling, Klaus","journal":"IFAC Proceedings Volumes","volume":"42","number":"22","pages":"91--96","year":"2009","publisher":"Elsevier","authors":["Markus Sauer","Martin Hess","Klaus Schilling"],"doi":"10.3182/20091006-3-US-4006.00016","venue":"","abstract":"Abstract Lack of situation awareness significantly decreases the performance in missions where a mobile robot is operated by a human from remote. The user interface is a key influencing element for situation awareness of the human operator. The information from the remote site is limited to what the robot\'s sensors can provide. In addition, this information is in general only delivered with a certain - maybe varying - communication delay. Predictive displays provide a promising approach to cope with these problems. In order to increase the situation awareness for the human operator in teleoperation scenarios predictive user interfaces can be used to achieve an artificial excoentric view. This work presents an approach how a predictive mixed reality user interface can be realized with the help of motion control theory. The human operator commands a virtual robot projected into the camera image delivered to the human operator from the physical robot. Hereby a trajectory for the real physical robot is generated, which is executed by the physical robot after a certain time. Combined with mixed reality technologies an artificial, exocentric view of the mobile robot is achieved which leads to a short time predictive user interface for mobile-robot teleoperation.","paperId":"edfb8e8b01e2ad5e82a861ae45c9d2674909e6d8","paperTitle":"Towards a Predictive Mixed Reality User Interface for Mobile Robot Teleoperation","paperUrl":"https://www.semanticscholar.org/paper/edfb8e8b01e2ad5e82a861ae45c9d2674909e6d8","images":[],"pdf":null},{"type":"article","key":"chong2009robot","title":"Robot Programming Using Augmented Reality: An Interactive Method for Planning Collision-Free Paths","author":"Chong, Jonathan Wun Shiung and Ong, SKc and Nee, Andrew YC and Youcef-Youmi, KB","journal":"Robotics and Computer-Integrated Manufacturing","volume":"25","number":"3","pages":"689--701","year":"2009","publisher":"Elsevier","authors":["Jonathan Wun Shiung Chong","SKc Ong","Andrew YC Nee","KB Youcef-Youmi"],"doi":"10.1016/J.RCIM.2008.05.002","venue":"","abstract":null,"paperId":"0a1e6585e932e1b6c79e327dbc0d05727da8d847","paperTitle":"Robot programming using augmented reality: An interactive method for planning collision-free paths","paperUrl":"https://www.semanticscholar.org/paper/0a1e6585e932e1b6c79e327dbc0d05727da8d847","images":[],"pdf":null},{"type":"article","key":"wassermann2018intuitive","title":"Intuitive Robot Programming through Environment Perception, Augmented Reality Simulation and Automated Program Verification","author":"Wassermann, Jonas and Vick, Axel and Kr\\"uger, J\\"org","journal":"Procedia CIRP","volume":"76","pages":"161--166","year":"2018","publisher":"Elsevier","authors":["Jonas Wassermann","Axel Vick","J\\"org Kr\\"uger"],"doi":"10.1016/J.PROCIR.2018.01.036","venue":"","abstract":null,"paperId":"5f44557ffdcce182e46b1a9fcdbf1f433c9449e8","paperTitle":"Intuitive robot programming through environment perception, augmented reality simulation and automated program verification","paperUrl":"https://www.semanticscholar.org/paper/5f44557ffdcce182e46b1a9fcdbf1f433c9449e8","images":[],"pdf":"https://doi.org/10.1016/j.procir.2018.01.036"},{"type":"article","key":"kousi2019enabling","title":"Enabling Human Robot Interaction in Flexible Robotic Assembly Lines: An Augmented Reality Based Software Suite","author":"Kousi, Niki and Stoubos, Christos and Gkournelos, Christos and Michalos, George and Makris, Sotiris","journal":"Procedia CIRP","volume":"81","pages":"1429--1434","year":"2019","publisher":"Elsevier","authors":["Niki Kousi","Christos Stoubos","Christos Gkournelos","George Michalos","Sotiris Makris"],"doi":"10.1016/J.PROCIR.2019.04.328","venue":"Procedia CIRP","abstract":null,"paperId":"43b7dbdb34cc28411959d8be10e416d47a528aac","paperTitle":"Enabling Human Robot Interaction in flexible robotic assembly lines: an Augmented Reality based software suite","paperUrl":"https://www.semanticscholar.org/paper/43b7dbdb34cc28411959d8be10e416d47a528aac","images":[],"pdf":"https://doi.org/10.1016/j.procir.2019.04.328"},{"type":"article","key":"tavares2019collaborative","title":"Collaborative Welding System Using BIM for Robotic Reprogramming and Spatial Augmented Reality","author":"Tavares, Pedro and Costa, Carlos M and Rocha, Lu\'is and Malaca, Pedro and Costa, Pedro and Moreira, Ant\'onio P and Sousa, Armando and Veiga, Germano","journal":"Automation in Construction","volume":"106","pages":"102825","year":"2019","publisher":"Elsevier","authors":["Pedro Tavares","Carlos M Costa","Lu\'is Rocha","Pedro Malaca","Pedro Costa","Ant\'onio P Moreira","Armando Sousa","Germano Veiga"],"doi":"10.1016/J.AUTCON.2019.04.020","venue":"Automation in Construction","abstract":null,"paperId":"33aa258b6ec6158be0cd6e95e04d1506551eec67","paperTitle":"Collaborative Welding System using BIM for Robotic Reprogramming and Spatial Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/33aa258b6ec6158be0cd6e95e04d1506551eec67","images":["https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/6-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/7-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/7-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/7-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/8-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/8-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/8-Figure14-1.png","https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/9-Figure15-1.png","https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/9-Figure16-1.png","https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/9-Figure17-1.png","https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/10-Figure19-1.png","https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/10-Figure20-1.png","https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/11-Figure21-1.png","https://d3i71xaburhd42.cloudfront.net/f5371007cc50e0ed8cecbd87fe4a6cba1ddcfd1f/11-Figure22-1.png"],"pdf":"https://repositorio.inesctec.pt/bitstream/123456789/10832/1/P-00Q-HXQ.pdf"},{"type":"article","key":"falk2005cardio","title":"Cardio Navigation: Planning, Simulation, and Augmented Reality in Robotic Assisted Endoscopic Bypass Grafting","author":"Falk, Volkmar and Mourgues, Fabien and Adhami, Loua\\"i and Jacobs, Stefan and Thiele, Holger and Nitzsche, Stefan and Mohr, Friedrich W and Coste-Mani`ere, `Eve","journal":"The Annals of thoracic surgery","volume":"79","number":"6","pages":"2040--2047","year":"2005","publisher":"Elsevier","authors":["Volkmar Falk","Fabien Mourgues","Loua\\"i Adhami","Stefan Jacobs","Holger Thiele","Stefan Nitzsche","Friedrich W Mohr","`Eve Coste-Mani`ere"],"doi":"10.1016/J.ATHORACSUR.2004.11.060","venue":"The Annals of thoracic surgery","abstract":null,"paperId":"453cf46c67c1bad39d20cbd53921b8e51387340a","paperTitle":"Cardio navigation: planning, simulation, and augmented reality in robotic assisted endoscopic bypass grafting.","paperUrl":"https://www.semanticscholar.org/paper/453cf46c67c1bad39d20cbd53921b8e51387340a","images":[],"pdf":"https://doi.org/10.1016/J.ATHORACSUR.2004.11.060"},{"type":"article","key":"schiavina2021augmented","title":"Augmented Reality to Guide Selective Clamping and Tumor Dissection during Robot-Assisted Partial Nephrectomy: A Preliminary Experience","author":"Schiavina, Riccardo and Bianchi, Lorenzo and Chessa, Francesco and Barbaresi, Umberto and Cercenelli, Laura and Lodi, Simone and Gaudiano, Caterina and Bortolani, Barbara and Angiolini, Andrea and Bianchi, Federico Mineo and others","journal":"Clinical genitourinary cancer","volume":"19","number":"3","pages":"e149--e155","year":"2021","publisher":"Elsevier","authors":["Riccardo Schiavina","Lorenzo Bianchi","Francesco Chessa","Umberto Barbaresi","Laura Cercenelli","Simone Lodi","Caterina Gaudiano","Barbara Bortolani","Andrea Angiolini","Federico Mineo Bianchi","undefined others"],"doi":"10.1016/j.clgc.2020.09.005","venue":"Clinical genitourinary cancer","abstract":null,"paperId":"18cb556ed0a410defa407349995e17b24be3af25","paperTitle":"Augmented Reality to Guide Selective Clamping and Tumor Dissection During Robot-assisted Partial Nephrectomy: A Preliminary Experience.","paperUrl":"https://www.semanticscholar.org/paper/18cb556ed0a410defa407349995e17b24be3af25","images":[],"pdf":"http://www.clinical-genitourinary-cancer.com/article/S1558767320302214/pdf"},{"type":"article","key":"livatino2010video","title":"Video and Laser Based Augmented Reality Stereoscopic Viewing for Mobile Robot Teleoperation","author":"Livatino, Salvatore and Muscato, Giovanni and Banno, Filippo and De Tommaso, Davide and Macaluso, Marco","journal":"IFAC Proceedings Volumes","volume":"43","number":"23","pages":"161--168","year":"2010","publisher":"Elsevier","authors":["Salvatore Livatino","Giovanni Muscato","Filippo Banno","Davide De Tommaso","Marco Macaluso"],"doi":"10.3182/20101005-4-RO-2018.00049","dblp":"conf/ta/LivatinoMBTM10","venue":"TA","abstract":"Abstract This paper proposes an augmented reality visualization interface to simultaneously present visual and laser sensors information further enhanced by stereoscopic viewing. The use of augmented layers is proposed to represent laser measurements suitably aligned to video information. This methodology enables an operator to intuitively comprehend object proximity and to respond in an accurate and timely manner. The use of augmented reality to assist teleoperation, sometime discussed in the literature, is here proposed following a systematic approach and developed based on authors\' previous work on stereoscopic teleoperation. The approach is experimented on a real telerobotic system where a user operates a mobile robot located thousands kilometers away. The result proved feasibility and simplicity of the proposed methodology and it represents a base for further studies.","paperId":"cabf167f64e4b129537a9dcff7266f2a6810c5af","paperTitle":"Video and Laser Based Augmented Reality Stereoscopic Viewing for Mobile Robot Teleoperation","paperUrl":"https://www.semanticscholar.org/paper/cabf167f64e4b129537a9dcff7266f2a6810c5af","images":[],"pdf":null},{"type":"article","key":"porpiglia2019three","title":"Three-Dimensional Elastic Augmented-Reality Robot-Assisted Radical Prostatectomy Using Hyperaccuracy Three-Dimensional Reconstruction Technology: A Step Further in the Identification of Capsular Involvement","author":"Porpiglia, Francesco and Checcucci, Enrico and Amparore, Daniele and Manfredi, Matteo and Massa, Federica and Piazzolla, Pietro and Manfrin, Diego and Piana, Alberto and Tota, Daniele and Bollito, Enrico and others","journal":"European urology","volume":"76","number":"4","pages":"505--514","year":"2019","publisher":"Elsevier","authors":["Francesco Porpiglia","Enrico Checcucci","Daniele Amparore","Matteo Manfredi","Federica Massa","Pietro Piazzolla","Diego Manfrin","Alberto Piana","Daniele Tota","Enrico Bollito","undefined others"],"doi":"10.1016/j.eururo.2019.03.037","venue":"European urology","abstract":null,"paperId":"c9cd7e00b1672d0d4febb2c297c2a8ecccf833b9","paperTitle":"Three-dimensional Elastic Augmented-reality Robot-assisted Radical Prostatectomy Using Hyperaccuracy Three-dimensional Reconstruction Technology: A Step Further in the Identification of Capsular Involvement.","paperUrl":"https://www.semanticscholar.org/paper/c9cd7e00b1672d0d4febb2c297c2a8ecccf833b9","images":[],"pdf":"https://doi.org/10.1016/j.eururo.2019.03.037"},{"type":"article","key":"schiavina2021real","title":"Real-Time Augmented Reality Three-Dimensional Guided Robotic Radical Prostatectomy: Preliminary Experience and Evaluation of the Impact on Surgical Planning","author":"Schiavina, Riccardo and Bianchi, Lorenzo and Lodi, Simone and Cercenelli, Laura and Chessa, Francesco and Bortolani, Barbara and Gaudiano, Caterina and Casablanca, Carlo and Droghetti, Matteo and Porreca, Angelo and others","journal":"European urology focus","volume":"7","number":"6","pages":"1260--1267","year":"2021","publisher":"Elsevier","authors":["Riccardo Schiavina","Lorenzo Bianchi","Simone Lodi","Laura Cercenelli","Francesco Chessa","Barbara Bortolani","Caterina Gaudiano","Carlo Casablanca","Matteo Droghetti","Angelo Porreca","undefined others"],"doi":"10.1016/j.euf.2020.08.004","venue":"European urology focus","abstract":null,"paperId":"71797dcc3c37953a7153be314f8005d7cca57ce6","paperTitle":"Real-time Augmented Reality Three-dimensional Guided Robotic Radical Prostatectomy: Preliminary Experience and Evaluation of the Impact on Surgical Planning.","paperUrl":"https://www.semanticscholar.org/paper/71797dcc3c37953a7153be314f8005d7cca57ce6","images":[],"pdf":"https://doi.org/10.1016/j.euf.2020.08.004"},{"type":"misc","key":"navab2016personalized","title":"Personalized, Relevance-Based Multimodal Robotic Imaging and Augmented Reality for Computer Assisted Interventions","author":"Navab, Nassir and Hennersperger, Christoph and Frisch, Benjamin and F\\"urst, Bernhard","journal":"Medical image analysis","volume":"33","pages":"64--71","year":"2016","publisher":"Elsevier","authors":["Nassir Navab","Christoph Hennersperger","Benjamin Frisch","Bernhard F\\"urst"],"doi":"10.1016/j.media.2016.06.021","dblp":"journals/mia/NavabFHFF16","venue":"Medical Image Anal.","abstract":null,"paperId":"ac995052b6da4d923ecdd43e88f5aa47b861258c","paperTitle":"Personalized, relevance-based Multimodal Robotic Imaging and augmented reality for Computer Assisted Interventions","paperUrl":"https://www.semanticscholar.org/paper/ac995052b6da4d923ecdd43e88f5aa47b861258c","images":[],"pdf":"https://doi.org/10.1016/j.media.2016.06.021"},{"type":"article","key":"lin2016mandibular","title":"Mandibular Angle Split Osteotomy Based on a Novel Augmented Reality Navigation Using Specialized Robot-Assisted Arms\u2014a Feasibility Study","author":"Lin, Li and Shi, Yunyong and Tan, Andy and Bogari, Melia and Zhu, Ming and Xin, Yu and Xu, Haisong and Zhang, Yan and Xie, Le and Chai, Gang","journal":"Journal of Cranio-Maxillofacial Surgery","volume":"44","number":"2","pages":"215--223","year":"2016","publisher":"Elsevier","authors":["Li Lin","Yunyong Shi","Andy Tan","Melia Bogari","Ming Zhu","Yu Xin","Haisong Xu","Yan Zhang","Le Xie","Gang Chai"],"doi":"10.1016/j.jcms.2015.10.024","venue":"Journal of cranio-maxillo-facial surgery : official publication of the European Association for Cranio-Maxillo-Facial Surgery","abstract":null,"paperId":"b14b0474f7a6bb0ed8d42a5f07b0abf6b6c71564","paperTitle":"Mandibular angle split osteotomy based on a novel augmented reality navigation using specialized robot-assisted arms--A feasibility study.","paperUrl":"https://www.semanticscholar.org/paper/b14b0474f7a6bb0ed8d42a5f07b0abf6b6c71564","images":[],"pdf":"https://doi.org/10.1016/j.jcms.2015.10.024"},{"type":"article","key":"mueller2019intuitive","title":"Intuitive Welding Robot Programming via Motion Capture and Augmented Reality","author":"Mueller, Fabian and Deuerlein, Christian and Koch, Michael","journal":"IFAC-PapersOnLine","volume":"52","number":"10","pages":"294--299","year":"2019","publisher":"Elsevier","authors":["Fabian Mueller","Christian Deuerlein","Michael Koch"],"doi":"10.1016/j.ifacol.2019.10.045","venue":"IFAC-PapersOnLine","abstract":null,"paperId":"0e57db206f19503036094425df723444914e6f36","paperTitle":"Intuitive Welding Robot Programming via Motion Capture and Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/0e57db206f19503036094425df723444914e6f36","images":[],"pdf":null},{"type":"article","key":"yew2017immersive","title":"Immersive Augmented Reality Environment for the Teleoperation of Maintenance Robots","author":"Yew, AWW and Ong, SK and Nee, AYC","journal":"Procedia Cirp","volume":"61","pages":"305--310","year":"2017","publisher":"Elsevier","authors":["AWW Yew","SK Ong","AYC Nee"],"doi":"10.1016/J.PROCIR.2016.11.183","venue":"","abstract":null,"paperId":"69c80e78fcd6d9d3cb659caa1449dabab241b76c","paperTitle":"Immersive Augmented Reality Environment for the Teleoperation of Maintenance Robots","paperUrl":"https://www.semanticscholar.org/paper/69c80e78fcd6d9d3cb659caa1449dabab241b76c","images":[],"pdf":"https://doi.org/10.1016/j.procir.2016.11.183"},{"type":"article","key":"guhl2018enabling","title":"Enabling Human-Robot-Interaction via Virtual and Augmented Reality in Distributed Control Systems","author":"Guhl, Jan and H\\"ugle, Johannes and Kr\\"uger, J\\"org","journal":"Procedia CIRP","volume":"76","pages":"167--170","year":"2018","publisher":"Elsevier","authors":["Jan Guhl","Johannes H\\"ugle","J\\"org Kr\\"uger"],"doi":"10.1016/J.PROCIR.2018.01.029","venue":"","abstract":null,"paperId":"dc3f8a1d2b51a82e34471164b39e1ed9d20e295a","paperTitle":"Enabling Human-Robot-Interaction via Virtual and Augmented Reality in Distributed Control Systems","paperUrl":"https://www.semanticscholar.org/paper/dc3f8a1d2b51a82e34471164b39e1ed9d20e295a","images":[],"pdf":"https://doi.org/10.1016/j.procir.2018.01.029"},{"type":"article","key":"wang2020closed","title":"Closed-Loop Augmented Reality towards Accurate Human-Robot Collaboration","author":"Wang, Xi Vincent and Wang, Lihui and Lei, Mingtian and Zhao, Yongqing","journal":"CIRP annals","volume":"69","number":"1","pages":"425--428","year":"2020","publisher":"Elsevier","authors":["Xi Vincent Wang","Lihui Wang","Mingtian Lei","Yongqing Zhao"],"doi":"10.1016/j.cirp.2020.03.014","venue":"","abstract":null,"paperId":"5a58704af5c575e8f098228a0f2b48d0c38119a7","paperTitle":"Closed-loop augmented reality towards accurate human-robot collaboration","paperUrl":"https://www.semanticscholar.org/paper/5a58704af5c575e8f098228a0f2b48d0c38119a7","images":[],"pdf":null},{"type":"inproceedings","key":"traub2004augmented","title":"Augmented Reality for Port Placement and Navigation in Robotically Assisted Minimally Invasive Cardiovascular Surgery","author":"Traub, J\\"org and Feuerstein, Marco and Bauer, Martin and Schirmbeck, Eva U and Najafi, Hesam and Bauernschmitt, Robert and Klinker, Gudrun","booktitle":"International Congress Series","volume":"1268","pages":"735--740","year":"2004","organization":"Elsevier","authors":["J\\"org Traub","Marco Feuerstein","Martin Bauer","Eva U Schirmbeck","Hesam Najafi","Robert Bauernschmitt","Gudrun Klinker"],"doi":"10.1016/J.ICS.2004.03.049","dblp":"conf/cars/TraubFBSNBK04","venue":"CARS","abstract":null,"paperId":"a642eb6e07e72c78f0129d6fb9321dd3cfc974b7","paperTitle":"Augmented reality for port placement and navigation in robotically assisted minimally invasive cardiovascular surgery","paperUrl":"https://www.semanticscholar.org/paper/a642eb6e07e72c78f0129d6fb9321dd3cfc974b7","images":["https://d3i71xaburhd42.cloudfront.net/a642eb6e07e72c78f0129d6fb9321dd3cfc974b7/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/a642eb6e07e72c78f0129d6fb9321dd3cfc974b7/4-Figure4-1.png"],"pdf":"http://campar.in.tum.de/pub/traub2004portplacement/traub2004portplacement.pdf"},{"type":"article","key":"zhao2017augmented","title":"Augmented Reality for Enhancing Tele-Robotic System with Force Feedback","author":"Zhao, Zhou and Huang, Panfeng and Lu, Zhenyu and Liu, Zhengxiong","journal":"Robotics and Autonomous Systems","volume":"96","pages":"93--101","year":"2017","publisher":"Elsevier","authors":["Zhou Zhao","Panfeng Huang","Zhenyu Lu","Zhengxiong Liu"],"doi":"10.1016/j.robot.2017.05.017","dblp":"journals/ras/ZhaoHLL17","venue":"Robotics Auton. Syst.","abstract":null,"paperId":"2878c19c2422b8e9ddc9a5007036752c05d8ab58","paperTitle":"Augmented reality for enhancing tele-robotic system with force feedback","paperUrl":"https://www.semanticscholar.org/paper/2878c19c2422b8e9ddc9a5007036752c05d8ab58","images":[],"pdf":null},{"type":"article","key":"chacko2020augmented2","title":"An Augmented Reality Framework for Robotic Tool-Path Teaching","author":"Chacko, Sonia Mary and Granado, Armando and Kapila, Vikram","journal":"Procedia CIRP","volume":"93","pages":"1218--1223","year":"2020","publisher":"Elsevier","authors":["Sonia Mary Chacko","Armando Granado","Vikram Kapila"],"doi":"10.1016/j.procir.2020.03.143","venue":"","abstract":null,"paperId":"22bd14b9ff0178fa5d223ce59290e0b2af3efbeb","paperTitle":"An Augmented Reality Framework for Robotic Tool-path Teaching","paperUrl":"https://www.semanticscholar.org/paper/22bd14b9ff0178fa5d223ce59290e0b2af3efbeb","images":[],"pdf":null},{"type":"article","key":"mourtzis2020augmented","title":"An Augmented Reality Application for Robotic Cell Customization","author":"Mourtzis, D and Synodinos, G and Angelopoulos, J and Panopoulos, N","journal":"Procedia CIRP","volume":"90","pages":"654--659","year":"2020","publisher":"Elsevier","authors":["D Mourtzis","G Synodinos","J Angelopoulos","N Panopoulos"],"doi":"10.1016/j.procir.2020.02.135","venue":"","abstract":null,"paperId":"42c7e0b8b688c343ae1c31058a857d071583fef4","paperTitle":"An augmented reality application for robotic cell customization","paperUrl":"https://www.semanticscholar.org/paper/42c7e0b8b688c343ae1c31058a857d071583fef4","images":[],"pdf":null},{"type":"article","key":"fang2012robot","title":"Robot Path and End-Effector Orientation Planning Using Augmented Reality","author":"Fang, HC and Ong, SK and Nee, AYC","journal":"Procedia CIRP","volume":"3","pages":"191--196","year":"2012","publisher":"Elsevier","authors":["HC Fang","SK Ong","AYC Nee"],"doi":"10.1016/J.PROCIR.2012.07.034","venue":"","abstract":null,"paperId":"3754b364c9555a9970f80c6684446e2d53278778","paperTitle":"Robot Path and End-Effector Orientation Planning Using Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/3754b364c9555a9970f80c6684446e2d53278778","images":[],"pdf":"https://doi.org/10.1016/j.procir.2012.07.034"},{"type":"inproceedings","key":"siegele2021optimizing","title":"Optimizing Collaborative Robotic Workspaces in Industry by Applying Mixed Reality","author":"Siegele, Dietmar and Steiner, Dieter and Giusti, Andrea and Riedl, Michael and Matt, Dominik T","booktitle":"International Conference on Augmented Reality, Virtual Reality and Computer Graphics","pages":"544--559","year":"2021","organization":"Springer","authors":["Dietmar Siegele","Dieter Steiner","Andrea Giusti","Michael Riedl","Dominik T Matt"],"doi":"10.1007/978-3-030-87595-4_40","dblp":"conf/avr/SiegeleSGRM21","venue":"AVR","abstract":null,"paperId":"d5fb7e8ad355389d70cda4972cb1ebf057827426","paperTitle":"Optimizing Collaborative Robotic Workspaces in Industry by Applying Mixed Reality","paperUrl":"https://www.semanticscholar.org/paper/d5fb7e8ad355389d70cda4972cb1ebf057827426","images":[],"pdf":null},{"type":"inproceedings","key":"prattico2020learning","title":"Is Learning by Teaching an Effective Approach in Mixed-Reality Robotic Training Systems?","author":"Prattic`o, Filippo Gabriele and Merino, Francisco Navarro and Lamberti, Fabrizio","booktitle":"International Conference on Intelligent Technologies for Interactive Entertainment","pages":"177--190","year":"2020","organization":"Springer","authors":["Filippo Gabriele Prattic`o","Francisco Navarro Merino","Fabrizio Lamberti"],"doi":"10.1007/978-3-030-76426-5_12","venue":"","abstract":null,"paperId":"345d4e4c323fcb8ad6684f835f75980abb16c472","paperTitle":"Is Learning by Teaching an Effective Approach in Mixed-Reality Robotic Training Systems?","paperUrl":"https://www.semanticscholar.org/paper/345d4e4c323fcb8ad6684f835f75980abb16c472","images":[],"pdf":null},{"type":"inproceedings","key":"murphy2020prototyping","title":"Prototyping Sensors and Actuators for Robot Swarms in Mixed Reality","author":"Murphy, Alex and Millard, Alan G","booktitle":"Annual Conference Towards Autonomous Robotic Systems","pages":"377--386","year":"2020","organization":"Springer","authors":["Alex Murphy","Alan G Millard"],"doi":"10.1007/978-3-030-63486-5_39","dblp":"conf/taros/MurphyM20","venue":"TAROS","abstract":"Swarm robotics is an approach to the coordination of large numbers of relatively simple robots with limited physical capabilities. Extending the capabilities of these robots usually requires either purchasing or prototyping new sensors or actuators (a costly and time-consuming process), often before even knowing whether these new sensors or actuators will be able to perform the necessary task effectively. This paper presents a software platform that enables researchers to prototype new sensors and actuators in the virtual world, eliminating the time and resources necessary to prototype physical hardware, and allows for experiments to be run in as many configurations as required in order to determine the efficacy of the proposed sensor/actuator.","paperId":"7501eeae2e93d998ca19e5d34ddb1a17ceaa829f","paperTitle":"Prototyping Sensors and Actuators for Robot Swarms in Mixed Reality","paperUrl":"https://www.semanticscholar.org/paper/7501eeae2e93d998ca19e5d34ddb1a17ceaa829f","images":[],"pdf":null},{"type":"inproceedings","key":"chen2020industrial","title":"Industrial Robot Training Platform Based on Virtual Reality and Mixed Reality Technology","author":"Chen, Zhe and Cao, Zhuohang and Ma, Peili and Xu, Lijun","booktitle":"International Conference on Man-Machine-Environment System Engineering","pages":"891--898","year":"2020","organization":"Springer","authors":["Zhe Chen","Zhuohang Cao","Peili Ma","Lijun Xu"],"doi":"10.1007/978-981-15-6978-4_102","venue":"","abstract":null,"paperId":"fe9bae02fad7ea8df20f69cc32d772ee1c87bb54","paperTitle":"Industrial Robot Training Platform Based on Virtual Reality and Mixed Reality Technology","paperUrl":"https://www.semanticscholar.org/paper/fe9bae02fad7ea8df20f69cc32d772ee1c87bb54","images":[],"pdf":null},{"type":"inproceedings","key":"cancedda2017mixed","title":"Mixed Reality-Based User Interaction Feedback for a Hand-Controlled Interface Targeted to Robot Teleoperation","author":"Cancedda, Laura and Cannav`o, Alberto and Garofalo, Giuseppe and Lamberti, Fabrizio and Montuschi, Paolo and Paravati, Gianluca","booktitle":"International Conference on Augmented Reality, Virtual Reality and Computer Graphics","pages":"447--463","year":"2017","organization":"Springer","authors":["Laura Cancedda","Alberto Cannav`o","Giuseppe Garofalo","Fabrizio Lamberti","Paolo Montuschi","Gianluca Paravati"],"doi":"10.1007/978-3-319-60928-7_38","dblp":"conf/avr/CanceddaCGLMP17","venue":"AVR","abstract":"The continuous progress in the field of robotics and the diffusion of its related application scenarios in today\u2019s modern world makes human interaction and communication with robots an aspect of fundamental importance. The development of interfaces based on natural interaction paradigms is getting an increasingly captivating topic in Human-Robot Interaction (HRI), due to their intrinsic capabilities in providing ever more intuitive and effective control modalities. Teleoperation systems require to handle a non-negligible amount of information coming from on-board sensors as well as input devices, thus increasing the workload of remote users. This paper presents the design of a 3D User Interface (3DUI) for the control of teleoperated robotic platforms aimed at increasing the interaction efficiency. A hand gesture driven controller is used as input modality to naturally map the position and gestures of the user\u2019s hand to suitable commands for controlling the platform components. The designed interface leverages on mixed reality to provide a visual feedback to the control commands issued by the user. The visualization of the 3DUI is superimposed to the video stream provided by an on-board camera. A user study confirmed that the proposed solution is able to improve the interaction efficiency by significantly reducing the completion time for tasks assigned in a remote reach-and-pick scenario.","paperId":"3a2db3a3d78de64c83d8f99881ea06a8c83cb411","paperTitle":"Mixed Reality-Based User Interaction Feedback for a Hand-Controlled Interface Targeted to Robot Teleoperation","paperUrl":"https://www.semanticscholar.org/paper/3a2db3a3d78de64c83d8f99881ea06a8c83cb411","images":[],"pdf":null},{"type":"incollection","key":"young2011mixed","title":"What Is Mixed Reality, Anyway? Considering the Boundaries of Mixed Reality in the Context of Robots","author":"Young, James and Sharlin, Ehud and Igarashi, Takeo","booktitle":"Mixed Reality and Human-Robot Interaction","pages":"1--11","year":"2011","publisher":"Springer","authors":["James Young","Ehud Sharlin","Takeo Igarashi"],"doi":"10.1007/978-94-007-0582-1_1","venue":"","abstract":"Mixed reality, as an approach in human-computer interaction, is often implicitly tied to particular implementation techniques (e.g., see-through device) and modalities (e.g., visual, graphical displays). In this paper we attempt to clarify the definition of mixed reality as a more abstract concept of combining the real and virtual worlds \u2013 that is, mixed reality is not a given technology but a concept that considers how the virtual and real worlds can be combined. Further, we use this discussion to posit robots as mixed-reality devices, and present a set of implications and questions for what this implies for mixed-reality interaction with robots.","paperId":"a204e215f3ce5b7ea755740e5ab8ef6f8f6aada5","paperTitle":"What Is Mixed Reality, Anyway? Considering the Boundaries of Mixed Reality in the Context of Robots","paperUrl":"https://www.semanticscholar.org/paper/a204e215f3ce5b7ea755740e5ab8ef6f8f6aada5","images":["https://d3i71xaburhd42.cloudfront.net/a204e215f3ce5b7ea755740e5ab8ef6f8f6aada5/4-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/a204e215f3ce5b7ea755740e5ab8ef6f8f6aada5/5-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/a204e215f3ce5b7ea755740e5ab8ef6f8f6aada5/6-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/a204e215f3ce5b7ea755740e5ab8ef6f8f6aada5/7-Figure4-1.png"],"pdf":"http://utouch.cpsc.ucalgary.ca/docs/WhatIsMixedRealityAnyway-SIS2011-JY.pdf"},{"type":"incollection","key":"dragone2009mixed","title":"Mixed Reality Agent (MiRA) Chameleons","author":"Dragone, Mauro and Holz, Thomas and O\u2019Hare, GMP and O\u2019Grady, Michael J","booktitle":"Agent-Based Ubiquitous Computing","pages":"13--33","year":"2009","publisher":"Springer","authors":["Mauro Dragone","Thomas Holz","GMP O\u2019Hare","Michael J O\u2019Grady"],"doi":"10.2991/978-94-91216-31-2_2","venue":"","abstract":"Human-Robot Interaction poses significant research challenges. Recent research suggests that rsonalisation and individualisation are key factors for establishing lifelong human-robot relationships. This raises difficulties as roboticists seek to incorporate robots into the digital society where an creasing amount of human activities relies on digital technologies and ubiquitous infrastructures. In essence, a robot may be perceived as either an embedded or mobile artefact in an arbitrary environment that must be interacted with in a seamless and intuitive fashion. This chapter explores some of the ternative ways and the design ssues to achieve these objectives. Specifically, it describes a new system, which we call Mixed Reality Agent (MiRA) Chameleon, that combines the latest advancements on agent-based ubiquitous architectures with mixed reality technology to deliver personalised and ubiquitous robot agents.","paperId":"47bfea2a7d934ff3fb17db44d1f9d2440b2405e9","paperTitle":"Mixed Reality Agent (MiRA) Chameleons","paperUrl":"https://www.semanticscholar.org/paper/47bfea2a7d934ff3fb17db44d1f9d2440b2405e9","images":[],"pdf":null},{"type":"inproceedings","key":"chen2006object","title":"Object Detection for a Mobile Robot Using Mixed Reality","author":"Chen, Hua and Wulf, Oliver and Wagner, Bernardo","booktitle":"International Conference on Virtual Systems and Multimedia","pages":"466--475","year":"2006","organization":"Springer","authors":["Hua Chen","Oliver Wulf","Bernardo Wagner"],"doi":"10.1007/11890881_51","dblp":"conf/vsmm/ChenWW06","venue":"VSMM","abstract":"This paper describes a novel Human-Robot Interface (HRI) that uses a Mixed Reality (MR) space to enhance and visualize object detection for mobile robot navigation. The MR space combines the 3D virtual model of a mobile robot and its navigating environment with the real data such as physical building measurement, the real-time acquired robot\u2019s position and laser scanned points. The huge amount of laser scanned points are rapidly segmented as belonging either to the background (i.e. fixed building) or newly appeared objects by comparing them with the 3D virtual model. This segmentation result can not only accelerate the object detection process but also facilitate the further process of object recognition with significant reduction of redundant sensor data. Such a MR space can also help human operators realizing effective surveillance through real-time visualization of the object detection results. It can be applied in a variety of mobile robot applications in a known environment. Experimental results verify the validity and feasibility of the proposed approach.","paperId":"3223924d134a561ebabf9a702ce6621db5195bbd","paperTitle":"Object Detection for a Mobile Robot Using Mixed Reality","paperUrl":"https://www.semanticscholar.org/paper/3223924d134a561ebabf9a702ce6621db5195bbd","images":[],"pdf":null},{"type":"article","key":"stewart2021study","title":"Study on Augmented Reality for Robotic Surgery Bedside Assistants","author":"Stewart, Camille Linick and Fong, Abigail and Payyavula, Govinda and DiMaio, Simon and Lafaro, Kelly and Tallmon, Kirsten and Wren, Sherry and Sorger, Jonathan and Fong, Yuman","journal":"Journal of Robotic Surgery","pages":"1--8","year":"2021","publisher":"Springer","authors":["Camille Linick Stewart","Abigail Fong","Govinda Payyavula","Simon DiMaio","Kelly Lafaro","Kirsten Tallmon","Sherry Wren","Jonathan Sorger","Yuman Fong"],"doi":"10.1007/s11701-021-01335-z","venue":"Journal of Robotic Surgery","abstract":"Robotic surgery bedside assistants play an important role in robotic procedures by performing intra-corporeal tasks while accommodating the physical presence of the robot. We hypothesized that an augmented reality headset enabling 3D intra-corporeal vision while facing the surgical field could decrease time and improve accuracy of robotic bedside tasks. Bedside assistants (one physician assistant, one medical student, three surgical trainees, and two attending surgeons) performed validated tasks within a mock abdominal cavity with a surgical robot docked. Tasks were performed with a bedside monitor providing 2D or 3D vision, or an optical see-through head-mounted augmented reality device with 2D or 3D vision. The effect of augmented reality device resolution on performance was also evaluated. For the simplest task of touching a straw, performance was generally high, regardless of mode of visualization. With more complex tasks, including stapling and pulling a ring along a path, 3D augmented reality decreased time and number of errors per task. 3D augmented reality allowed the physician assistant to perform at the level of an attending surgeon using 3D augmented reality (p\u2009=\u20090.08). All participants had improved times for the ring path task with better resolution (lower resolution 23\u2009\xb1\u200911 s vs higher resolution 14\u2009\xb1\u20094 s, p\u2009=\u20090.002). 3D augmented reality vision with high resolution decreased time and improved accuracy of more complex tasks, enabling a less experienced robotic surgical bedside assistant to function similar to attending surgeons. These data warrant further study with additional complex tasks and bedside assistants at various levels of training.","paperId":"68cb798c3315abe1a52043dbfd121d6f5f913d08","paperTitle":"Study on augmented reality for robotic surgery bedside assistants","paperUrl":"https://www.semanticscholar.org/paper/68cb798c3315abe1a52043dbfd121d6f5f913d08","images":["https://d3i71xaburhd42.cloudfront.net/68cb798c3315abe1a52043dbfd121d6f5f913d08/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/68cb798c3315abe1a52043dbfd121d6f5f913d08/6-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/68cb798c3315abe1a52043dbfd121d6f5f913d08/6-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/68cb798c3315abe1a52043dbfd121d6f5f913d08/6-Table3-1.png"],"pdf":"https://www.ncbi.nlm.nih.gov/pubmed/34762249"},{"type":"article","key":"ji2021closed","title":"A Closed-Loop Brain-Computer Interface with Augmented Reality Feedback for Industrial Human-Robot Collaboration","author":"Ji, Zhenrui and Liu, Quan and Xu, Wenjun and Yao, Bitao and Liu, Jiayi and Zhou, Zude","year":"2021","authors":["Zhenrui Ji","Quan Liu","Wenjun Xu","Bitao Yao","Jiayi Liu","Zude Zhou"],"doi":"10.21203/RS.3.RS-283263/V1","venue":"","abstract":"\\n Industrial human-robot collaboration (HRC) aims to combine the human intelligence and robotic capability to achieve the higher productiveness. In industrial HRC, the communication between human and robot is essential to enhance the understanding the intent of each other to make a more fluent collaboration. Brain-computer interface (BCI) is a technology that could record the user\u2019s brain activity that can be translated into interaction messages (e.g., control commands) to the outside world, which is able to build a direct and efficient communication channel between human and robot. However, due to lacking of information feedback mechanism, it is challenging for BCI to control robots with high degree of freedom with the limited number of classifiable mental state. To address this problem, this paper proposes a close-loop BCI with contextual visual feedback by an augmented reality (AR) headset. In such BCI, the electroencephalogram (EEG) patterns from the multiple voluntary eye blinks are considered as the input and the its online detection algorithm is proposed whose average accuracy can reach 94.31%. Moreover, an AR-enable information feedback interface is designed which enable to achieve an interactive robotic path planning. A case study of an industrial HRC assembly task is also develop which shows that the proposed closed-up BCI could shorten the time of user input in human-robot interaction.","paperId":"f414d895a917bfad8a9e6c2c10e469cc9da07883","paperTitle":"A Closed-Loop Brain-Computer Interface with Augmented Reality Feedback for Industrial Human-Robot Collaboration","paperUrl":"https://www.semanticscholar.org/paper/f414d895a917bfad8a9e6c2c10e469cc9da07883","images":[],"pdf":"https://www.researchsquare.com/article/rs-283263/v1.pdf?c=1617071139000"},{"type":"article","key":"laranjeira20203d","title":"3D Perception and Augmented Reality Developments in Underwater Robotics for Ocean Sciences","author":"Laranjeira, Matheus and Arnaubec, Aur\'elien and Brignone, Lorenzo and Dune, Claire and Opderbecke, Jan","journal":"Current Robotics Reports","pages":"1--8","year":"2020","publisher":"Springer","authors":["Matheus Laranjeira","Aur\'elien Arnaubec","Lorenzo Brignone","Claire Dune","Jan Opderbecke"],"doi":"10.1007/s43154-020-00014-5","venue":"","abstract":"This paper addresses the benefits and challenges of mixed reality (MR) for the exploration of deep-sea environments with remotely operated vehicles. The approach is twofold: virtual reality (VR) let the scientist explore the environment via a visual 3D model, overcoming limitations of local perception. Augmented reality (AR) concepts are designed in order to improve environment perception and interaction. The key to such concepts is the implementation of 3D visual geo-referenced terrain models from the imaging feedback gathered by the vehicle exploring its unknown surroundings. Image processing, underwater vehicle navigation, and user-friendly displays for robotic intervention are addressed in an integrated concept. A broad development programme carried out at the French Institute for Ocean Science, IFREMER, is described and illustrates technical topics and use cases. 3D perception derived from camera vision is shown to enable AR concepts that will significantly improve remote exploration and intervention in unknown natural environments. Cumulative geo-referenced 3D model building is in the process of being taken to reliable functioning in real-world underwater applications, accomplishing a milestone change in the capacity to view and understand the obscure and inaccessible deep-sea world.","paperId":"1b9c19c65e5472846e83b13d4cc625732a4fa363","paperTitle":"3D Perception and Augmented Reality Developments in Underwater Robotics for Ocean Sciences","paperUrl":"https://www.semanticscholar.org/paper/1b9c19c65e5472846e83b13d4cc625732a4fa363","images":[],"pdf":"https://link.springer.com/content/pdf/10.1007/s43154-020-00014-5.pdf"},{"type":"article","key":"chen2020optimization","title":"Optimization of Virtual and Real Registration Technology Based on Augmented Reality in a Surgical Navigation System","author":"Chen, Long and Zhang, Fengfeng and Zhan, Wei and Gan, Minfeng and Sun, Lining","journal":"Biomedical engineering online","volume":"19","number":"1","pages":"1--28","year":"2020","publisher":"BioMed Central","authors":["Long Chen","Fengfeng Zhang","Wei Zhan","Minfeng Gan","Lining Sun"],"doi":"10.1186/s12938-019-0745-z","venue":"Biomedical engineering online","abstract":"BackgroundThe traditional navigation interface was intended only for two-dimensional observation by doctors; thus, this interface does not display the total spatial information for the lesion area. Surgical navigation systems have become essential tools that enable for doctors to accurately and safely perform complex operations. The image navigation interface is separated from the operating area, and the doctor needs to switch the field of vision between the screen and the patient\u2019s lesion area. In this paper, augmented reality (AR) technology was applied to spinal surgery to provide more intuitive information to surgeons. The accuracy of virtual and real registration was improved via research on AR technology. During the operation, the doctor could observe the AR image and the true shape of the internal spine through the skin.MethodsTo improve the accuracy of virtual and real registration, a virtual and real registration technique based on an improved identification method and robot-assisted method was proposed. The experimental method was optimized by using the improved identification method. X-ray images were used to verify the effectiveness of the puncture performed by the robot.ResultsThe final experimental results show that the average accuracy of the virtual and real registration based on the general identification method was 9.73\u2009\xb1\u20090.46\xa0mm (range 8.90\u201310.23\xa0mm). The average accuracy of the virtual and real registration based on the improved identification method was 3.54\u2009\xb1\u20090.13\xa0mm (range 3.36\u20133.73\xa0mm). Compared with the virtual and real registration based on the general identification method, the accuracy was improved by approximately 65%. The highest accuracy of the virtual and real registration based on the robot-assisted method was 2.39\xa0mm. The accuracy was improved by approximately 28.5% based on the improved identification method.ConclusionThe experimental results show that the two optimized methods are highly very effective. The proposed AR navigation system has high accuracy and stability. This system may have value in future spinal surgeries.","paperId":"33b70249a74c9b8820fcf1931aa40f303deb1d18","paperTitle":"Optimization of virtual and real registration technology based on augmented reality in a surgical navigation system","paperUrl":"https://www.semanticscholar.org/paper/33b70249a74c9b8820fcf1931aa40f303deb1d18","images":["https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/4-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/8-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/9-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/9-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/7-Table4-1.png","https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/16-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/10-Table5-1.png","https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/17-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/11-Table6-1.png","https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/17-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/18-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/21-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/21-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/22-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/22-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/23-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/23-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/24-Figure14-1.png","https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/25-Figure15-1.png","https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/25-Figure16-1.png","https://d3i71xaburhd42.cloudfront.net/5ff0a0130657500791080982b9cb598556211411/26-Figure17-1.png"],"pdf":"https://biomedical-engineering-online.biomedcentral.com/track/pdf/10.1186/s12938-019-0745-z"},{"type":"inproceedings","key":"montalvo2020industrial","title":"Industrial Control Robot Based on Augmented Reality and IoT Protocol","author":"Montalvo, William and Bonilla-Vasconez, Pablo and Altamirano, Santiago and Garcia, Carlos A and Garcia, Marcelo V","booktitle":"International Conference on Augmented Reality, Virtual Reality and Computer Graphics","pages":"345--363","year":"2020","organization":"Springer","authors":["William Montalvo","Pablo Bonilla-Vasconez","Santiago Altamirano","Carlos A Garcia","Marcelo V Garcia"],"doi":"10.1007/978-3-030-58468-9_25","dblp":"conf/avr/MontalvoBAGG20","venue":"AVR","abstract":"The use of robotic systems into production processes has resulted in the reduction of manufacturing costs and an improvement in product quality, However, as technologies advance, complex and high-cost control systems are required, hence the need to create low-cost and efficient control systems, with low complexity when operating them. For these reasons, the research presents a control system for the Scorbot ER 4U manipulator arm using a low-cost embedded board and augmented reality platform that brings to the users a real experience with virtual add-ons, thus facilitating the training and management of robotic equipment. A virtual control proposed in Unity 3D allowing the application to be much more interactive and very easy for the user. For the validation of the system, operating tests were carried out, resulting in movements with great precision and in a very simple way without having greater complexity when indicating positions to which the manipulator must reach.","paperId":"bf0310e51d69eac45007c2f7e155a99295eabf3e","paperTitle":"Industrial Control Robot Based on Augmented Reality and IoT Protocol","paperUrl":"https://www.semanticscholar.org/paper/bf0310e51d69eac45007c2f7e155a99295eabf3e","images":[],"pdf":null},{"type":"article","key":"bardosi2020ciguide","title":"CIGuide: In Situ Augmented Reality Laser Guidance","author":"B\'ardosi, Zolt\'an and Plattner, Christian and \\"Ozbek, Yusuf and Hofmann, Thomas and Milosavljevic, Srdjan and Schartinger, Volker and Freysinger, Wolfgang","journal":"International journal of computer assisted radiology and surgery","volume":"15","number":"1","pages":"49--57","year":"2020","publisher":"Springer","authors":["Zolt\'an B\'ardosi","Christian Plattner","Yusuf \\"Ozbek","Thomas Hofmann","Srdjan Milosavljevic","Volker Schartinger","Wolfgang Freysinger"],"doi":"10.1007/s11548-019-02066-1","dblp":"journals/cars/BardosiPOHMSF20","venue":"International Journal of Computer Assisted Radiology and Surgery","abstract":"Purpose\xa0 A robotic intraoperative laser guidance system with hybrid optic-magnetic tracking for skull base surgery is presented. It provides in situ augmented reality guidance for microscopic interventions at the lateral skull base with minimal mental and workload overhead on surgeons working without a monitor and dedicated pointing tools. Methods\xa0 Three components were developed: a registration tool (Rhinospider), a hybrid magneto-optic-tracked robotic feedback control scheme and a modified robotic end-effector. Rhinospider optimizes registration of patient and preoperative CT data by excluding user errors in fiducial localization with magnetic tracking. The hybrid controller uses an integrated microscope HD camera for robotic control with a guidance beam shining on a dual plate setup avoiding magnetic field distortions. A robotic needle insertion platform (iSYS Medizintechnik GmbH, Austria) was modified to position a laser beam with high precision in a surgical scene compatible to microscopic surgery. Results\xa0 System accuracy was evaluated quantitatively at various target positions on a phantom. The accuracy found is 1.2 mm \xb1 0.5 mm. Errors are primarily due to magnetic tracking. This application accuracy seems suitable for most surgical procedures in the lateral skull base. The system was evaluated quantitatively during a mastoidectomy of an anatomic head specimen and was judged useful by the surgeon. Conclusion\xa0 A hybrid robotic laser guidance system with direct visual feedback is proposed for navigated drilling and intraoperative structure localization. The system provides visual cues directly on/in the patient anatomy, reducing the standard limitations of AR visualizations like depth perception. The custom- built end-effector for the iSYS robot is transparent to using surgical microscopes and compatible with magnetic tracking. The cadaver experiment showed that guidance was accurate and that the end-effector is unobtrusive. This laser guidance has potential to aid the surgeon in finding the optimal mastoidectomy trajectory in more difficult interventions.","paperId":"f71f77c24c5b13ab5601624fee8d633c5cb23e21","paperTitle":"CIGuide: in situ augmented reality laser guidance","paperUrl":"https://www.semanticscholar.org/paper/f71f77c24c5b13ab5601624fee8d633c5cb23e21","images":["https://d3i71xaburhd42.cloudfront.net/3d7abbdbbf012783fca7f64de4e01e495148a332/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/3d7abbdbbf012783fca7f64de4e01e495148a332/7-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/3d7abbdbbf012783fca7f64de4e01e495148a332/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/3d7abbdbbf012783fca7f64de4e01e495148a332/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/3d7abbdbbf012783fca7f64de4e01e495148a332/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/3d7abbdbbf012783fca7f64de4e01e495148a332/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/3d7abbdbbf012783fca7f64de4e01e495148a332/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/3d7abbdbbf012783fca7f64de4e01e495148a332/6-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/3d7abbdbbf012783fca7f64de4e01e495148a332/7-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/3d7abbdbbf012783fca7f64de4e01e495148a332/7-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/3d7abbdbbf012783fca7f64de4e01e495148a332/7-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/3d7abbdbbf012783fca7f64de4e01e495148a332/8-Figure12-1.png"],"pdf":"https://link.springer.com/content/pdf/10.1007/s11548-019-02066-1.pdf"},{"type":"inproceedings","key":"phillips2020robotic","title":"A Robotic Augmented Reality Virtual Window for Law Enforcement Operations","author":"Phillips, Nate and Kruse, Brady and Khan, Farzana Alam and Swan II, J Edward and Bethel, Cindy L","booktitle":"International Conference on Human-Computer Interaction","pages":"591--610","year":"2020","organization":"Springer","authors":["Nate Phillips","Brady Kruse","Farzana Alam Khan","J Edward Swan II","Cindy L Bethel"],"doi":"10.1007/978-3-030-49695-1_40","dblp":"conf/hci/PhillipsKKSB20","venue":"HCI","abstract":"In room-clearing tasks, SWAT team members suffer from a lack of initial environmental information: knowledge about what is in a room and what relevance or threat level it represents for mission parameters. Normally this gap in situation awareness is rectified only upon room entry, forcing SWAT team members to rely on quick responses and near-instinctual reactions. This can lead to dangerously escalating situations or important missed information which, in turn, can increase the likelihood of injury and even mortality. Thus, we present an x-ray vision system for the dynamic scanning and display of room content, using a robotic platform to mitigate operator risk. This system maps a room using a robot-equipped stereo depth camera and, using an augmented reality (AR) system, presents the resulting geographic information according to the perspective of each officer. This intervention has the potential to notably lower risk and increase officer situation awareness, all while team members are in the relative safety of cover. With these potential stakes, it is important to test the viability of this system natively and in an operational SWAT team context.","paperId":"4bde1ec60fe1c252add9a340aa58805e291984bd","paperTitle":"A Robotic Augmented Reality Virtual Window for Law Enforcement Operations","paperUrl":"https://www.semanticscholar.org/paper/4bde1ec60fe1c252add9a340aa58805e291984bd","images":[],"pdf":null},{"type":"inproceedings","key":"papcun2019augmented","title":"Augmented Reality for Humans-Robots Interaction in Dynamic Slotting \\"Chaotic Storage\\" Smart Warehouses","author":"Papcun, Peter and Cabadaj, Jan and Kajati, Erik and Romero, David and Landryova, Lenka and Vascak, Jan and Zolotova, Iveta","booktitle":"IFIP International Conference on Advances in Production Management Systems","pages":"633--641","year":"2019","organization":"Springer","authors":["Peter Papcun","Jan Cabadaj","Erik Kajati","David Romero","Lenka Landryova","Jan Vascak","Iveta Zolotova"],"doi":"10.1007/978-3-030-30000-5_77","dblp":"conf/ifip5-7/PapcunCKRLVZ19","venue":"APMS","abstract":"Nowadays, smart warehouses mostly use Automated Guided Vehicles (AGVs) controlled through magnetic or painted paths. This approach is suitable for \u201cstatic slotting\u201d warehouses, and for places where humans do not cross paths with mobile robots. Therefore, fixed-path AGVs are not an optimal solution for dynamic slotting \u201cchaotic storage\u201d warehouses, where picking and delivery paths are often changing. Hence, it is important to create an environment where AGVs have planned their path, and storekeepers can see their paths, and mark restricted areas by virtual means if needed, for these mobile robots and humans to move and stand safely around a smart warehouse. In this paper, we have proposed an Augmented Reality (AR) environment for storekeepers, where they can see an AGV planned path, and they can add virtual obstacles and walls to the mobile robots\u2019 cyber-physical navigation view. These virtual obstacles and walls can be used to determine restricted areas for mobile robots, which can be seen for example as safe areas for humans\u2019 and/or robots\u2019 stationary work. Finally, we introduce the system architecture supporting the proposed AR environment for humans-mobile robots safe and productive interaction.","paperId":"653550c1fa6920cd516560f0a675904b133602f3","paperTitle":"Augmented Reality for Humans-Robots Interaction in Dynamic Slotting \\"Chaotic Storage\\" Smart Warehouses","paperUrl":"https://www.semanticscholar.org/paper/653550c1fa6920cd516560f0a675904b133602f3","images":[],"pdf":null},{"type":"inproceedings","key":"ren2019augmented","title":"Augmented Reality Based Actuated Monitor Manipulation from Dual Point of View","author":"Ren, Ying and Tanaka, Jiro","booktitle":"International Conference on Human-Computer Interaction","pages":"93--107","year":"2019","organization":"Springer","authors":["Ying Ren","Jiro Tanaka"],"doi":"10.1007/978-3-030-21565-1_7","dblp":"conf/hci/RenT19","venue":"HCI","abstract":"The mobile robots are increasingly used in domestic space for room surveillance. However, joystick based controller dose not realize direct and intuitive motion control for monitor. To solve this problem, we propose an augmented reality based interface to control monitor from dual point of view, referring to the third-person view and rear first-person view. In this system, augmented reality models used to represent monitor are superimposed on the dual point of view, which enables users to control each part of actuated monitor intuitively with interaction of augmented reality models on screen. Through augmented reality models, our system realized the concept of user-centered manipulation since the control of target object is described in user\u2019s coordination system. In addition, we carried out a preliminary user study to evaluate our design and the performance of the system, and a positive feedback has been received.","paperId":"5bc29bd042021004291a71c6e005a55ea45ebca6","paperTitle":"Augmented Reality Based Actuated Monitor Manipulation from Dual Point of View","paperUrl":"https://www.semanticscholar.org/paper/5bc29bd042021004291a71c6e005a55ea45ebca6","images":[],"pdf":null},{"type":"article","key":"zhou2017robot","title":"Robot-Assisted Surgery for Mandibular Angle Split Osteotomy Using Augmented Reality: Preliminary Results on Clinical Animal Experiment","author":"Zhou, Chaozheng and Zhu, Ming and Shi, Yunyong and Lin, Li and Chai, Gang and Zhang, Yan and Xie, Le","journal":"Aesthetic plastic surgery","volume":"41","number":"5","pages":"1228--1236","year":"2017","publisher":"Springer","authors":["Chaozheng Zhou","Ming Zhu","Yunyong Shi","Li Lin","Gang Chai","Yan Zhang","Le Xie"],"doi":"10.1007/s00266-017-0900-5","venue":"Aesthetic Plastic Surgery","abstract":"Abstract\\nMandibular angle split osteotomy (MASO) is a procedure widely used for prominent mandibular angles. However, conventional mandibular plastic surgery is invasive and high risk. It may induce postoperative neurosensory disturbance of the inferior alveolar nerve, fractures and infection due to the complexity of the anatomical structure and the narrow surgical field of view. The success rate of MASO surgery usually depends on the clinical experience and skills of the surgeon. To evaluate the performance of inexperienced plastic surgeons conducting this surgery, a self-developed and constructed robot system based on augmented reality is used. This robot system provides for sufficient accuracy and safety within the clinical environment. To evaluate the accuracy and safety of MASO surgery, an animal study using this robot was performed in the clinical room, and the results were then evaluated. Four osteotomy planes were successfully performed on two dogs; that is, twenty tunnels (each dog drilled on bilaterally) were drilled in the dogs\u2019 mandible bones. Errors at entrance and target points were 1.04\xa0\xb1\xa00.19 and 1.22\xa0\xb1\xa00.24\xa0mm, respectively. The angular error between the planned and drilled tunnels was 6.69\xb0\xa0\xb1\xa01.05\xb0. None of the dogs experienced severe complications. Therefore, this technique can be regarded as a useful approach for training inexperienced plastic surgeons on the various aspects of plastic surgery.No Level Assigned This journal requires that authors assign a level of evidence to each article. For a full description of these Evidence-Based Medicine ratings, please refer to the Table of Contents or the online Instructions to Authors www.springer.com/00266.","paperId":"71a7a53f7328e7ad1ad20da953168212df467453","paperTitle":"Robot-Assisted Surgery for Mandibular Angle Split Osteotomy Using Augmented Reality: Preliminary Results on Clinical Animal Experiment","paperUrl":"https://www.semanticscholar.org/paper/71a7a53f7328e7ad1ad20da953168212df467453","images":["https://d3i71xaburhd42.cloudfront.net/71a7a53f7328e7ad1ad20da953168212df467453/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/71a7a53f7328e7ad1ad20da953168212df467453/6-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/71a7a53f7328e7ad1ad20da953168212df467453/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/71a7a53f7328e7ad1ad20da953168212df467453/7-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/71a7a53f7328e7ad1ad20da953168212df467453/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/71a7a53f7328e7ad1ad20da953168212df467453/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/71a7a53f7328e7ad1ad20da953168212df467453/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/71a7a53f7328e7ad1ad20da953168212df467453/7-Figure7-1.png"],"pdf":"https://www.ncbi.nlm.nih.gov/pubmed/28725963"},{"type":"article","key":"zeng2017surgical","title":"A Surgical Robot with Augmented Reality Visualization for Stereoelectroencephalography Electrode Implantation","author":"Zeng, Bowei and Meng, Fanle and Ding, Hui and Wang, Guangzhi","journal":"International journal of computer assisted radiology and surgery","volume":"12","number":"8","pages":"1355--1368","year":"2017","publisher":"Springer","authors":["Bowei Zeng","Fanle Meng","Hui Ding","Guangzhi Wang"],"doi":"10.1007/s11548-017-1634-1","dblp":"journals/cars/ZengMDW17","venue":"International Journal of Computer Assisted Radiology and Surgery","abstract":"PurposeUsing existing stereoelectroencephalography (SEEG) electrode implantation surgical robot systems, it is difficult to intuitively validate registration accuracy and display the electrode entry points (EPs) and the anatomical structure around the electrode trajectories in the patient space to the surgeon. This paper proposes a prototype system that can realize video see-through augmented reality (VAR) and spatial augmented reality (SAR) for SEEG implantation. The system helps the surgeon quickly and intuitively confirm the registration accuracy, locate EPs and visualize the internal anatomical structure in the image space and patient space.MethodsWe designed and developed a projector-camera system (PCS) attached to the distal flange of a robot arm. First, system calibration is performed. Second, the PCS is used to obtain the point clouds of the surface of the patient\u2019s head, which are utilized for patient-to-image registration. Finally, VAR is produced by merging the real-time video of the patient and the preoperative three-dimensional (3D) operational planning model. In addition, SAR is implemented by projecting the planning electrode trajectories and local anatomical structure onto the patient\u2019s scalp.ResultsThe error of registration, the electrode EPs and the target points are evaluated on a phantom. The fiducial registration error is $$0.25 \\\\pm 0.23$$0.25\xb10.23 mm (max 1.22 mm), and the target registration error is $$0.62\\\\pm 0.28$$0.62\xb10.28 mm (max 1.18 mm). The projection overlay error is $$0.75\\\\pm 0.52$$0.75\xb10.52 mm, and the TP error after the pre-warped projection is $$0.82\\\\pm 0.23$$0.82\xb10.23 mm. The TP error caused by a surgeon\u2019s viewpoint deviation is also evaluated.ConclusionThe presented system can help surgeons quickly verify registration accuracy during SEEG procedures and can provide accurate EP locations and internal structural information to the surgeon. With more intuitive surgical information, the surgeon may have more confidence and be able to perform surgeries with better outcomes.","paperId":"366701cd6763d728fa40b106307e3e3db284931d","paperTitle":"A surgical robot with augmented reality visualization for stereoelectroencephalography electrode implantation","paperUrl":"https://www.semanticscholar.org/paper/366701cd6763d728fa40b106307e3e3db284931d","images":["https://d3i71xaburhd42.cloudfront.net/366701cd6763d728fa40b106307e3e3db284931d/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/366701cd6763d728fa40b106307e3e3db284931d/7-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/366701cd6763d728fa40b106307e3e3db284931d/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/366701cd6763d728fa40b106307e3e3db284931d/8-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/366701cd6763d728fa40b106307e3e3db284931d/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/366701cd6763d728fa40b106307e3e3db284931d/10-Table3-1.png","https://d3i71xaburhd42.cloudfront.net/366701cd6763d728fa40b106307e3e3db284931d/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/366701cd6763d728fa40b106307e3e3db284931d/10-Table4-1.png","https://d3i71xaburhd42.cloudfront.net/366701cd6763d728fa40b106307e3e3db284931d/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/366701cd6763d728fa40b106307e3e3db284931d/11-Table5-1.png","https://d3i71xaburhd42.cloudfront.net/366701cd6763d728fa40b106307e3e3db284931d/7-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/366701cd6763d728fa40b106307e3e3db284931d/8-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/366701cd6763d728fa40b106307e3e3db284931d/8-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/366701cd6763d728fa40b106307e3e3db284931d/9-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/366701cd6763d728fa40b106307e3e3db284931d/10-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/366701cd6763d728fa40b106307e3e3db284931d/10-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/366701cd6763d728fa40b106307e3e3db284931d/11-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/366701cd6763d728fa40b106307e3e3db284931d/12-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/366701cd6763d728fa40b106307e3e3db284931d/12-Figure14-1.png"],"pdf":"https://www.ncbi.nlm.nih.gov/pubmed/28664416"},{"type":"inproceedings","key":"sengiku2017augmented","title":"Augmented Reality Navigation System for Robot-Assisted Laparoscopic Partial Nephrectomy","author":"Sengiku, Atsushi and Koeda, Masanao and Sawada, Atsuro and Kono, Jin and Terada, Naoki and Yamasaki, Toshinari and Mizushino, Kiminori and Kunii, Takahiro and Onishi, Katsuhiko and Noborio, Hiroshi and others","booktitle":"International Conference of Design, User Experience, and Usability","pages":"575--584","year":"2017","organization":"Springer","authors":["Atsushi Sengiku","Masanao Koeda","Atsuro Sawada","Jin Kono","Naoki Terada","Toshinari Yamasaki","Kiminori Mizushino","Takahiro Kunii","Katsuhiko Onishi","Hiroshi Noborio","undefined others"],"doi":"10.1007/978-3-319-58637-3_45","dblp":"conf/hci/SengikuKSKTYMKO17","venue":"HCI","abstract":"We have developed a surgical navigation system for robot-assisted laparoscopic partial nephrectomy (RALPN). In this system, a three-dimensional computer graphics (3DCG) model generated from each patient\u2019s computed tomography image is overlaid on the endoscopic image, and we control it manually to navigate the vascular structure and tumor location in real time. The position and orientation of the 3DCG model is calculated from the optical flow of the endoscopic camera images, which enables the model to move semi-automatically. We conducted 20 navigations for RALPN from April 2014 to December 2016. Our support system worked appropriately and was helpful to localize the tumor and determine the resection line.","paperId":"de0f255e43f5b459743e7d3a87cad780b726d5e8","paperTitle":"Augmented Reality Navigation System for Robot-Assisted Laparoscopic Partial Nephrectomy","paperUrl":"https://www.semanticscholar.org/paper/de0f255e43f5b459743e7d3a87cad780b726d5e8","images":[],"pdf":null},{"type":"inproceedings","key":"ruffaldi2016third","title":"Third Point of View Augmented Reality for Robot Intentions Visualization","author":"Ruffaldi, Emanuele and Brizzi, Filippo and Tecchia, Franco and Bacinelli, Sandro","booktitle":"International Conference on Augmented Reality, Virtual Reality and Computer Graphics","pages":"471--478","year":"2016","organization":"Springer","authors":["Emanuele Ruffaldi","Filippo Brizzi","Franco Tecchia","Sandro Bacinelli"],"doi":"10.1007/978-3-319-40621-3_35","dblp":"conf/avr/RuffaldiBTB16","venue":"AVR","abstract":"Lightweight, head-up displays integrated in industrial helmets allow to provide contextual information for industrial scenarios such as in maintenance. Moving from single display and single camera solutions to stereo perception and display opens new interaction possibilities. In particular this paper addresses the case of information sharing by a Baxter robot displayed to the user overlooking at the real scene. System design and interaction ideas are being presented.","paperId":"518be9d87f329aeeb7833d412b66b655e718c83d","paperTitle":"Third Point of View Augmented Reality for Robot Intentions Visualization","paperUrl":"https://www.semanticscholar.org/paper/518be9d87f329aeeb7833d412b66b655e718c83d","images":["https://d3i71xaburhd42.cloudfront.net/ed0378db187153a47683655bb3b5aecde25c4a01/4-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/ed0378db187153a47683655bb3b5aecde25c4a01/5-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/ed0378db187153a47683655bb3b5aecde25c4a01/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/ed0378db187153a47683655bb3b5aecde25c4a01/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/ed0378db187153a47683655bb3b5aecde25c4a01/7-Figure5-1.png"],"pdf":"http://www.eruffaldi.com/papers/2016_C_RuffaldiAVRTAUM.pdf"},{"type":"inproceedings","key":"edgcumbe2016augmented","title":"Augmented Reality Imaging for Robot-Assisted Partial Nephrectomy Surgery","author":"Edgcumbe, Philip and Singla, Rohit and Pratt, Philip and Schneider, Caitlin and Nguan, Christopher and Rohling, Robert","booktitle":"International Conference on Medical Imaging and Augmented Reality","pages":"139--150","year":"2016","organization":"Springer","authors":["Philip Edgcumbe","Rohit Singla","Philip Pratt","Caitlin Schneider","Christopher Nguan","Robert Rohling"],"doi":"10.1007/978-3-319-43775-0_13","dblp":"conf/miar/EdgcumbeSPSNR16","venue":"MIAR","abstract":"Laparoscopic partial nephrectomy (LPN) is a standard of care for small kidney cancer tumours. A successful LPN is the complete excision of the kidney tumour while preserving as much of the non-cancerous kidney as possible. This is a challenging procedure because the surgeon has a limited field of view and reduced or no haptic feedback while performing delicate excisions as fast as possible. This work introduces and evaluates a novel surgical navigation marker called the Dynamic Augmented Reality Tracker (DART). The DART is used in a novel intra-operative augmented reality ultrasound navigation system (ARUNS) for robot-assisted minimally invasive surgery to overcome some of these challenges. The DART is inserted into a kidney and the DART and pick-up laparoscopic ultrasound transducer are tracked during an intra-operative freehand ultrasound scan of the tumour. After ultrasound, the system continues to track the DART and display the segmented 3D tumour and location of surgical instruments relative to the tumour throughout the surgery. The ultrasound point reconstruction root mean squared error (RMSE) was 0.9 mm, the RMSE of tracking the da Vinci surgical instruments was 1.5 mm and the total system RMSE, which includes ultrasound imaging and da Vinci kinematic instrument tracking, was 5.1 mm. The system was evaluated by an expert surgeon who used the DART and ARUNS to excise a tumour from a kidney phantom. This work serves as a preliminary evaluation in anticipation of further refinement and validation in vivo.","paperId":"98bdc4025f03543851acf2975b34b6a23706d1da","paperTitle":"Augmented Reality Imaging for Robot-Assisted Partial Nephrectomy Surgery","paperUrl":"https://www.semanticscholar.org/paper/98bdc4025f03543851acf2975b34b6a23706d1da","images":[],"pdf":null},{"type":"article","key":"liu2015augmented","title":"Augmented Reality and Cone Beam CT Guidance for Transoral Robotic Surgery","author":"Liu, Wen P and Richmon, Jeremy D and Sorger, Jonathan M and Azizian, Mahdi and Taylor, Russell H","journal":"Journal of robotic surgery","volume":"9","number":"3","pages":"223--233","year":"2015","publisher":"Springer","authors":["Wen P Liu","Jeremy D Richmon","Jonathan M Sorger","Mahdi Azizian","Russell H Taylor"],"doi":"10.1007/s11701-015-0520-5","venue":"Journal of Robotic Surgery","abstract":"In transoral robotic surgery preoperative image data do not reflect large deformations of the operative workspace from perioperative setup. To address this challenge, in this study we explore image guidance with cone beam computed tomographic angiography to guide the dissection of critical vascular landmarks and resection of base-of-tongue neoplasms with adequate margins for transoral robotic surgery. We identify critical vascular landmarks from perioperative c-arm imaging to augment the stereoscopic view of a da Vinci si robot in addition to incorporating visual feedback from relative tool positions. Experiments resecting base-of-tongue mock tumors were conducted on a series of ex vivo and in vivo animal models comparing the proposed workflow for video augmentation to standard non-augmented practice and alternative, fluoroscopy-based image guidance. Accurate identification of registered augmented critical anatomy during controlled arterial dissection and en bloc mock tumor resection was possible with the augmented reality system. The proposed image-guided robotic system also achieved improved resection ratios of mock tumor margins (1.00) when compared to control scenarios (0.0) and alternative methods of image guidance (0.58). The experimental results show the feasibility of the proposed workflow and advantages of cone beam computed tomography image guidance through video augmentation of the primary stereo endoscopy as compared to control and alternative navigation methods.","paperId":"6fca5dd2e7e15bc528135d0986dfbbe68330d08c","paperTitle":"Augmented reality and cone beam CT guidance for transoral robotic surgery","paperUrl":"https://www.semanticscholar.org/paper/6fca5dd2e7e15bc528135d0986dfbbe68330d08c","images":["https://d3i71xaburhd42.cloudfront.net/6fca5dd2e7e15bc528135d0986dfbbe68330d08c/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/6fca5dd2e7e15bc528135d0986dfbbe68330d08c/6-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/6fca5dd2e7e15bc528135d0986dfbbe68330d08c/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/6fca5dd2e7e15bc528135d0986dfbbe68330d08c/7-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/6fca5dd2e7e15bc528135d0986dfbbe68330d08c/7-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/6fca5dd2e7e15bc528135d0986dfbbe68330d08c/8-Figure4-1.png"],"pdf":"https://europepmc.org/articles/pmc4634572?pdf=render"},{"type":"inproceedings","key":"gianni2013augmented","title":"ARE: Augmented Reality Environment for Mobile Robots","author":"Gianni, Mario and Ferri, Federico and Pirri, Fiora","booktitle":"Conference Towards Autonomous Robotic Systems","pages":"470--483","year":"2013","organization":"Springer","authors":["Mario Gianni","Federico Ferri","Fiora Pirri"],"doi":"10.1007/978-3-662-43645-5_48","dblp":"conf/taros/GianniFP13","venue":"TAROS","abstract":"In this paper we present ARE, an Augmented Reality Environment, with the main purpose of providing cognitive robotics modelers with a development tool for constructing, at real-time, complex planning scenarios for robots, eliminating the need to model the dynamics of both the robot and the real environment as it would be required by whole simulation environments. The framework also builds a world model representation that serves as ground truth for training and validating algorithms for vision, motion planning and control. We demonstrate the application of the AR-based framework for evaluating the capability of the robot to plan safe paths to goal locations in real outdoor scenarios, while the planning scene dynamically changes, being augmented by virtual objects.","paperId":"51a7249af2bcfe60edfe23e1f3b263bc9d0ef6ec","paperTitle":"ARE: Augmented Reality Environment for Mobile Robots","paperUrl":"https://www.semanticscholar.org/paper/51a7249af2bcfe60edfe23e1f3b263bc9d0ef6ec","images":["https://d3i71xaburhd42.cloudfront.net/51a7249af2bcfe60edfe23e1f3b263bc9d0ef6ec/1-Figure5-1.png"],"pdf":"http://www.dis.uniroma1.it/~alcor/site/documents/TAROS2013_Gianni_Mario.pdf"},{"type":"inproceedings","key":"zalud2014color","title":"Color and Thermal Image Fusion for Augmented Reality in Rescue Robotics","author":"Zalud, Ludek and Kocmanova, Petra and Burian, Frantisek and Jilek, Tomas","booktitle":"The 8th International Conference on Robotic, Vision, Signal Processing & Power Applications","pages":"47--55","year":"2014","organization":"Springer","authors":["Ludek Zalud","Petra Kocmanova","Frantisek Burian","Tomas Jilek"],"doi":"10.1007/978-981-4585-42-2_6","venue":"","abstract":"At the beginning of this article, the authors address the main problems of todays remotely-operated reconnaissance robots. The reconnaissance robots Orpheus-AC, Orpheus-AC2 and Orpheus-Explorer, made in the Department of Control and Instrumentation (DCI), are then shortly described. Since all the described robotic systems use visual telepresence as the main control technique, visual information from the robots surroundings is essential for the operator. For this reason, the authors make a fusion of data from a Charge-Coupled Device (CCD) color camera, and a thermovision camera to provide the operator with data in all visibility conditions, such as complete darkness, fog, smoke, etc.","paperId":"663d9a40bbfeccef560974e9409a762420d9f559","paperTitle":"Color and Thermal Image Fusion for Augmented Reality in Rescue Robotics","paperUrl":"https://www.semanticscholar.org/paper/663d9a40bbfeccef560974e9409a762420d9f559","images":[],"pdf":null},{"type":"article","key":"martin2020design","title":"Design of a Hyper-Redundant Robot and Teleoperation Using Mixed Reality for Inspection Tasks","author":"Mart\'in-Barrio, Andr\'es and Rold\'an-G\'omez, Juan Jes\'us and Rodr\'iguez, Iv\'an and Del Cerro, Jaime and Barrientos, Antonio","journal":"Sensors","volume":"20","number":"8","pages":"2181","year":"2020","publisher":"Multidisciplinary Digital Publishing Institute","authors":["Andr\'es Mart\'in-Barrio","Juan Jes\'us Rold\'an-G\'omez","Iv\'an Rodr\'iguez","Jaime Del Cerro","Antonio Barrientos"],"doi":"10.3390/s20082181","dblp":"journals/sensors/Martin-BarrioRR20","venue":"Sensors","abstract":"Hyper-redundant robots are highly articulated devices that present numerous technical challenges such as their design, control or remote operation. However, they offer superior kinematic skills than traditional robots for multiple applications. This work proposes an original and custom-made design for a discrete and hyper-redundant manipulator. It is comprised of 7 sections actuated by cables and 14 degrees of freedom. It has been optimized to be very robust, accurate and capable of moving payloads with high dexterity. Furthermore, it has been efficiently controlled from the actuators to high-level strategies based on the management of its shape. However, these highly articulated systems often exhibit complex shapes that frustrate their spatial understanding. Immersive technologies emerge as a good solution to remotely and safely teleoperate the presented robot for an inspection task in a hazardous environment. Experimental results validate the proposed robot design and control strategies. As a result, it is concluded that hyper-redundant robots and immersive technologies should play an important role in the near future of automated and remote applications.","paperId":"b4790458b5830108bf4180d48c1e4b14749078b8","paperTitle":"Design of a Hyper-Redundant Robot and Teleoperation Using Mixed Reality for Inspection Tasks","paperUrl":"https://www.semanticscholar.org/paper/b4790458b5830108bf4180d48c1e4b14749078b8","images":["https://d3i71xaburhd42.cloudfront.net/68d37937e16eec324f475b06073de56199045bb9/4-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/68d37937e16eec324f475b06073de56199045bb9/5-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/68d37937e16eec324f475b06073de56199045bb9/6-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/68d37937e16eec324f475b06073de56199045bb9/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/68d37937e16eec324f475b06073de56199045bb9/7-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/68d37937e16eec324f475b06073de56199045bb9/7-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/68d37937e16eec324f475b06073de56199045bb9/8-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/68d37937e16eec324f475b06073de56199045bb9/9-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/68d37937e16eec324f475b06073de56199045bb9/9-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/68d37937e16eec324f475b06073de56199045bb9/10-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/68d37937e16eec324f475b06073de56199045bb9/10-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/68d37937e16eec324f475b06073de56199045bb9/12-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/68d37937e16eec324f475b06073de56199045bb9/13-Figure13-1.png"],"pdf":null},{"type":"article","key":"zhang2020collaborative","title":"Collaborative Robot and Mixed Reality Assisted Microgravity Assembly for Large Space Mechanism","author":"Zhang, Renjie and Liu, Xinyu and Shuai, Jiazhou and Zheng, Lianyu","journal":"Procedia Manufacturing","volume":"51","pages":"38--45","year":"2020","publisher":"Elsevier","authors":["Renjie Zhang","Xinyu Liu","Jiazhou Shuai","Lianyu Zheng"],"doi":"10.1016/j.promfg.2020.10.007","venue":"","abstract":null,"paperId":"0dbd4a68f34690d1c6d8a5dacd5af67fd0f2eaea","paperTitle":"Collaborative robot and mixed reality assisted microgravity assembly for large space mechanism","paperUrl":"https://www.semanticscholar.org/paper/0dbd4a68f34690d1c6d8a5dacd5af67fd0f2eaea","images":[],"pdf":null},{"type":"article","key":"sievers2020concept","title":"Concept of a Mixed-Reality Learning Environment for Collaborative Robotics","author":"Sievers, Torsten Sebastian and Schmitt, Bianca and R\\"uckert, Patrick and Petersen, Maren and Tracht, Kirsten","journal":"Procedia Manufacturing","volume":"45","pages":"19--24","year":"2020","publisher":"Elsevier","authors":["Torsten Sebastian Sievers","Bianca Schmitt","Patrick R\\"uckert","Maren Petersen","Kirsten Tracht"],"doi":"10.1016/j.promfg.2020.04.034","venue":"","abstract":null,"paperId":"e1204c19dbceeb4124e29d7a1a13966d22b3f2e8","paperTitle":"Concept of a Mixed-Reality Learning Environment for Collaborative Robotics","paperUrl":"https://www.semanticscholar.org/paper/e1204c19dbceeb4124e29d7a1a13966d22b3f2e8","images":[],"pdf":null},{"type":"article","key":"ostanin2019interactive","title":"Interactive Robots Control Using Mixed Reality","author":"Ostanin, M and Yagfarov, R and Klimchik, A","journal":"IFAC-PapersOnLine","volume":"52","number":"13","pages":"695--700","year":"2019","publisher":"Elsevier","authors":["M Ostanin","R Yagfarov","A Klimchik"],"doi":"10.1016/j.ifacol.2019.11.307","venue":"IFAC-PapersOnLine","abstract":null,"paperId":"cd6ffc75b9aa26684f8949b297aa3c46f339bf47","paperTitle":"Interactive Robots Control Using Mixed Reality","paperUrl":"https://www.semanticscholar.org/paper/cd6ffc75b9aa26684f8949b297aa3c46f339bf47","images":[],"pdf":null},{"type":"article","key":"coovert2014spatial","title":"Spatial Augmented Reality as a Method for a Mobile Robot to Communicate Intended Movement","author":"Coovert, Michael D and Lee, Tiffany and Shindev, Ivan and Sun, Yu","journal":"Computers in Human Behavior","volume":"34","pages":"241--248","year":"2014","publisher":"Elsevier","authors":["Michael D Coovert","Tiffany Lee","Ivan Shindev","Yu Sun"],"doi":"10.1016/j.chb.2014.02.001","dblp":"journals/chb/CoovertLSS14","venue":"Comput. Hum. Behav.","abstract":null,"paperId":"ba6ae11c48b48ae9829f9edd985ec6f8bc26df88","paperTitle":"Spatial augmented reality as a method for a mobile robot to communicate intended movement","paperUrl":"https://www.semanticscholar.org/paper/ba6ae11c48b48ae9829f9edd985ec6f8bc26df88","images":["https://d3i71xaburhd42.cloudfront.net/ba6ae11c48b48ae9829f9edd985ec6f8bc26df88/4-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/ba6ae11c48b48ae9829f9edd985ec6f8bc26df88/6-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/ba6ae11c48b48ae9829f9edd985ec6f8bc26df88/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/ba6ae11c48b48ae9829f9edd985ec6f8bc26df88/7-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/ba6ae11c48b48ae9829f9edd985ec6f8bc26df88/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/ba6ae11c48b48ae9829f9edd985ec6f8bc26df88/6-Figure4-1.png"],"pdf":null},{"type":"article","key":"sauer2007potential","title":"Potential and Challenges of Stereo Augmented Reality for Mobile Robot Teleoperation","author":"Sauer, Markus and Driewer, Frauke and G\\"ollnitz, Manuel and Schilling, Klaus","journal":"IFAC Proceedings Volumes","volume":"40","number":"16","pages":"183--188","year":"2007","publisher":"Elsevier","authors":["Markus Sauer","Frauke Driewer","Manuel G\\"ollnitz","Klaus Schilling"],"doi":"10.3182/20070904-3-KR-2922.00032","dblp":"conf/ifachms/SauerDGS07","venue":"IFAC HMS","abstract":"Abstract In many mobile robot applications a human operator is still required to control the robot. The design of the user interface is the major element affecting the performance and efficiency of the task execution in the teleoperation scenario. This work investigates an approach to visualize all information gathered by the mobile robot in an integrated fashion by applying stereo augmented reality. Initial experiences and results with this type of interface for navigation tasks are presented.","paperId":"eaf491071de383fdf25163670aa3e85e1ee0efbe","paperTitle":"Potential and challenges of stereo augmented reality for mobile robot teleoperation","paperUrl":"https://www.semanticscholar.org/paper/eaf491071de383fdf25163670aa3e85e1ee0efbe","images":[],"pdf":null},{"type":"article","key":"xiang2021mobile","title":"Mobile Projective Augmented Reality for Collaborative Robots in Construction","author":"Xiang, Siyuan and Wang, Ruoyu and Feng, Chen","journal":"Automation in Construction","volume":"127","pages":"103704","year":"2021","publisher":"Elsevier","authors":["Siyuan Xiang","Ruoyu Wang","Chen Feng"],"doi":"10.1016/J.AUTCON.2021.103704","venue":"Automation in Construction","abstract":null,"paperId":"c7bcaedbe40079d425ce77848cc9dab774d3f3f6","paperTitle":"Mobile projective augmented reality for collaborative robots in construction","paperUrl":"https://www.semanticscholar.org/paper/c7bcaedbe40079d425ce77848cc9dab774d3f3f6","images":[],"pdf":null},{"type":"article","key":"makris2016augmented","title":"Augmented Reality System for Operator Support in Human--Robot Collaborative Assembly","author":"Makris, Sotiris and Karagiannis, Panagiotis and Koukas, Spyridon and Matthaiakis, Aleksandros-Stereos","journal":"CIRP Annals","volume":"65","number":"1","pages":"61--64","year":"2016","publisher":"Elsevier","authors":["Sotiris Makris","Panagiotis Karagiannis","Spyridon Koukas","Aleksandros-Stereos Matthaiakis"],"doi":"10.1016/J.CIRP.2016.04.038","venue":"","abstract":null,"paperId":"de4056209d00e74d93e46e1e2e8d30c1e5685cc0","paperTitle":"Augmented reality system for operator support in human\u2013robot collaborative assembly","paperUrl":"https://www.semanticscholar.org/paper/de4056209d00e74d93e46e1e2e8d30c1e5685cc0","images":[],"pdf":null},{"type":"article","key":"danielsson2017assessing","title":"Assessing Instructions in Augmented Reality for Human-Robot Collaborative Assembly by Using Demonstrators","author":"Danielsson, Oscar and Syberfeldt, Anna and Brewster, Rodney and Wang, Lihui","journal":"Procedia CIRP","volume":"63","pages":"89--94","year":"2017","publisher":"Elsevier","authors":["Oscar Danielsson","Anna Syberfeldt","Rodney Brewster","Lihui Wang"],"doi":"10.1016/J.PROCIR.2017.02.038","venue":"","abstract":null,"paperId":"a6d33719d3b3f2417238dc1067c378e67d3718d1","paperTitle":"Assessing Instructions in Augmented Reality for Human-robot Collaborative Assembly by Using Demonstrators","paperUrl":"https://www.semanticscholar.org/paper/a6d33719d3b3f2417238dc1067c378e67d3718d1","images":[],"pdf":"https://doi.org/10.1016/j.procir.2017.02.038"},{"type":"article","key":"lambrecht2021towards","title":"Towards Commissioning, Resilience and Added Value of Augmented Reality in Robotics: Overcoming Technical Obstacles to Industrial Applicability","author":"Lambrecht, Jens and K\\"astner, Linh and Guhl, Jan and Kr\\"uger, J\\"org","journal":"Robotics and Computer-Integrated Manufacturing","volume":"71","pages":"102178","year":"2021","publisher":"Elsevier","authors":["Jens Lambrecht","Linh K\\"astner","Jan Guhl","J\\"org Kr\\"uger"],"doi":"10.1016/J.RCIM.2021.102178","dblp":"journals/rcim/LambrechtKGK21","venue":"Robotics Comput. Integr. Manuf.","abstract":null,"paperId":"07f2a0f311dc5e08b11f87f224fd1799444a1f73","paperTitle":"Towards commissioning, resilience and added value of Augmented Reality in robotics: Overcoming technical obstacles to industrial applicability","paperUrl":"https://www.semanticscholar.org/paper/07f2a0f311dc5e08b11f87f224fd1799444a1f73","images":[],"pdf":"https://doi.org/10.1016/j.rcim.2021.102178"},{"type":"article","key":"lazna2018visualization","title":"The Visualization of Threats Using the Augmented Reality and a Remotely Controlled Robot","author":"Lazna, Tomas","journal":"IFAC-PapersOnLine","volume":"51","number":"6","pages":"444--449","year":"2018","publisher":"Elsevier","authors":["Tomas Lazna"],"doi":"10.1016/J.IFACOL.2018.07.113","venue":"","abstract":null,"paperId":"0e8a5ea6391736e82e4e1fa78f855c4bcf193a37","paperTitle":"The Visualization of Threats Using the Augmented Reality and a Remotely Controlled Robot","paperUrl":"https://www.semanticscholar.org/paper/0e8a5ea6391736e82e4e1fa78f855c4bcf193a37","images":["https://d3i71xaburhd42.cloudfront.net/0e8a5ea6391736e82e4e1fa78f855c4bcf193a37/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/0e8a5ea6391736e82e4e1fa78f855c4bcf193a37/2-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/0e8a5ea6391736e82e4e1fa78f855c4bcf193a37/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/0e8a5ea6391736e82e4e1fa78f855c4bcf193a37/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/0e8a5ea6391736e82e4e1fa78f855c4bcf193a37/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/0e8a5ea6391736e82e4e1fa78f855c4bcf193a37/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/0e8a5ea6391736e82e4e1fa78f855c4bcf193a37/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/0e8a5ea6391736e82e4e1fa78f855c4bcf193a37/5-Figure7-1.png"],"pdf":"https://doi.org/10.1016/j.ifacol.2018.07.113"},{"type":"article","key":"fang2012interactive","title":"Interactive Robot Trajectory Planning and Simulation Using Augmented Reality","author":"Fang, HC and Ong, SK and Nee, AYC","journal":"Robotics and Computer-Integrated Manufacturing","volume":"28","number":"2","pages":"227--237","year":"2012","publisher":"Elsevier","authors":["HC Fang","SK Ong","AYC Nee"],"doi":"10.1016/J.RCIM.2011.09.003","venue":"","abstract":null,"paperId":"e54982d0c97383223132e6c142890e1489e44870","paperTitle":"Interactive robot trajectory planning and simulation using Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/e54982d0c97383223132e6c142890e1489e44870","images":[],"pdf":null},{"type":"article","key":"ong2020augmented","title":"Augmented Reality-Assisted Robot Programming System for Industrial Applications","author":"Ong, SK and Yew, AWW and Thanigaivel, NK and Nee, AYC","journal":"Robotics and Computer-Integrated Manufacturing","volume":"61","pages":"101820","year":"2020","publisher":"Elsevier","authors":["SK Ong","AWW Yew","NK Thanigaivel","AYC Nee"],"doi":"10.1016/J.RCIM.2019.101820","dblp":"journals/rcim/OngYTN20","venue":"Robotics Comput. Integr. Manuf.","abstract":null,"paperId":"cdc9659495d6e0cfd803fe1fa23dad832d035057","paperTitle":"Augmented reality-assisted robot programming system for industrial applications","paperUrl":"https://www.semanticscholar.org/paper/cdc9659495d6e0cfd803fe1fa23dad832d035057","images":[],"pdf":"https://doi.org/10.1016/j.rcim.2019.101820"},{"type":"article","key":"iqbal2021augmented","title":"Augmented Reality in Robotic Assisted Orthopaedic Surgery: A Pilot Study","author":"Iqbal, Hisham and Tatti, Fabio and y Baena, Ferdinando Rodriguez","journal":"Journal of Biomedical Informatics","volume":"120","pages":"103841","year":"2021","publisher":"Elsevier","authors":["Hisham Iqbal","Fabio Tatti","Ferdinando Rodriguez y Baena"],"doi":"10.1016/j.jbi.2021.103841","dblp":"journals/jbi/IqbalTB21","venue":"J. Biomed. Informatics","abstract":null,"paperId":"1939d7454d34b82cf8dd9a6e5b189046da11c4e6","paperTitle":"Augmented reality in robotic assisted orthopaedic surgery: A pilot study","paperUrl":"https://www.semanticscholar.org/paper/1939d7454d34b82cf8dd9a6e5b189046da11c4e6","images":[],"pdf":"https://doi.org/10.1016/j.jbi.2021.103841"},{"type":"article","key":"leutert2015augmented","title":"Augmented Reality for Telemaintenance and-Inspection in Force-Sensitive Industrial Robot Applications","author":"Leutert, Florian and Schilling, Klaus","journal":"IFAC-PapersOnLine","volume":"48","number":"10","pages":"153--158","year":"2015","publisher":"Elsevier","authors":["Florian Leutert","Klaus Schilling"],"doi":"10.1016/J.IFACOL.2015.08.124","venue":"","abstract":null,"paperId":"8bef7d32f63d3f39c8f4325748a00d411e880ac5","paperTitle":"Augmented Reality for Telemaintenance and -inspection in Force-Sensitive Industrial Robot Applications","paperUrl":"https://www.semanticscholar.org/paper/8bef7d32f63d3f39c8f4325748a00d411e880ac5","images":["https://d3i71xaburhd42.cloudfront.net/8bef7d32f63d3f39c8f4325748a00d411e880ac5/2-Figure1-1.png"],"pdf":"https://doi.org/10.1016/j.ifacol.2015.08.124"},{"type":"article","key":"michalos2016augmented","title":"Augmented Reality (AR) Applications for Supporting Human-Robot Interactive Cooperation","author":"Michalos, George and Karagiannis, Panagiotis and Makris, Sotiris and Tok\xe7alar, \\"Onder and Chryssolouris, George","journal":"Procedia CIRP","volume":"41","pages":"370--375","year":"2016","publisher":"Elsevier","authors":["George Michalos","Panagiotis Karagiannis","Sotiris Makris","\\"Onder Tok\xe7alar","George Chryssolouris"],"doi":"10.1016/J.PROCIR.2015.12.005","venue":"","abstract":null,"paperId":"92e3c614111db94ede77621d6bd90e959600da62","paperTitle":"Augmented Reality (AR) Applications for Supporting Human-robot Interactive Cooperation","paperUrl":"https://www.semanticscholar.org/paper/92e3c614111db94ede77621d6bd90e959600da62","images":[],"pdf":"https://doi.org/10.1016/j.procir.2015.12.005"},{"type":"article","key":"jiang2021adjacent","title":"Adjacent Surface Trajectory Planning of Robot-Assisted Tooth Preparation Based on Augmented Reality","author":"Jiang, Jingang and Guo, Yafeng and Huang, Zhiyuan and Zhang, Yongde and Wu, Dianhao and Liu, Yi","journal":"Engineering Science and Technology, an International Journal","year":"2021","publisher":"Elsevier","authors":["Jingang Jiang","Yafeng Guo","Zhiyuan Huang","Yongde Zhang","Dianhao Wu","Yi Liu"],"doi":"10.1016/J.JESTCH.2021.05.005","venue":"","abstract":null,"paperId":"e1901b6bddd25aa15f37462972c417a7eb13e4dd","paperTitle":"Adjacent surface trajectory planning of robot-assisted tooth preparation based on augmented reality","paperUrl":"https://www.semanticscholar.org/paper/e1901b6bddd25aa15f37462972c417a7eb13e4dd","images":[],"pdf":"https://doi.org/10.1016/j.jestch.2021.05.005"},{"type":"inproceedings","key":"coste2003planning","title":"Planning, Simulation, and Augmented Reality for Robotic Cardiac Procedures: The STARS System of the ChIR Team","author":"Coste-Mani`ere, `Eve and Adhami, Loua\\"i and Mourgues, Fabien and Carpentier, Alain","booktitle":"Seminars in thoracic and cardiovascular surgery","volume":"15","number":"2","pages":"141--156","year":"2003","organization":"Elsevier","authors":["`Eve Coste-Mani`ere","Loua\\"i Adhami","Fabien Mourgues","Alain Carpentier"],"doi":"10.1016/S1043-0679(03)70022-7","venue":"Seminars in thoracic and cardiovascular surgery","abstract":null,"paperId":"26e18b6ffb032e3181bb640b92d2d527e32eab05","paperTitle":"Planning, simulation, and augmented reality for robotic cardiac procedures: The STARS system of the ChIR team.","paperUrl":"https://www.semanticscholar.org/paper/26e18b6ffb032e3181bb640b92d2d527e32eab05","images":[],"pdf":"https://doi.org/10.1016/S1043-0679%2803%2970022-7"},{"type":"article","key":"green2008collaborating","title":"Collaborating with a Mobile Robot: An Augmented Reality Multimodal Interface","author":"Green, Scott A and Chen, Xioa Qi and Billinghurst, Mark and Chase, J Geoffrey","journal":"IFAC Proceedings Volumes","volume":"41","number":"2","pages":"15595--15600","year":"2008","publisher":"Elsevier","authors":["Scott A Green","Xioa Qi Chen","Mark Billinghurst","J Geoffrey Chase"],"doi":"10.3182/20080706-5-KR-1001.02637","venue":"","abstract":"Abstract We have created an infrastructure that allows a human to collaborate in a natural manner with a robotic system. In this paper we describe our system and its implementation with a mobile robot. In our prototype the human communicates with the mobile robot using natural speech and gestures, for example, by selecting a point in 3D space and saying \u201cgo here\u201d or \u201cgo behind that\u201d. The robot responds using speech so the human is able to understand its intentions and beliefs. Augmented Reality (AR) technology is used to facilitate natural use of gestures and provide a common 3D spatial reference for both the robot and human, thus providing a means for grounding of communication and maintaining spatial awareness. This paper first discusses related work then gives a brief overview of AR and its capabilities. The architectural design we have developed is outlined and then a case study is discussed.","paperId":"ff49f61e06571ebf4f02e9d3b517befb7a12123a","paperTitle":"Collaborating with a Mobile Robot: An Augmented Reality Multimodal Interface","paperUrl":"https://www.semanticscholar.org/paper/ff49f61e06571ebf4f02e9d3b517befb7a12123a","images":["https://d3i71xaburhd42.cloudfront.net/ff49f61e06571ebf4f02e9d3b517befb7a12123a/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/ff49f61e06571ebf4f02e9d3b517befb7a12123a/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/ff49f61e06571ebf4f02e9d3b517befb7a12123a/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/ff49f61e06571ebf4f02e9d3b517befb7a12123a/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/ff49f61e06571ebf4f02e9d3b517befb7a12123a/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/ff49f61e06571ebf4f02e9d3b517befb7a12123a/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/ff49f61e06571ebf4f02e9d3b517befb7a12123a/5-Figure7-1.png"],"pdf":"http://ir.canterbury.ac.nz/bitstream/10092/2211/1/12612078_Green IFAC 08 Collaborating with a Mobile Robot_FINAL.pdf"},{"type":"inproceedings","key":"tran2021robot","title":"Robot-Generated Mixed Reality Gestures Improve Human-Robot Interaction","author":"Tran, Nhan and Grant, Trevor and Phung, Thao and Hirshfield, Leanne and Wickens, Christopher and Williams, Tom","booktitle":"International Conference on Social Robotics","pages":"768--773","year":"2021","organization":"Springer","authors":["Nhan Tran","Trevor Grant","Thao Phung","Leanne Hirshfield","Christopher Wickens","Tom Williams"],"doi":"10.1007/978-3-030-90525-5_69","dblp":"conf/socrob/TranGPHWW21","venue":"ICSR","abstract":null,"paperId":"1fda6e515c7eb163d0bfadaed51b3047cb094b80","paperTitle":"Robot-Generated Mixed Reality Gestures Improve Human-Robot Interaction","paperUrl":"https://www.semanticscholar.org/paper/1fda6e515c7eb163d0bfadaed51b3047cb094b80","images":[],"pdf":null},{"type":"inproceedings","key":"mahajan2020adapting","title":"Adapting Usability Metrics for a Socially Assistive, Kinesthetic, Mixed Reality Robot Tutoring Environment","author":"Mahajan, Kartik and Groechel, Thomas and Pakkar, Roxanna and Cordero, Julia and Lee, Haemin and Matari\'c, Maja J","booktitle":"International Conference on Social Robotics","pages":"381--391","year":"2020","organization":"Springer","authors":["Kartik Mahajan","Thomas Groechel","Roxanna Pakkar","Julia Cordero","Haemin Lee","Maja J Matari\'c"],"doi":"10.1007/978-3-030-62056-1_32","dblp":"conf/socrob/MahajanGPCLM20","venue":"ICSR","abstract":"The field of Socially Assistive Robot (SAR) tutoring has extensively explored both subjective and objective usability metrics for seated tablet-based human-robot interactions. As SAR tutoring introduces kinesthetic mixed reality environments where students can move around and physically manipulate virtual objects, usability metrics for such interactions need to be re-evaluated. This paper applies standard usability metrics from seated 2D interactions to a kinesthetic mixed reality environment and validates those metrics with post-interaction survey data. Using data from a pilot study (n = 9) conducted with a mixed reality SAR tutor, three commonly-used metrics of usability for seated 2D tutoring interfaces were collected: performance, manipulation time, and gaze. The strength of each usability metric was compared to subjective survey-based scores measured with the System Usability Scale (SUS). The results show that usability scores were correlated with the gaze metric but not with the manipulation time or performance metrics. The findings provide interesting implications for the design and evaluation of kinesthetic mixed reality robot tutoring environments.","paperId":"c688d8fd19ab932c28ca281260766bab6d45fd7c","paperTitle":"Adapting Usability Metrics for a Socially Assistive, Kinesthetic, Mixed Reality Robot Tutoring Environment","paperUrl":"https://www.semanticscholar.org/paper/c688d8fd19ab932c28ca281260766bab6d45fd7c","images":["https://d3i71xaburhd42.cloudfront.net/c688d8fd19ab932c28ca281260766bab6d45fd7c/4-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/c688d8fd19ab932c28ca281260766bab6d45fd7c/6-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/c688d8fd19ab932c28ca281260766bab6d45fd7c/7-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/c688d8fd19ab932c28ca281260766bab6d45fd7c/8-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/c688d8fd19ab932c28ca281260766bab6d45fd7c/8-Figure5-1.png"],"pdf":"http://robotics.usc.edu/publications/downloads/pub/1107/"},{"type":"article","key":"selecky2019analysis","title":"Analysis of Using Mixed Reality Simulations for Incremental Development of Multi-Uav Systems","author":"Seleck`y, Martin and Faigl, Jan and Rollo, Milan","journal":"Journal of Intelligent & Robotic Systems","volume":"95","number":"1","pages":"211--227","year":"2019","publisher":"Springer","authors":["Martin Seleck`y","Jan Faigl","Milan Rollo"],"doi":"10.1007/S10846-018-0875-8","dblp":"journals/jirs/SeleckyFR19","venue":"J. Intell. Robotic Syst.","abstract":"Developing complex robotic systems requires expensive and time-consuming verification and testing which, especially in a case of multi-robot unmanned aerial systems (UASs), aggregates risk of hardware failures and may pose legal issues in experiments where operating more than one unmanned aircraft simultaneously is required. Thus, it is highly favorable to find and resolve most of the eventual design flaws and system bugs in a simulation, where their impacts are significantly lower. On the other hand, as the system development process approaches the final stages, the fidelity of the simulation needs to rise. However, since some phenomena that can significantly influence the system behavior are difficult to be modeled precisely, a partial embodiment of the simulation in the physical world is necessary. In this paper, we present a method for incremental development of complex unmanned aerial systems with the help of mixed reality simulations. The presented methodology is accompanied with a cost analysis to further show its benefits. The generality and versatility of the method is demonstrated in three practical use cases of various aviation systems development: (i) an unmanned system consisting of heterogeneous team of autonomous unmanned aircraft; (ii) a system for verification of collision avoidance methods among fixed wing unmanned aerial vehicles; and (iii) a system for planning collision-free paths for light-sport aircraft.","paperId":"45b0ce97dd125f59566b706341fc5184bc57b967","paperTitle":"Analysis of Using Mixed Reality Simulations for Incremental Development of Multi-UAV Systems","paperUrl":"https://www.semanticscholar.org/paper/45b0ce97dd125f59566b706341fc5184bc57b967","images":["https://d3i71xaburhd42.cloudfront.net/45b0ce97dd125f59566b706341fc5184bc57b967/2-Figure1-1.png"],"pdf":"https://doi.org/10.1007/s10846-018-0875-8"},{"type":"inproceedings","key":"kato2019robot","title":"A Robot System Using Mixed Reality to Encourage Driving Review","author":"Kato, Yuta and Aikawa, Yuya and Kanoh, Masayoshi and Jimenez, Felix and Hayase, Mitsuhiro and Tanaka, Takahiro and Kanamori, Hitoshi","booktitle":"International Conference on Human-Computer Interaction","pages":"112--117","year":"2019","organization":"Springer","authors":["Yuta Kato","Yuya Aikawa","Masayoshi Kanoh","Felix Jimenez","Mitsuhiro Hayase","Takahiro Tanaka","Hitoshi Kanamori"],"doi":"10.1007/978-3-030-23528-4_16","dblp":"conf/hci/KatoAKJHTK19","venue":"HCI","abstract":"This paper proposes a robot system for driving review by using mixed reality that presents driving videos. By using mixed reality, the users can see the robot and the videos within the same field of view. The users therefore can review their own driving in an environment similar to a lecture at a driving school. Comparative experiments were performed with three groups; a group in which mixed reality was used to display driving situations (proposed system), a group in which tablet terminals were used, and a group in which only a robot was used (no video). The results show that using the proposed system for driving review may increase attachment to the robot.","paperId":"94fa015fd2df6f48edc8b108557ba02b0743eedc","paperTitle":"A Robot System Using Mixed Reality to Encourage Driving Review","paperUrl":"https://www.semanticscholar.org/paper/94fa015fd2df6f48edc8b108557ba02b0743eedc","images":[],"pdf":null},{"type":"inproceedings","key":"sibirtseva2019exploring","title":"Exploring Temporal Dependencies in Multimodal Referring Expressions with Mixed Reality","author":"Sibirtseva, Elena and Ghadirzadeh, Ali and Leite, Iolanda and Bj\\"orkman, Maarten and Kragic, Danica","booktitle":"International Conference on Human-Computer Interaction","pages":"108--123","year":"2019","organization":"Springer","authors":["Elena Sibirtseva","Ali Ghadirzadeh","Iolanda Leite","Maarten Bj\\"orkman","Danica Kragic"],"doi":"10.1007/978-3-030-21565-1_8","dblp":"journals/corr/abs-1902-01117","venue":"HCI","abstract":"In collaborative tasks, people rely both on verbal and non-verbal cues simultaneously to communicate with each other. For human-robot interaction to run smoothly and naturally, a robot should be equipped with the ability to robustly disambiguate referring expressions. In this work, we propose a model that can disambiguate multimodal fetching requests using modalities such as head movements, hand gestures, and speech. We analysed the acquired data from mixed reality experiments and formulated a hypothesis that modelling temporal dependencies of events in these three modalities increases the model\'s predictive power. We evaluated our model on a Bayesian framework to interpret referring expressions with and without exploiting a temporal prior.","paperId":"4e56fe01b092c588e29f02eff0f6c8694c7adb5f","paperTitle":"Exploring Temporal Dependencies in Multimodal Referring Expressions with Mixed Reality","paperUrl":"https://www.semanticscholar.org/paper/4e56fe01b092c588e29f02eff0f6c8694c7adb5f","images":["https://d3i71xaburhd42.cloudfront.net/4e56fe01b092c588e29f02eff0f6c8694c7adb5f/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/4e56fe01b092c588e29f02eff0f6c8694c7adb5f/13-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/4e56fe01b092c588e29f02eff0f6c8694c7adb5f/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/4e56fe01b092c588e29f02eff0f6c8694c7adb5f/13-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/4e56fe01b092c588e29f02eff0f6c8694c7adb5f/8-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/4e56fe01b092c588e29f02eff0f6c8694c7adb5f/9-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/4e56fe01b092c588e29f02eff0f6c8694c7adb5f/11-Figure5-1.png"],"pdf":null},{"type":"inproceedings","key":"chen2010analysing","title":"Analysing Mixed Reality Simulation for Industrial Applications: A Case Study in the Development of a Robotic Screw Remover System","author":"Chen, Ian Yen-Hung and MacDonald, Bruce and W\\"unsche, Burkhard and Biggs, Geoffrey and Kotoku, Tetsuo","booktitle":"International Conference on Simulation, Modeling, and Programming for Autonomous Robots","pages":"350--361","year":"2010","organization":"Springer","authors":["Ian Yen-Hung Chen","Bruce MacDonald","Burkhard W\\"unsche","Geoffrey Biggs","Tetsuo Kotoku"],"doi":"10.1007/978-3-642-17319-6_33","dblp":"conf/simpar/ChenMWBK10","venue":"SIMPAR","abstract":"A Mixed Reality (MR) simulation aims to enable robot developers to create safe and close-to-real world environments from a mixture of real and virtual components for experimenting with robot systems. However, the reliability of the simulation results and its usefulness in solving practical problems remain to be validated. This paper presents an evaluation of an MR simulator by examining its use for the development of a robotic screw remover system. Quantitative evaluation compares the robot\'s trajectories produced in our MR simulation with those from a real world experiment, yielding results that indicate the MR simulation reliably represents the real world. A user study was conducted and the results demonstrate that the MR simulator gives users a stronger confidence of accurate results in comparison to a virtual simulator.","paperId":"3e28781162d0bf0492bfcca5a54eea9fa6cb8f38","paperTitle":"Analysing Mixed Reality Simulation for Industrial Applications: A Case Study in the Development of a Robotic Screw Remover System","paperUrl":"https://www.semanticscholar.org/paper/3e28781162d0bf0492bfcca5a54eea9fa6cb8f38","images":["https://d3i71xaburhd42.cloudfront.net/3e28781162d0bf0492bfcca5a54eea9fa6cb8f38/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/3e28781162d0bf0492bfcca5a54eea9fa6cb8f38/7-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/3e28781162d0bf0492bfcca5a54eea9fa6cb8f38/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/3e28781162d0bf0492bfcca5a54eea9fa6cb8f38/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/3e28781162d0bf0492bfcca5a54eea9fa6cb8f38/7-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/3e28781162d0bf0492bfcca5a54eea9fa6cb8f38/10-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/3e28781162d0bf0492bfcca5a54eea9fa6cb8f38/11-Figure8-1.png"],"pdf":"http://www.cs.auckland.ac.nz/%7Eburkhard/Publications/SIMPAR2010_ChenEtAl.pdf"},{"type":"inproceedings","key":"arevalo2021exploring","title":"Exploring the Visual Space to Improve Depth Perception in Robot Teleoperation Using Augmented Reality: The Role of Distance and Target\'s Pose in Time, Success, and Certainty","author":"Ar\'evalo Arboleda, Stephanie and Dierks, Tim and R\\"ucker, Franziska and Gerken, Jens","booktitle":"IFIP Conference on Human-Computer Interaction","pages":"522--543","year":"2021","organization":"Springer","authors":["Stephanie Ar\'evalo Arboleda","Tim Dierks","Franziska R\\"ucker","Jens Gerken"],"doi":"10.1007/978-3-030-85623-6_31","dblp":"conf/interact/ArboledaDRG21","venue":"INTERACT","abstract":null,"paperId":"0379414edd5606c4fcd405e751a1ae9c2e5b9a89","paperTitle":"Exploring the Visual Space to Improve Depth Perception in Robot Teleoperation Using Augmented Reality: The Role of Distance and Target\'s Pose in Time, Success, and Certainty","paperUrl":"https://www.semanticscholar.org/paper/0379414edd5606c4fcd405e751a1ae9c2e5b9a89","images":[],"pdf":null},{"type":"article","key":"solanes2020teleoperation","title":"Teleoperation of Industrial Robot Manipulators Based on Augmented Reality","author":"Solanes, J Ernesto and Mu~noz, Adolfo and Gracia, Luis and Mart\'i, Ana and Girb\'es-Juan, Vicent and Tornero, Josep","journal":"The International Journal of Advanced Manufacturing Technology","volume":"111","number":"3","pages":"1077--1097","year":"2020","publisher":"Springer","authors":["J Ernesto Solanes","Adolfo Mu~noz","Luis Gracia","Ana Mart\'i","Vicent Girb\'es-Juan","Josep Tornero"],"doi":"10.1007/s00170-020-05997-1","venue":"","abstract":"This research develops a novel teleoperation for robot manipulators based on augmented reality. The proposed interface is equipped with full capabilities in order to replace the classical teach pendant of the robot for carrying out teleoperation tasks. The proposed interface is based on an augmented reality headset for projecting computer-generated graphics onto the real environment and a gamepad to interact with the computer-generated graphics and provide robot commands. In order to demonstrate the benefits of the proposed method, several usability tests were conducted using a 6R industrial robot manipulator in order to compare the proposed interface and the conventional teach pendant interface for teleoperation tasks. In particular, the results of these usability tests show that the proposed approach is more intuitive, ergonomic, and easy to use. Furthermore, the comparison results also show that the proposed method clearly improves the velocity of the teleoperation task, regardless of the user\u2019s previous experience in robotics and augmented reality technology.","paperId":"5437bf1242f9f9f9f83653978ceef57afda9774a","paperTitle":"Teleoperation of industrial robot manipulators based on augmented reality","paperUrl":"https://www.semanticscholar.org/paper/5437bf1242f9f9f9f83653978ceef57afda9774a","images":[],"pdf":"https://riunet.upv.es/bitstream/10251/162868/7/Solanes%3bMu%c3%b1oz%3bGracia%20-%20Teleoperation%20of%20industrial%20robot%20manipulators%20based%20on%20augmented%20reality.pdf"},{"type":"inproceedings","key":"wang2020construction","title":"Construction of Human-Robot Cooperation Assembly Simulation System Based on Augmented Reality","author":"Wang, Qiang and Fan, Xiumin and Luo, Mingyu and Yin, Xuyue and Zhu, Wenmin","booktitle":"International Conference on Human-Computer Interaction","pages":"629--642","year":"2020","organization":"Springer","authors":["Qiang Wang","Xiumin Fan","Mingyu Luo","Xuyue Yin","Wenmin Zhu"],"doi":"10.1007/978-3-030-49695-1_42","dblp":"conf/hci/WangFLYZ20","venue":"HCI","abstract":"Human-Robot cooperation (HRC) is the developing trend in the field of industrial assembly. Design and evaluation of the HRC assembly workstation considering the human factor is very important. In order to evaluate the transformational construction scenario of a manual assembly workstation to a HRC workstation fast and safely, a HRC assembly simulation system is constructed which is based on Augmented Reality (AR) with human-in-loop interaction. It enables a real operator to interact with virtual robot in a real scene, and the assembly steps of real workers can be restored and mapped to a virtual human model for further ergonomic analysis. Kinect and LeapMotion are used as the sensors for human-robot interaction decision and feedback. An automobile gearbox assembly is taken as an example for different assembly task verification, operators\u2019 data are collected and analyzed by RULA scores and NASA-TLX questionnaires. The result shows that the simulation system can be used for the human factor evaluation of different HRC task configuration schemes.","paperId":"f496f19fb20725d6f15965d20327586f0adf6f7f","paperTitle":"Construction of Human-Robot Cooperation Assembly Simulation System Based on Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/f496f19fb20725d6f15965d20327586f0adf6f7f","images":[],"pdf":null},{"type":"inproceedings","key":"reardon2020enabling","title":"Enabling Situational Awareness via Augmented Reality of Autonomous Robot-Based Environmental Change Detection","author":"Reardon, Christopher and Gregory, Jason and Nieto-Granda, Carlos and Rogers, John G","booktitle":"International Conference on Human-Computer Interaction","pages":"611--628","year":"2020","organization":"Springer","authors":["Christopher Reardon","Jason Gregory","Carlos Nieto-Granda","John G Rogers"],"doi":"10.1007/978-3-030-49695-1_41","dblp":"conf/hci/ReardonGNR20","venue":"HCI","abstract":"Accurately detecting changes in one\u2019s environment is an important ability for many application domains, but can be challenging for humans. Autonomous robots can easily be made to autonomously detect metric changes in the environment, but unlike humans, understanding context can be challenging for robots. We present a novel system that uses an autonomous robot performing point cloud-based change detection to facilitate information-gathering tasks and provides enhanced situational awareness. The robotic system communicates detected changes via augmented reality to a human teammate for evaluation. We present results from a fielded system using two differently-equipped robots to examine implementation questions of point cloud density and its effect on visualization of changes. Our results show that there are trade-offs between implementations that we believe will be constructive towards similar systems in the future.","paperId":"ca62a88e5d1bac0da878303233f1f515434daf46","paperTitle":"Enabling Situational Awareness via Augmented Reality of Autonomous Robot-Based Environmental Change Detection","paperUrl":"https://www.semanticscholar.org/paper/ca62a88e5d1bac0da878303233f1f515434daf46","images":[],"pdf":null},{"type":"inproceedings","key":"williams2020using","title":"Using Augmented Reality to Better Study Human-Robot Interaction","author":"Williams, Tom and Hirshfield, Leanne and Tran, Nhan and Grant, Trevor and Woodward, Nicholas","booktitle":"International Conference on Human-Computer Interaction","pages":"643--654","year":"2020","organization":"Springer","authors":["Tom Williams","Leanne Hirshfield","Nhan Tran","Trevor Grant","Nicholas Woodward"],"doi":"10.1007/978-3-030-49695-1_43","dblp":"conf/hci/0001HTGW20","venue":"HCI","abstract":"In the field of Human-Robot Interaction, researchers often techniques such as the Wizard-of-Oz paradigms in order to better study narrow scientific questions while carefully controlling robots\u2019 capabilities unrelated to those questions, especially when those other capabilities are not yet easy to automate. However, those techniques often impose limitations on the type of collaborative tasks that can be used, and the perceived realism of those tasks and the task context. In this paper, we discuss how Augmented Reality can be used to address these concerns while increasing researchers\u2019 level of experimental control, and discuss both advantages and disadvantages of this approach.","paperId":"938d582df76f0f28b3bf3d445e7a0c3faff95f1c","paperTitle":"Using Augmented Reality to Better Study Human-Robot Interaction","paperUrl":"https://www.semanticscholar.org/paper/938d582df76f0f28b3bf3d445e7a0c3faff95f1c","images":["https://d3i71xaburhd42.cloudfront.net/938d582df76f0f28b3bf3d445e7a0c3faff95f1c/3-Figure1-1.png"],"pdf":"https://mirrorlab.mines.edu/wp-content/uploads/sites/198/2020/02/williams2020vamr.pdf"},{"type":"inproceedings","key":"hamada2020current","title":"The Current Status and Challenges in Augmented-Reality Navigation System for Robot-Assisted Laparoscopic Partial Nephrectomy","author":"Hamada, Akihiro and Sawada, Atsuro and Kono, Jin and Koeda, Masanao and Onishi, Katsuhiko and Kobayashi, Takashi and Yamasaki, Toshinari and Inoue, Takahiro and Noborio, Hiroshi and Ogawa, Osamu","booktitle":"International Conference on Human-Computer Interaction","pages":"620--629","year":"2020","organization":"Springer","authors":["Akihiro Hamada","Atsuro Sawada","Jin Kono","Masanao Koeda","Katsuhiko Onishi","Takashi Kobayashi","Toshinari Yamasaki","Takahiro Inoue","Hiroshi Noborio","Osamu Ogawa"],"doi":"10.1007/978-3-030-49062-1_42","dblp":"conf/hci/HamadaSKKOKYINO20","venue":"HCI","abstract":null,"paperId":"fa45b86542e4d3966703e543e2dbbf9ec03c4a8f","paperTitle":"The Current Status and Challenges in Augmented-Reality Navigation System for Robot-Assisted Laparoscopic Partial Nephrectomy","paperUrl":"https://www.semanticscholar.org/paper/fa45b86542e4d3966703e543e2dbbf9ec03c4a8f","images":[],"pdf":null},{"type":"inproceedings","key":"gao2020visual","title":"Visual Reference of Ambiguous Objects for Augmented Reality-Powered Human-Robot Communication in a Shared Workspace","author":"Gao, Peng and Reily, Brian and Paul, Savannah and Zhang, Hao","booktitle":"International Conference on Human-Computer Interaction","pages":"550--561","year":"2020","organization":"Springer","authors":["Peng Gao","Brian Reily","Savannah Paul","Hao Zhang"],"doi":"10.1007/978-3-030-49695-1_37","dblp":"conf/hci/GaoRPZ20","venue":"HCI","abstract":null,"paperId":"eea0a637c006b0ec49f080260827ff89a695b0d3","paperTitle":"Visual Reference of Ambiguous Objects for Augmented Reality-Powered Human-Robot Communication in a Shared Workspace","paperUrl":"https://www.semanticscholar.org/paper/eea0a637c006b0ec49f080260827ff89a695b0d3","images":[],"pdf":null},{"type":"inproceedings","key":"reardon2019augmented","title":"Augmented Reality for Human-Robot Teaming in Field Environments","author":"Reardon, Christopher and Lee, Kevin and Rogers, John G and Fink, Jonathan","booktitle":"International Conference on Human-Computer Interaction","pages":"79--92","year":"2019","organization":"Springer","authors":["Christopher Reardon","Kevin Lee","John G Rogers","Jonathan Fink"],"doi":"10.1007/978-3-030-21565-1_6","dblp":"conf/hci/ReardonLRF19","venue":"HCI","abstract":"For teams of humans and mobile robots to work together, several challenges must be overcome, including understanding each others\u2019 position, merging map information, sharing recognition of salient features of the environment, and establishing contextually-grounded communication. These challenges are further compounded for teams operating in field environments, which are unconstrained, uninstrumented, and unknown. While most modern studies that use augmented reality (AR) in human-robot teaming side-step these challenges by focusing on problems addressable in instrumented environments, we argue that current AR technology combined with novel approaches can enable successful teaming in such challenging, real-world settings. To support this, we present a set of prototypes that combine AR with an intelligent, autonomous robot to enable better human-robot teaming in field environments.","paperId":"0d94c029b6357817d977a675736b583509940556","paperTitle":"Augmented Reality for Human-Robot Teaming in Field Environments","paperUrl":"https://www.semanticscholar.org/paper/0d94c029b6357817d977a675736b583509940556","images":[],"pdf":null},{"type":"article","key":"han2015examining","title":"Examining Young Children\'s Perception toward Augmented Reality-Infused Dramatic Play","author":"Han, Jeonghye and Jo, Miheon and Hyun, Eunja and So, Hyo-Jeong","journal":"Educational Technology Research and Development","volume":"63","number":"3","pages":"455--474","year":"2015","publisher":"Springer","authors":["Jeonghye Han","Miheon Jo","Eunja Hyun","Hyo-Jeong So"],"doi":"10.1007/S11423-015-9374-9","venue":"","abstract":"Amid the increasing interest in applying augmented reality (AR) in educational settings, this study explores the design and enactment of an AR-infused robot system to enhance children\u2019s satisfaction and sensory engagement with dramatic play activities. In particular, we conducted an exploratory study to empirically examine children\u2019s perceptions toward the computer- and robot-mediated AR systems designed to make dramatic play activities interactive and participatory. A multi-disciplinary expert group consisting of early childhood education experts, preschool teachers, AR specialists, and robot engineers collaborated to develop a learning scenario and technological systems for dramatic play. The experiment was conducted in a kindergarten setting in Korea, with 81 children (aged 5\u20136\xa0years old). The participants were placed either in the computer-mediated AR condition (n\xa0=\xa040) or the robot-mediated AR condition (n\xa0=\xa041). We administered an instrument to measure children\u2019s perceived levels of the following variables: (a) satisfaction (i.e., interest in dramatic play & user-friendliness), (b) sensory immersion (i.e., self-engagement, environment-engagement & interaction-engagement), and (c) media recognition (i.e., collaboration with media, media function & empathy with media). Data analysis indicates that children in the robot-mediated condition showed significantly higher perceptions than those in the computer-mediated condition regarding the following aspects: interest in dramatic play (satisfaction), interactive engagement (sensory immersion), and empathy with media (media recognition). Furthermore, it was found that the younger-aged children and girls, in particular, perceived AR-infused dramatic play more positively than the older-aged children and boys, respectively. The contribution of this study is to provide empirical evidence about the affordances of robots and AR-based learning systems for young children. This remains a relatively unexplored area of research in the field of learning technologies. Implications of the current study and future research directions are also discussed.","paperId":"b5d361b62eb5f1aac46032b6fa765ee83472bb56","paperTitle":"Examining young children\u2019s perception toward augmented reality-infused dramatic play","paperUrl":"https://www.semanticscholar.org/paper/b5d361b62eb5f1aac46032b6fa765ee83472bb56","images":[],"pdf":null},{"type":"article","key":"fang2014novel","title":"A Novel Augmented Reality-Based Interface for Robot Path Planning","author":"Fang, HC and Ong, SK and Nee, AYC","journal":"International Journal on Interactive Design and Manufacturing (IJIDeM)","volume":"8","number":"1","pages":"33--42","year":"2014","publisher":"Springer","authors":["HC Fang","SK Ong","AYC Nee"],"doi":"10.1007/S12008-013-0191-2","venue":"","abstract":"Intuitive and efficient interfaces for robot task planning have been a challenging issue in robotics as it is essential for the prevalence of robots supporting humans in key areas of activities. This paper presents a novel augmented reality (AR) based interface for interactive robot path and end-effector (EE) orientation planning. A number of human-virtual robot interaction methods have been formulated and implemented with respect to the various types of robotic operations needed in different applications. A Euclidean distance-based method is developed to assist the users in the modification of the waypoints so as to update the planned paths and/or orientation profiles within the proposed AR environment. The virtual cues augmented in the real environment can support and enhance human-virtual robot interaction at different stages of the robot tasks planning process. Two case studies are presented to demonstrate the successful implementation of the proposed AR-based interface in planning robot pick-and-place tasks and path following tasks.","paperId":"9dbaf3e5a2be38a1cc53a562bac35f755efde370","paperTitle":"A novel augmented reality-based interface for robot path planning","paperUrl":"https://www.semanticscholar.org/paper/9dbaf3e5a2be38a1cc53a562bac35f755efde370","images":[],"pdf":null},{"type":"article","key":"fang2013orientation","title":"Orientation Planning of Robot End-Effector Using Augmented Reality","author":"Fang, HC and Ong, SK and Nee, AYC","journal":"The International Journal of Advanced Manufacturing Technology","volume":"67","number":"9-12","pages":"2033--2049","year":"2013","publisher":"Springer","authors":["HC Fang","SK Ong","AYC Nee"],"doi":"10.1007/S00170-012-4629-7","venue":"","abstract":"This paper presents a methodology for planning the orientation of the end-effector for an industrial robot based on the application of augmented reality. The targeted applications are those where the end-effector is constrained to follow a visible path, which position and model are unknown, at suitable inclination angles with respect to the path. The proposed approach enables the users to create a list of control points interactively on a parameterized curve model, define the orientation of the end-effector associated with each control point, and generate a ruled surface representing the path to be planned. An approximated time-optimal trajectory, which is a determined subject to robot actuators and joint velocity constraints using convex optimization techniques, is implemented to simulate a virtual robot, allowing the users to visually evaluate the trajectory planning process. A case study is presented and discussed.","paperId":"1587ea358d09d0a884877a3d82aad0734a0510e7","paperTitle":"Orientation planning of robot end-effector using augmented reality","paperUrl":"https://www.semanticscholar.org/paper/1587ea358d09d0a884877a3d82aad0734a0510e7","images":[],"pdf":"https://page-one.springer.com/pdf/preview/10.1007/s00170-012-4629-7"},{"type":"incollection","key":"maidi2013evaluation","title":"An Evaluation of Camera Pose Methods for an Augmented Reality System: Application to Teaching Industrial Robots","author":"Maidi, Madjid and Mallem, Malik and Benchikh, Laredj and Otmane, Samir","booktitle":"Transactions on Computational Science XVII","pages":"3--30","year":"2013","publisher":"Springer","authors":["Madjid Maidi","Malik Mallem","Laredj Benchikh","Samir Otmane"],"doi":"10.1007/978-3-642-35840-1_1","dblp":"journals/tcos/MaidiMBO13","venue":"Trans. Comput. Sci.","abstract":"In automotive industry, industrial robots are widely used in production lines for many tasks such as welding, painting or assembly. Their use requires, from users, both a good manipulation and robot control. Recently, new tools have been developed to realize fast and accurate trajectories in many production sectors by using the real prototype of vehicle or a generalized design within a virtual simulation platform. However, many issues could be considered in these cases: the delay between the design of the vehicle and its production is often important, moreover, the virtual modeling presents a non realistic aspect of the real robot and vehicle, so this factor could introduce localization inacurracies in performing trajectories. Our work is registered as a part of TRI project (Teleteaching Industrial Robots) which aims to realize a demonstrator showing the interaction of industrial robots with virtual components and allowing to train users to perform successfully their tasks on a virtual representation of a production entity.","paperId":"296dacf108258df50e2b96d7dfac3406c516d413","paperTitle":"An Evaluation of Camera Pose Methods for an Augmented Reality System: Application to Teaching Industrial Robots","paperUrl":"https://www.semanticscholar.org/paper/296dacf108258df50e2b96d7dfac3406c516d413","images":["https://d3i71xaburhd42.cloudfront.net/296dacf108258df50e2b96d7dfac3406c516d413/7-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/296dacf108258df50e2b96d7dfac3406c516d413/15-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/296dacf108258df50e2b96d7dfac3406c516d413/8-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/296dacf108258df50e2b96d7dfac3406c516d413/17-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/296dacf108258df50e2b96d7dfac3406c516d413/9-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/296dacf108258df50e2b96d7dfac3406c516d413/18-Table3-1.png","https://d3i71xaburhd42.cloudfront.net/296dacf108258df50e2b96d7dfac3406c516d413/9-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/296dacf108258df50e2b96d7dfac3406c516d413/20-Table4-1.png","https://d3i71xaburhd42.cloudfront.net/296dacf108258df50e2b96d7dfac3406c516d413/11-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/296dacf108258df50e2b96d7dfac3406c516d413/11-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/296dacf108258df50e2b96d7dfac3406c516d413/22-Table6-1.png","https://d3i71xaburhd42.cloudfront.net/296dacf108258df50e2b96d7dfac3406c516d413/13-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/296dacf108258df50e2b96d7dfac3406c516d413/16-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/296dacf108258df50e2b96d7dfac3406c516d413/16-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/296dacf108258df50e2b96d7dfac3406c516d413/17-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/296dacf108258df50e2b96d7dfac3406c516d413/18-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/296dacf108258df50e2b96d7dfac3406c516d413/19-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/296dacf108258df50e2b96d7dfac3406c516d413/21-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/296dacf108258df50e2b96d7dfac3406c516d413/21-Figure14-1.png"],"pdf":"https://hal.archives-ouvertes.fr/hal-00817072/file/An_Evaluation_of_Camera_Pose_Methods_for_an_Augmented_Reality_System.pdf"},{"type":"inproceedings","key":"suzuki2010scorpion","title":"Scorpion Shaped Endoscopic Surgical Robot for NOTES and SPS with Augmented Reality Functions","author":"Suzuki, Naoki and Hattori, Asaki and Tanoue, Kazuo and Ieiri, Satoshi and Konishi, Kozo and Tomikawa, Morimasa and Kenmotsu, Hajime and Hashizume, Makoto","booktitle":"International Workshop on Medical Imaging and Virtual Reality","pages":"541--550","year":"2010","organization":"Springer","authors":["Naoki Suzuki","Asaki Hattori","Kazuo Tanoue","Satoshi Ieiri","Kozo Konishi","Morimasa Tomikawa","Hajime Kenmotsu","Makoto Hashizume"],"doi":"10.1007/978-3-642-15699-1_57","dblp":"conf/miar/SuzukiHTIKTKH10","venue":"MIAR","abstract":"In the process of developing an endoscopic surgical robot system that adapts to NOTES (Natural Orifice Translumenal Endoscopic Surgery) and SPS (Single port surgery), by making the tip a soft tubular structure and adding an augmented reality function to the system, we were able to improve the general function of the surgical robot system. First, we added a haptic sense function to avoid breaking the soft tissue and to avoid the danger of cutting it. These occur due to the small size of the touching surface between the tip of the robot arm and the soft tissue. We were able to conduct operation by feeding back to the surgeon the force applied to the soft tissue by detecting the haptic sense of the small forceps at the tip through measuring the tension variation at the base of the wire that drives the robot arm. We also mounted various numbers of augmented reality function such as grasping the exact location of the surgical robot inside the human body and information on how the robot is reaching the location of surgery. As a result, we were able to build a system that can conduct safe surgery with the system\'s two main characteristics - the smallness and the high degree of freedom to move.","paperId":"18408cc996b9275998677b9131eb3d522ca10d8d","paperTitle":"Scorpion Shaped Endoscopic Surgical Robot for NOTES and SPS With Augmented Reality Functions","paperUrl":"https://www.semanticscholar.org/paper/18408cc996b9275998677b9131eb3d522ca10d8d","images":[],"pdf":null},{"type":"inproceedings","key":"lenhardt2010augmented","title":"An Augmented-Reality Based Brain-Computer Interface for Robot Control","author":"Lenhardt, Alexander and Ritter, Helge","booktitle":"International Conference on Neural Information Processing","pages":"58--65","year":"2010","organization":"Springer","authors":["Alexander Lenhardt","Helge Ritter"],"doi":"10.1007/978-3-642-17534-3_8","dblp":"conf/iconip/LenhardtR10","venue":"ICONIP","abstract":"In this study we demonstrate how the combination of Augmented-Reality (AR) techniques and an asynchronous P300-based Brain-Computer Interface (BCI) can be used to control a robotic actuator by thought.We show results of an experimental study which required the users to move several objects placed on a desk by concentrating on a specific object. Competitive communication speed of up to 5.9 correct symbols per minute at a 100% accuracy level could be achieved for one subject using an asynchronous paradigm which enables the user to start communicating a command at an arbitrary time and thus mitigating the drawbacks of the standard cue based P300 protcols.","paperId":"dfd98dcd7203d33fd9c09b458785d5d126ab8749","paperTitle":"An Augmented-Reality Based Brain-Computer Interface for Robot Control","paperUrl":"https://www.semanticscholar.org/paper/dfd98dcd7203d33fd9c09b458785d5d126ab8749","images":[],"pdf":null},{"type":"incollection","key":"mollet2009virtual","title":"Virtual and Augmented Reality Tools for Teleoperation: Improving Distant Immersion and Perception","author":"Mollet, Nicolas and Chellali, Ryad and Brayda, Luca","booktitle":"Transactions on edutainment II","pages":"135--159","year":"2009","publisher":"Springer","authors":["Nicolas Mollet","Ryad Chellali","Luca Brayda"],"doi":"10.1007/978-3-642-03270-7_10","dblp":"journals/tedu/MolletCB09","venue":"Trans. Edutainment","abstract":"This paper reports on the development of a collaborative system enabling to tele-operate groups of robots. The general aim is to allow a group of tele-operators to share the control of robots. This system enables the joint team of operators and robots to achieve complex tasks such as inspecting an area or exploring unknown parts of an unknown environment. \\n \\nThanks to virtual and augmented reality techniques, a Virtual and Augmented Collaborative Environment (VACE) is built. This last supports a N*M*K scheme: N 1;N tele-operators control M 1;M robots at K 1;K abstraction levels. Indeed, our VACE allows to N people to control any robot at different abstraction\'s levels (from individual actuator\'s control K = 1 to final status specification K = 3). On the other hand, the VACE enables to build synthetic representations of the robots and their world. Robots may appear to tele-operators as individuals or reduced to a single virtual entity. \\n \\nWe present in this paper an overview of this system and an application, namely a museum visit. We show how visitors can control robots and improve their immersion using a head-tracking system combined with a VR helmet to control the active vision systems on the remote mobile robots. We also introduce the ability to control the remote robots configuration, at a group level. We finally show how Augmented and Virtual Reality add-ons are included to ease the execution of remote tasks.","paperId":"364f26064da9cbc6c27f4181262691839cbd66eb","paperTitle":"Virtual and Augmented Reality Tools for Teleoperation: Improving Distant Immersion and Perception","paperUrl":"https://www.semanticscholar.org/paper/364f26064da9cbc6c27f4181262691839cbd66eb","images":[],"pdf":"https://www.wikidata.org/entity/Q60184138"},{"type":"inproceedings","key":"lee2008augmented","title":"Augmented Reality Based Vision System for Network Based Mobile Robot","author":"Lee, Ho-Dong and Kim, Dongwon and Park, Min-Chul and Park, Gwi-Tae","booktitle":"Asia-Pacific Conference on Computer Human Interaction","pages":"123--130","year":"2008","organization":"Springer","authors":["Ho-Dong Lee","Dongwon Kim","Min-Chul Park","Gwi-Tae Park"],"doi":"10.1007/978-3-540-70585-7_14","dblp":"conf/apchi/LeeKPP08","venue":"APCHI","abstract":"The human machine interface is an essential part of intelligent robotic system. Through the human machine interface, human user can interact with the robot. Especially, in tele-robotic environment, the human machine interface can be developed with remarkable extended functionality. In this paper, we introduce a tele-presence vision system for monitoring of a network based mobile robot. The vision system is a vision part of human machine interface with augmented reality for the network based mobile robot. We synchronize head motion of human user and the camera motion of the mobile robot using visual information. So user of the mobile robot can monitor environment of the mobile robot as eyesight of mobile robot. Also, the system partially creates a panoramic image to solve a pose latency problem. In this paper, we evaluate and compare panoramic images from two different methods. One is pattern matching method which is simple and fast, the other is Scale Invariant Feature Transform (SIFT) which is complex and slow. Finally, proposed vision system provides high resolution and wide Field Of View (FOV) to user who is wearing a Head Mounted Display (HMD).","paperId":"3719101f57e1ec88d32628904e9a535cc506f9e7","paperTitle":"Augmented Reality Based Vision System for Network Based Mobile Robot","paperUrl":"https://www.semanticscholar.org/paper/3719101f57e1ec88d32628904e9a535cc506f9e7","images":[],"pdf":null},{"type":"inproceedings","key":"lee2007human","title":"Human Machine Interface with Augmented Reality for the Network Based Mobile Robot","author":"Lee, Ho-Dong and Lee, Hyun-Gu and Kim, Joo-Hyung and Park, Min-Chul and Park, Gwi-Tae","booktitle":"International Conference on Knowledge-Based and Intelligent Information and Engineering Systems","pages":"57--64","year":"2007","organization":"Springer","authors":["Ho-Dong Lee","Hyun-Gu Lee","Joo-Hyung Kim","Min-Chul Park","Gwi-Tae Park"],"doi":"10.1007/978-3-540-74829-8_8","dblp":"conf/kes/LeeLKPP07","venue":"KES","abstract":"The human-machine interface is an essential part of intelligent robotic system. Through the human-machine interface, human being can interact with the robot. Especially, in tele-robotics environment, the human-machine interface can be developed with remarkable extended functionality. In this paper, we propose a human-machine interface with augmented reality for the network based mobile robot. Generally, we can take some meaningful information from human\'s motion such as movement of head or fingers. So, it is very useful to take these motions as input for systems. We synchronize head motion of human being and the camera motion of the mobile robot using visual information. So user of the mobile robot can monitor environment of the mobile robot as eyesight of mobile robot. Then we use gesture recognition for control the mobile robot. In the implemented framework, the user can monitor what happens in environment as eyesight of mobile robot and control the mobile robot easily and intuitively by using gesture.","paperId":"df327fe9d5027e5e2aaf241910e28764b2fdac4f","paperTitle":"Human Machine Interface with Augmented Reality for the Network Based Mobile Robot","paperUrl":"https://www.semanticscholar.org/paper/df327fe9d5027e5e2aaf241910e28764b2fdac4f","images":[],"pdf":null},{"type":"article","key":"su2021mixed","title":"Mixed Reality-Enhanced Intuitive Teleoperation with Hybrid Virtual Fixtures for Intelligent Robotic Welding","author":"Su, Yun-Peng and Chen, Xiao-Qi and Zhou, Tony and Pretty, Christopher and Chase, J Geoffrey","journal":"Applied Sciences","volume":"11","number":"23","pages":"11280","year":"2021","publisher":"Multidisciplinary Digital Publishing Institute","authors":["Yun-Peng Su","Xiao-Qi Chen","Tony Zhou","Christopher Pretty","J Geoffrey Chase"],"doi":"10.3390/app112311280","venue":"Applied Sciences","abstract":"This paper presents an integrated scheme based on a mixed reality (MR) and haptic feedback approach for intuitive and immersive teleoperation of robotic welding systems. By incorporating MR technology, the user is fully immersed in a virtual operating space augmented by real-time visual feedback from the robot working space. The proposed robotic tele-welding system features imitative motion mapping from the user\u2019s hand movements to the welding robot motions, and it enables the spatial velocity-based control of the robot tool center point (TCP). The proposed mixed reality virtual fixture (MRVF) integration approach implements hybrid haptic constraints to guide the operator\u2019s hand movements following the conical guidance to effectively align the welding torch for welding and constrain the welding operation within a collision-free area. Onsite welding and tele-welding experiments identify the operational differences between professional and unskilled welders and demonstrate the effectiveness of the proposed MRVF tele-welding framework for novice welders. The MRVF-integrated visual/haptic tele-welding scheme reduced the torch alignment times by 56% and 60% compared to the MRnoVF and baseline cases, with minimized cognitive workload and optimal usability. The MRVF scheme effectively stabilized welders\u2019 hand movements and eliminated undesirable collisions while generating smooth welds.","paperId":"7a322fd2bd6d2fd8c6980e2ce76c2f8d556a1df1","paperTitle":"Mixed Reality-Enhanced Intuitive Teleoperation with Hybrid Virtual Fixtures for Intelligent Robotic Welding","paperUrl":"https://www.semanticscholar.org/paper/7a322fd2bd6d2fd8c6980e2ce76c2f8d556a1df1","images":[],"pdf":"https://www.mdpi.com/2076-3417/11/23/11280/pdf"},{"type":"article","key":"vzidek2021cnn","title":"CNN Training Using 3D Virtual Models for Assisted Assembly with Mixed Reality and Collaborative Robots","author":"\u017bidek, Kamil and Pitel\', J\'an and Balog, Michal and Ho\u0161ovsk`y, Alexander and Hladk`y, Vratislav and Lazor\'ik, Peter and Iakovets, Angelina and Dem\u010b\'ak, Jakub","journal":"Applied Sciences","volume":"11","number":"9","pages":"4269","year":"2021","publisher":"Multidisciplinary Digital Publishing Institute","authors":["Kamil \u017bidek","J\'an Pitel\'","Michal Balog","Alexander Ho\u0161ovsk`y","Vratislav Hladk`y","Peter Lazor\'ik","Angelina Iakovets","Jakub Dem\u010b\'ak"],"doi":"10.3390/APP11094269","venue":"","abstract":null,"paperId":"c0186ada22bfa12c3a57cb519c7c00f323da01ae","paperTitle":"CNN Training Using 3D Virtual Models for Assisted Assembly with Mixed Reality and Collaborative Robots","paperUrl":"https://www.semanticscholar.org/paper/c0186ada22bfa12c3a57cb519c7c00f323da01ae","images":[],"pdf":"https://www.mdpi.com/2076-3417/11/9/4269/pdf"},{"type":"article","key":"lupetti2018design","title":"Design and Evaluation of a Mixed-Reality Playground for Child-Robot Games","author":"Lupetti, Maria Luce and Piumatti, Giovanni and Germak, Claudio and Lamberti, Fabrizio","journal":"Multimodal Technologies and Interaction","volume":"2","number":"4","pages":"69","year":"2018","publisher":"Multidisciplinary Digital Publishing Institute","authors":["Maria Luce Lupetti","Giovanni Piumatti","Claudio Germak","Fabrizio Lamberti"],"doi":"10.3390/mti2040069","venue":"Multimodal Technologies and Interaction","abstract":"In this article we present the Phygital Game project, a mixed-reality game platform in which children can play with or against a robot. The project was developed by adopting a human-centered design approach, characterized by the engagement of both children and parents in the design process, and situating the game platform in a real context\u2014an educational center for children. We report the results of both the preliminary studies and the final testing session, which focused on the evaluation of usability factors. By providing a detailed description of the process and the results, this work aims at sharing the findings and the lessons learned about both the implications of adopting a human-centered approach across the whole design process and the specific challenges of developing a mixed-reality playground.","paperId":"22b7b450179dcdafee0a2862fc936483814a7be8","paperTitle":"Design and Evaluation of a Mixed-Reality Playground for Child\u2012Robot Games","paperUrl":"https://www.semanticscholar.org/paper/22b7b450179dcdafee0a2862fc936483814a7be8","images":["https://d3i71xaburhd42.cloudfront.net/22b7b450179dcdafee0a2862fc936483814a7be8/7-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/22b7b450179dcdafee0a2862fc936483814a7be8/8-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/22b7b450179dcdafee0a2862fc936483814a7be8/9-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/22b7b450179dcdafee0a2862fc936483814a7be8/10-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/22b7b450179dcdafee0a2862fc936483814a7be8/12-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/22b7b450179dcdafee0a2862fc936483814a7be8/12-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/22b7b450179dcdafee0a2862fc936483814a7be8/12-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/22b7b450179dcdafee0a2862fc936483814a7be8/13-Figure8-1.png"],"pdf":null},{"type":"article","key":"wen2017augmented","title":"Augmented Reality Guidance with Multimodality Imaging Data and Depth-Perceived Interaction for Robot-Assisted Surgery","author":"Wen, Rong and Chng, Chin-Boon and Chui, Chee-Kong","journal":"Robotics","volume":"6","number":"2","pages":"13","year":"2017","publisher":"Multidisciplinary Digital Publishing Institute","authors":["Rong Wen","Chin-Boon Chng","Chee-Kong Chui"],"doi":"10.3390/robotics6020013","dblp":"journals/robotics/WenCC17","venue":"Robotics","abstract":"Image-guided surgical procedures are challenged by mono image modality, two-dimensional anatomical guidance and non-intuitive human-machine interaction. The introduction of Tablet-based augmented reality (AR) into surgical robots may assist surgeons with overcoming these problems. In this paper, we proposed and developed a robot-assisted surgical system with interactive surgical guidance using tablet-based AR with a Kinect sensor for three-dimensional (3D) localization of patient anatomical structures and intraoperative 3D surgical tool navigation. Depth data acquired from the Kinect sensor was visualized in cone-shaped layers for 3D AR-assisted navigation. Virtual visual cues generated by the tablet were overlaid on the images of the surgical field for spatial reference. We evaluated the proposed system and the experimental results showed that the tablet-based visual guidance system could assist surgeons in locating internal organs, with errors between 1.74 and 2.96 mm. We also demonstrated that the system was able to provide mobile augmented guidance and interaction for surgical tool navigation.","paperId":"b21c1266d17dc3990d41ea3d72ad04d59efbcffb","paperTitle":"Augmented Reality Guidance with Multimodality Imaging Data and Depth-Perceived Interaction for Robot-Assisted Surgery","paperUrl":"https://www.semanticscholar.org/paper/b21c1266d17dc3990d41ea3d72ad04d59efbcffb","images":["https://d3i71xaburhd42.cloudfront.net/b21c1266d17dc3990d41ea3d72ad04d59efbcffb/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/b21c1266d17dc3990d41ea3d72ad04d59efbcffb/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/b21c1266d17dc3990d41ea3d72ad04d59efbcffb/11-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/b21c1266d17dc3990d41ea3d72ad04d59efbcffb/12-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/b21c1266d17dc3990d41ea3d72ad04d59efbcffb/15-Figure15-1.png"],"pdf":null},{"type":"article","key":"strazdas2021robo","title":"Robo-HUD: Interaction Concept for Contactless Operation of Industrial Cobotic Systems","author":"Strazdas, Dominykas and Hintz, Jan and Al-Hamadi, Ayoub","journal":"Applied Sciences","volume":"11","number":"12","pages":"5366","year":"2021","publisher":"Multidisciplinary Digital Publishing Institute","authors":["Dominykas Strazdas","Jan Hintz","Ayoub Al-Hamadi"],"doi":"10.3390/APP11125366","venue":"","abstract":"Intuitive and safe interfaces for robots are challenging issues in robotics. Robo-HUD is a gadget-less interaction concept for contactless operation of industrial systems. We use virtual collision detection based on time-of-flight sensor data, combined with augmented reality and audio feedback, allowing the operators to navigate a virtual menu by \u201chover and hold\u201d gestures. When incorporated with virtual safety barriers, the collision detection also functions as a safety feature, slowing or stopping the robot if a barrier is breached. Additionally, a user focus recognition module monitors the awareness, enabling the interaction only when intended. Early case studies show that these features present good use-cases for inspection tasks and operation in difficult environments, where contactless operation is needed.","paperId":"e48cb4ee1036c76ed047b4495789e1fb552a061b","paperTitle":"Robo-HUD: Interaction Concept for Contactless Operation of Industrial Cobotic Systems","paperUrl":"https://www.semanticscholar.org/paper/e48cb4ee1036c76ed047b4495789e1fb552a061b","images":[],"pdf":"https://doi.org/10.3390/app11125366"},{"type":"article","key":"giannone2021augmented","title":"Augmented Reality and Image-Guided Robotic Liver Surgery","author":"Giannone, Fabio and Felli, Emanuele and Cherkaoui, Zineb and Mascagni, Pietro and Pessaux, Patrick","journal":"Cancers","volume":"13","number":"24","pages":"6268","year":"2021","publisher":"Multidisciplinary Digital Publishing Institute","authors":["Fabio Giannone","Emanuele Felli","Zineb Cherkaoui","Pietro Mascagni","Patrick Pessaux"],"doi":"10.3390/cancers13246268","venue":"Cancers","abstract":"Simple Summary Robotic surgery has gained much attention in liver resection for its potential to increase surgical dexterity in a minimally invasive scenario. Different series are reported in the literature with promising results, although strong evidence is lacking. In addition, the robotic system presents the advantage of creating a hybrid interface in which pre- and intra-operative imaging tools could be exploited alone or together in order to guide surgical resection. These technologies have been developed with the aim of increasing surgical safety and improving oncological results. However, some drawbacks are still present, and the literature lacks data given the relatively recent distribution of the robotic platform and of some of these technologies. Abstract Artificial intelligence makes surgical resection easier and safer, and, at the same time, can improve oncological results. The robotic system fits perfectly with these more or less diffused technologies, and it seems that this benefit is mutual. In liver surgery, robotic systems help surgeons to localize tumors and improve surgical results with well-defined preoperative planning or increased intraoperative detection. Furthermore, they can balance the absence of tactile feedback and help recognize intrahepatic biliary or vascular structures during parenchymal transection. Some of these systems are well known and are already widely diffused in open and laparoscopic hepatectomies, such as indocyanine green fluorescence or ultrasound-guided resections, whereas other tools, such as Augmented Reality, are far from being standardized because of the high complexity and elevated costs. In this paper, we review all the experiences in the literature on the use of artificial intelligence systems in robotic liver resections, describing all their practical applications and their weaknesses.","paperId":"074cb4bcd36e368cd6a02eeb8c12d4362014587f","paperTitle":"Augmented Reality and Image-Guided Robotic Liver Surgery","paperUrl":"https://www.semanticscholar.org/paper/074cb4bcd36e368cd6a02eeb8c12d4362014587f","images":["https://d3i71xaburhd42.cloudfront.net/7e41000588a040f6ba5f52a4e9a9238c86042eed/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/7e41000588a040f6ba5f52a4e9a9238c86042eed/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/7e41000588a040f6ba5f52a4e9a9238c86042eed/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/7e41000588a040f6ba5f52a4e9a9238c86042eed/6-Figure4-1.png"],"pdf":null},{"type":"inproceedings","key":"pinciroli2018simulating","title":"Simulating Kilobots within Argos: Models and Experimental Validation","author":"Pinciroli, Carlo and Talamali, Mohamed S and Reina, Andreagiovanni and Marshall, James AR and Trianni, Vito","booktitle":"International Conference on Swarm Intelligence","pages":"176--187","year":"2018","organization":"Springer","doi":"10.1007/978-3-030-00533-7_14","authors":["Carlo Pinciroli","Mohamed S Talamali","Andreagiovanni Reina","James AR Marshall","Vito Trianni"],"dblp":"conf/antsw/PinciroliTRMT18","venue":"ANTS Conference","abstract":"The Kilobot is a popular platform for swarm robotics research due to its low cost and ease of manufacturing. Despite this, the effort to bootstrap the design of new behaviours and the time necessary to develop and debug new behaviours is considerable. To make this process less burdensome, high-performing and flexible simulation tools are important. In this paper, we present a plugin for the ARGoS simulator designed to simplify and accelerate experimentation with Kilobots. First, the plugin supports cross-compiling against the real robot platform, removing the need to translate algorithms across different languages. Second, it is highly configurable to match the real robot behaviour. Third, it is fast and allows running simulations with several hundreds of Kilobots in a fraction of real time. We present the design choices that drove our work and report on experiments with physical robots performed to validate simulated behaviours.","paperId":"c847a0a28fde2117c47c8bd6f8ff73b78eb24286","paperTitle":"Simulating Kilobots Within ARGoS: Models and Experimental Validation","paperUrl":"https://www.semanticscholar.org/paper/c847a0a28fde2117c47c8bd6f8ff73b78eb24286","images":["https://d3i71xaburhd42.cloudfront.net/c847a0a28fde2117c47c8bd6f8ff73b78eb24286/5-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/c847a0a28fde2117c47c8bd6f8ff73b78eb24286/6-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/c847a0a28fde2117c47c8bd6f8ff73b78eb24286/7-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/c847a0a28fde2117c47c8bd6f8ff73b78eb24286/8-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/c847a0a28fde2117c47c8bd6f8ff73b78eb24286/9-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/c847a0a28fde2117c47c8bd6f8ff73b78eb24286/10-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/c847a0a28fde2117c47c8bd6f8ff73b78eb24286/11-Figure7-1.png"],"pdf":"http://eprints.whiterose.ac.uk/133194/1/pinciroli-ANTS2018.pdf"},{"type":"article","key":"omidshafiei2016measurable","title":"Measurable Augmented Reality for Prototyping Cyberphysical Systems: A Robotics Platform to Aid the Hardware Prototyping and Performance Testing of Algorithms","author":"Omidshafiei, Shayegan and Agha-Mohammadi, Ali-Akbar and Chen, Yu Fan and Ure, Nazim Kemal and Liu, Shih-Yuan and Lopez, Brett T and Surati, Rajeev and How, Jonathan P and Vian, John","journal":"IEEE Control Systems Magazine","volume":"36","number":"6","pages":"65--87","year":"2016","publisher":"IEEE","doi":"10.1109/MCS.2016.2602090","authors":["Shayegan Omidshafiei","Ali-Akbar Agha-Mohammadi","Yu Fan Chen","Nazim Kemal Ure","Shih-Yuan Liu","Brett T Lopez","Rajeev Surati","Jonathan P How","John Vian"],"venue":"IEEE Control Systems","abstract":"Planning, control, perception, and learning are current research challenges in multirobot systems. The transition dynamics of the robots may be unknown or stochastic, making it difficult to select the best action each robot must take at a given time. The observation model, a function of the robots\' sensor systems, may be noisy or partial, meaning that deterministic knowledge of the team\'s state is often impossible to attain. Moreover, the actions each robot can take may have an associated success rate and/or a probabilistic completion time. Robots designed for real-world applications require careful consideration of such sources of uncertainty, regardless of the control scheme or planning or learning algorithms used for a specific problem. Understanding the underlying mechanisms of planning algorithms can be challenging due to the latent variables they often operate on. When performance testing such algorithms on hardware, the simultaneous use of the debugging and visualization tools available on a workstation can be difficult. This transition from experimentation to implementation becomes especially challenging when the experiments need to replicate some feature of the software tool set in hardware, such as simulation of visually complex environments. This article details a robotics prototyping platform, called measurable augmented reality for prototyping cyberphysical systems (MAR-CPS), that directly addresses this problem, allowing for the real-time visualization of latent state information to aid hardware prototyping and performance testing of algorithms.","paperId":"c0efa3c424a15a43c485652e5ea8ebd2c4bc2105","paperTitle":"Measurable Augmented Reality for Prototyping Cyberphysical Systems: A Robotics Platform to Aid the Hardware Prototyping and Performance Testing of Algorithms","paperUrl":"https://www.semanticscholar.org/paper/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105","images":["https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/42-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/43-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/44-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/45-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/46-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/47-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/48-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/49-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/50-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/51-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/52-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/53-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/54-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/55-Figure14-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/56-Figure15-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/57-Figure16-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/58-Figure17-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/59-Figure18-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/60-Figure19-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/61-Figure20-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/62-Figure21-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/63-Figure22-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/64-Figure23-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/65-Figure24-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/66-Figure25-1.png","https://d3i71xaburhd42.cloudfront.net/c0efa3c424a15a43c485652e5ea8ebd2c4bc2105/67-Figure26-1.png"],"pdf":"https://dspace.mit.edu/bitstream/1721.1/114291/1/omidshafiei_marcps_csm16.pdf"},{"type":"incollection","key":"chen2019pinpointfly","title":"PinpointFly: An Egocentric Position-Pointing Drone Interface Using Mobile AR","author":"Chen, Linfeng and Ebi, Akiyuki and Takashima, Kazuki and Fujita, Kazuyuki and Kitamura, Yoshifumi","booktitle":"SIGGRAPH Asia 2019 Emerging Technologies","pages":"34--35","year":"2019","doi":"10.1145/3355049.3360534","authors":["Linfeng Chen","Akiyuki Ebi","Kazuki Takashima","Kazuyuki Fujita","Yoshifumi Kitamura"],"dblp":"conf/siggrapha/ChenETFK19","venue":"SIGGRAPH ASIA Emerging Technologies","abstract":"We propose PinpointFly, an egocentric drone interface that allows users to arbitrarily position and rotate a flying drone using position control interactions on a see-through mobile AR where the position and direction of the drone are visually enhanced with a virtual cast shadow. Unlike traditional speed control methods, users hold a smartphone and precisely edit the drone\u2019s motions and directions by dragging the cast shadow or a slider bar on the touchscreen.","paperId":"e0da2179896c76f5c618a42283191eef16fb2459","paperTitle":"PinpointFly: An Egocentric Position-pointing Drone Interface using Mobile AR","paperUrl":"https://www.semanticscholar.org/paper/e0da2179896c76f5c618a42283191eef16fb2459","images":["https://d3i71xaburhd42.cloudfront.net/edb9eec4465b94b4a0e1cd5e1a4f3157c51b231a/2-Figure2-1.png"],"pdf":"https://doi.org/10.1145/3355049.3360534"},{"type":"incollection","key":"hiraki2015phygital","title":"Phygital Field: Integrated Field with Visible Images and Robot Swarm Controlled by Invisible Images","author":"Hiraki, Takefumi and Takahashi, Issei and Goto, Shotaro and Fukushima, Shogo and Naemura, Takeshi","booktitle":"ACM SIGGRAPH 2015 Posters","pages":"1--1","year":"2015","doi":"10.1145/2787626.2792604","authors":["Takefumi Hiraki","Issei Takahashi","Shotaro Goto","Shogo Fukushima","Takeshi Naemura"],"dblp":"conf/siggraph/HirakiTGFN15","venue":"SIGGRAPH Posters","abstract":"Forming images by using a swarm of mobile robots has emerged as a new platform for computer entertainment. Each robot has colored lighting, and the swarm represents various abstract patterns by using the lighting and the locomotion.","paperId":"5aed670f20509dba63262bdfe812989d191f5399","paperTitle":"Phygital field: integrated field with visible images and robot swarm controlled by invisible images","paperUrl":"https://www.semanticscholar.org/paper/5aed670f20509dba63262bdfe812989d191f5399","images":["https://d3i71xaburhd42.cloudfront.net/5aed670f20509dba63262bdfe812989d191f5399/1-Figure1-1.png"],"pdf":"https://doi.org/10.1145/2787626.2792604"},{"type":"inproceedings","key":"mueller2012interactive","title":"Interactive Construction: Interactive Fabrication of Functional Mechanical Devices","author":"Mueller, Stefanie and Lopes, Pedro and Baudisch, Patrick","booktitle":"Proceedings of the 25th annual ACM symposium on User interface software and technology","pages":"599--606","year":"2012","doi":"10.1145/2380116.2380191","authors":["Stefanie Mueller","Pedro Lopes","Patrick Baudisch"],"dblp":"conf/uist/MullerLB12","venue":"UIST","abstract":"Personal fabrication tools, such as laser cutters and 3D printers allow users to create precise objects quickly. However, working through a CAD system removes users from the workpiece. Recent interactive fabrication tools reintroduce this directness, but at the expense of precision. In this paper, we introduce constructable, an interactive drafting table that produces precise physical output in every step. Users interact by drafting directly on the workpiece using a hand-held laser pointer. The system tracks the pointer, beautifies its path, and implements its effect by cutting the workpiece using a fast high-powered laser cutter. Constructable achieves precision through tool-specific constraints, user-defined sketch lines, and by using the laser cutter itself for all visual feedback, rather than using a screen or projection. We demonstrate how constructable allows creating simple but functional devices, including a simple gearbox, that cannot be created with traditional interactive fabrication tools.","paperId":"bca7715e0639d97f5fa89932d6b9566e40ea0655","paperTitle":"Interactive construction: interactive fabrication of functional mechanical devices","paperUrl":"https://www.semanticscholar.org/paper/bca7715e0639d97f5fa89932d6b9566e40ea0655","images":["https://d3i71xaburhd42.cloudfront.net/bca7715e0639d97f5fa89932d6b9566e40ea0655/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/bca7715e0639d97f5fa89932d6b9566e40ea0655/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/bca7715e0639d97f5fa89932d6b9566e40ea0655/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/bca7715e0639d97f5fa89932d6b9566e40ea0655/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/bca7715e0639d97f5fa89932d6b9566e40ea0655/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/bca7715e0639d97f5fa89932d6b9566e40ea0655/3-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/bca7715e0639d97f5fa89932d6b9566e40ea0655/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/bca7715e0639d97f5fa89932d6b9566e40ea0655/4-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/bca7715e0639d97f5fa89932d6b9566e40ea0655/4-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/bca7715e0639d97f5fa89932d6b9566e40ea0655/4-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/bca7715e0639d97f5fa89932d6b9566e40ea0655/4-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/bca7715e0639d97f5fa89932d6b9566e40ea0655/5-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/bca7715e0639d97f5fa89932d6b9566e40ea0655/6-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/bca7715e0639d97f5fa89932d6b9566e40ea0655/7-Figure14-1.png"],"pdf":"http://plopes.org/wp-content/uploads/papers/2012-UIST-Constructable-Mueller-Lopes.pdf"},{"type":"inproceedings","key":"park2014qr","title":"Qr-Code Based Online Robot Augmented Reality System for Education","author":"Park, Jung Pil and Park, Min Woo and Jung, Soon Ki","booktitle":"Proceedings of the 29th Annual ACM Symposium on Applied Computing","pages":"180--185","year":"2014","doi":"10.1145/2554850.2555038","authors":["Jung Pil Park","Min Woo Park","Soon Ki Jung"],"dblp":"conf/sac/ParkPJ14","venue":"SAC","abstract":"In this paper, we propose an efficient, augmented reality system for education utilizing scenarios acquired from the metadata of quick response (QR) code. Our system enables interaction using an assembled robot in a virtual environment. First, we build a virtual environment around the robot according to the metadata. In addition, we create and manage a wide variety of content using metadata. We estimate the camera pose by using both marker-based and markerless methods. Lastly, we use a Bluetooth module and metadata of QR codes to interact with objects in the virtual environment.","paperId":"e3f04388cb3c843f6dc98f64ac9f9bf768591b41","paperTitle":"QR-code based online robot augmented reality system for education","paperUrl":"https://www.semanticscholar.org/paper/e3f04388cb3c843f6dc98f64ac9f9bf768591b41","images":["https://d3i71xaburhd42.cloudfront.net/e3f04388cb3c843f6dc98f64ac9f9bf768591b41/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/e3f04388cb3c843f6dc98f64ac9f9bf768591b41/3-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/e3f04388cb3c843f6dc98f64ac9f9bf768591b41/3-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/e3f04388cb3c843f6dc98f64ac9f9bf768591b41/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/e3f04388cb3c843f6dc98f64ac9f9bf768591b41/3-Table3-1.png","https://d3i71xaburhd42.cloudfront.net/e3f04388cb3c843f6dc98f64ac9f9bf768591b41/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/e3f04388cb3c843f6dc98f64ac9f9bf768591b41/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/e3f04388cb3c843f6dc98f64ac9f9bf768591b41/4-Table4-1.png","https://d3i71xaburhd42.cloudfront.net/e3f04388cb3c843f6dc98f64ac9f9bf768591b41/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/e3f04388cb3c843f6dc98f64ac9f9bf768591b41/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/e3f04388cb3c843f6dc98f64ac9f9bf768591b41/5-Figure7-1.png"],"pdf":"https://doi.org/10.1145/2554850.2555038"},{"type":"inproceedings","key":"lee2020realization","title":"Realization of Robot Fish with 3D Hologram Fish Using Augmented Reality","author":"Lee, Jae Young and Lee, Jong-Wook and Talluri, Teressa and Angani, Amarnathvarma and Lee, Jeong Bea","booktitle":"2020 IEEE 2nd International Conference on Architecture, Construction, Environment and Hydraulics (ICACEH)","pages":"102--104","year":"2020","organization":"IEEE","doi":"10.1109/ICACEH51803.2020.9366226","authors":["Jae Young Lee","Jong-Wook Lee","Teressa Talluri","Amarnathvarma Angani","Jeong Bea Lee"],"venue":"2020 IEEE 2nd International Conference on Architecture, Construction, Environment and Hydraulics (ICACEH)","abstract":"The present research designed a tracking system of fish with 3D holographic augmented reality (AR). To track the fish, OpenCv through a camera with an algorithm was used. 3D holographic fish followed color marks that were used to track the fish. If the marks disappeared, 3D holographic fish stopped swimming. This process was repeated until the algorithm stopped. This experiment showed that 3D holographic fish followed the real fish.","paperId":"9c839820c273241159f15dc0efd28d7f534da842","paperTitle":"Realization of Robot Fish with 3D Hologram Fish using Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/9c839820c273241159f15dc0efd28d7f534da842","images":["https://d3i71xaburhd42.cloudfront.net/9c839820c273241159f15dc0efd28d7f534da842/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/9c839820c273241159f15dc0efd28d7f534da842/1-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/9c839820c273241159f15dc0efd28d7f534da842/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/9c839820c273241159f15dc0efd28d7f534da842/2-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/9c839820c273241159f15dc0efd28d7f534da842/2-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/9c839820c273241159f15dc0efd28d7f534da842/3-Figure8-1.png"],"pdf":"https://doi.org/10.1109/ICACEH51803.2020.9366226"},{"type":"inproceedings","key":"frank2016realizing","title":"Realizing Mixed-Reality Environments with Tablets for Intuitive Human-Robot Collaboration for Object Manipulation Tasks","author":"Frank, Jared A and Moorhead, Matthew and Kapila, Vikram","booktitle":"2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)","pages":"302--307","year":"2016","organization":"IEEE","doi":"10.1109/ROMAN.2016.7745146","authors":["Jared A Frank","Matthew Moorhead","Vikram Kapila"],"dblp":"conf/ro-man/FrankMK16","venue":"2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)","abstract":"Although gesture-based input and augmented reality (AR) facilitate intuitive human-robot interactions (HRI), prior implementations have relied on research-grade hardware and software. This paper explores using tablets to render mixed-reality visual environments that support human-robot collaboration for object manipulation. A mobile interface is created on a tablet by integrating real-time vision, 3D graphics, touchscreen interaction, and wireless communication. This mobile interface augments a live video of physical objects in a robot\'s workspace with corresponding virtual objects that can be manipulated by a user to intuitively command the robot to manipulate the physical objects. By generating the mixed-reality environment on an exocentric view provided by the tablet camera, the interface establishes a common frame of reference for the user and the robot to effectively communicate spatial information for object manipulation. After addressing challenges due to limitations in mobile sensing and computation, the interface is evaluated with participants to examine the performance and user experience with the suggested approach.","paperId":"2b9e902b210fc67b64730b0292069b0570256cc4","paperTitle":"Realizing mixed-reality environments with tablets for intuitive human-robot collaboration for object manipulation tasks","paperUrl":"https://www.semanticscholar.org/paper/2b9e902b210fc67b64730b0292069b0570256cc4","images":["https://d3i71xaburhd42.cloudfront.net/bbb43dd3eaa79a700b84766b3e3fdcfa3d86555b/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/bbb43dd3eaa79a700b84766b3e3fdcfa3d86555b/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/bbb43dd3eaa79a700b84766b3e3fdcfa3d86555b/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/bbb43dd3eaa79a700b84766b3e3fdcfa3d86555b/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/bbb43dd3eaa79a700b84766b3e3fdcfa3d86555b/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/bbb43dd3eaa79a700b84766b3e3fdcfa3d86555b/6-Figure6-1.png"],"pdf":"https://d-miller.github.io/DRK12/topic1/5505.pdf"},{"type":"article","key":"calife2009robot","title":"Robot Arena: An Augmented Reality Platform for Game Development","author":"Calife, Daniel and Bernardes Jr, Jo~ao Luiz and Tori, Romero","journal":"Computers in Entertainment (CIE)","volume":"7","number":"1","pages":"1--26","year":"2009","publisher":"ACM New York, NY, USA","doi":"10.1145/1486508.1486519","authors":["Daniel Calife","Jo~ao Luiz Bernardes Jr","Romero Tori"],"dblp":"journals/cie/CalifeBT09","venue":"CIE","abstract":"Nowadays electronic games are of great importance to the economic sector, to computing, and to academic research, and not are limited to entertainment applications only. Augmented reality is one of the new frontiers to be explored in the search for innovation in gameplay and in interactive interfaces of electronic games. This article presents the research, development, and testing of an infrastructure, called Robot Arena, for the development of innovative games using the spatial augmented reality and new interfaces that this technology brings. Based on horizontal interfaces with elements of hardware and software, Robot ARena has a flexible architecture that enables different ways of interaction and visualization. Another aspect planned for this project is the control and communication of robots, which can be used as avatars in games. A Robot ARena prototype was implemented, tested, and used in the development of two game prototypes, FootBot ARena and TanSpace, which bring new challenges to the infrastructure, such as exploration of the influence of virtual objects on real objects and the application of tangible interfaces.","paperId":"0731d6b09541bd777c62bc3a872a9a96c27b4dbe","paperTitle":"Robot Arena: An augmented reality platform for game development","paperUrl":"https://www.semanticscholar.org/paper/0731d6b09541bd777c62bc3a872a9a96c27b4dbe","images":["https://d3i71xaburhd42.cloudfront.net/0731d6b09541bd777c62bc3a872a9a96c27b4dbe/4-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/0731d6b09541bd777c62bc3a872a9a96c27b4dbe/5-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/0731d6b09541bd777c62bc3a872a9a96c27b4dbe/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/0731d6b09541bd777c62bc3a872a9a96c27b4dbe/10-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/0731d6b09541bd777c62bc3a872a9a96c27b4dbe/10-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/0731d6b09541bd777c62bc3a872a9a96c27b4dbe/12-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/0731d6b09541bd777c62bc3a872a9a96c27b4dbe/13-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/0731d6b09541bd777c62bc3a872a9a96c27b4dbe/13-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/0731d6b09541bd777c62bc3a872a9a96c27b4dbe/14-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/0731d6b09541bd777c62bc3a872a9a96c27b4dbe/14-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/0731d6b09541bd777c62bc3a872a9a96c27b4dbe/15-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/0731d6b09541bd777c62bc3a872a9a96c27b4dbe/15-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/0731d6b09541bd777c62bc3a872a9a96c27b4dbe/16-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/0731d6b09541bd777c62bc3a872a9a96c27b4dbe/17-Figure14-1.png","https://d3i71xaburhd42.cloudfront.net/0731d6b09541bd777c62bc3a872a9a96c27b4dbe/17-Figure15-1.png","https://d3i71xaburhd42.cloudfront.net/0731d6b09541bd777c62bc3a872a9a96c27b4dbe/18-Figure16-1.png","https://d3i71xaburhd42.cloudfront.net/0731d6b09541bd777c62bc3a872a9a96c27b4dbe/20-Figure17-1.png","https://d3i71xaburhd42.cloudfront.net/0731d6b09541bd777c62bc3a872a9a96c27b4dbe/21-Figure18-1.png","https://d3i71xaburhd42.cloudfront.net/0731d6b09541bd777c62bc3a872a9a96c27b4dbe/22-Figure19-1.png"],"pdf":"https://doi.org/10.1145/1486508.1486519"},{"type":"inproceedings","key":"von2016robot","title":"Robot Gardens: An Augmented Reality Prototype for Plant-Robot Biohybrid Systems","author":"von Mammen, Sebastian and Hamann, Heiko and Heider, Michael","booktitle":"Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology","pages":"139--142","year":"2016","doi":"10.1145/2993369.2993400","authors":["Sebastian von Mammen","Heiko Hamann","Michael Heider"],"dblp":"conf/vrst/MammenHH16","venue":"VRST","abstract":"Robot Gardens are an augmented reality concept allowing a human user to design a biohybrid, plant-robot system. Plants growing from deliberately placed seeds are directed by robotic units that the user can position, configure and activate. For example, the robotic units may serve as physical shields or frames but they may also guide the plants\' growth through emission of light. The biohybrid system evolves over time to redefine architectural spaces. This gives rise to the particular challenge of designing a biohybrid system before its actual implementation and potentially long before its developmental processes unfold. Here, an augmented reality interface featuring according simulation models of plants and robotic units allows one to explore the design space a priori. In this work, we present our first functional augmented reality prototype to design biohybrid systems. We provide details about its workings and elaborate on first empirical studies on its usability.","paperId":"8844dff4aec58c747b89a92b1373acd2d65e898a","paperTitle":"Robot gardens: an augmented reality prototype for plant-robot biohybrid systems","paperUrl":"https://www.semanticscholar.org/paper/8844dff4aec58c747b89a92b1373acd2d65e898a","images":["https://d3i71xaburhd42.cloudfront.net/8844dff4aec58c747b89a92b1373acd2d65e898a/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/8844dff4aec58c747b89a92b1373acd2d65e898a/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/8844dff4aec58c747b89a92b1373acd2d65e898a/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/8844dff4aec58c747b89a92b1373acd2d65e898a/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/8844dff4aec58c747b89a92b1373acd2d65e898a/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/8844dff4aec58c747b89a92b1373acd2d65e898a/4-Figure7-1.png"],"pdf":"http://vonmammen.org/publications/2016-VRST-RobotGardens.pdf"},{"type":"inproceedings","key":"sarai2017robot","title":"Robot Programming for Manipulators through Volume Sweeping and Augmented Reality","author":"Sarai, Yasumitsu and Maeda, Yusuke","booktitle":"2017 13th ieee conference on automation science and engineering (case)","pages":"302--307","year":"2017","organization":"IEEE","doi":"10.1109/COASE.2017.8256120","authors":["Yasumitsu Sarai","Yusuke Maeda"],"dblp":"conf/case/SaraiM17","venue":"2017 13th IEEE Conference on Automation Science and Engineering (CASE)","abstract":"Today\'s industrial robots require that human operators teach motions in advance. However, conventional methods for robot programming need deep knowledge and skills about robots or great effort for inputting information of working environment into computers. Therefore, a robot programming method in which everyone can easily teach robots \u201cgood\u201d motions is demanded. For this purpose, our group proposed a robot programming method that uses manual volume sweeping by operators and automatic motion planning together to generate motion plans with short cycle times. Because a swept volume is the space through which the robot has passed without collision, it is movable space of the robot that can be used in motion planning. In this paper, we proposed using augmented reality in this programming method. We constructed a system in which operators can perceive obtained swept volumes and generated paths intuitively through augmented reality. Teaching experiments showed that non-skilled operators can make a robot move in shorter time than teaching/playback by direct teaching.","paperId":"6e15e23cd49cb020329be84bd20c3b1e61bfdb26","paperTitle":"Robot programming for manipulators through volume sweeping and augmented reality","paperUrl":"https://www.semanticscholar.org/paper/6e15e23cd49cb020329be84bd20c3b1e61bfdb26","images":["https://d3i71xaburhd42.cloudfront.net/6e15e23cd49cb020329be84bd20c3b1e61bfdb26/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/6e15e23cd49cb020329be84bd20c3b1e61bfdb26/5-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/6e15e23cd49cb020329be84bd20c3b1e61bfdb26/5-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/6e15e23cd49cb020329be84bd20c3b1e61bfdb26/6-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/6e15e23cd49cb020329be84bd20c3b1e61bfdb26/6-Figure14-1.png","https://d3i71xaburhd42.cloudfront.net/6e15e23cd49cb020329be84bd20c3b1e61bfdb26/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/6e15e23cd49cb020329be84bd20c3b1e61bfdb26/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/6e15e23cd49cb020329be84bd20c3b1e61bfdb26/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/6e15e23cd49cb020329be84bd20c3b1e61bfdb26/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/6e15e23cd49cb020329be84bd20c3b1e61bfdb26/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/6e15e23cd49cb020329be84bd20c3b1e61bfdb26/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/6e15e23cd49cb020329be84bd20c3b1e61bfdb26/4-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/6e15e23cd49cb020329be84bd20c3b1e61bfdb26/5-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/6e15e23cd49cb020329be84bd20c3b1e61bfdb26/6-TableI-1.png"],"pdf":"https://doi.org/10.1109/COASE.2017.8256120"},{"type":"inproceedings","key":"fang2009robot","title":"Robot Programming Using Augmented Reality","author":"Fang, Hongchao and Ong, Soh Khim and Nee, Andrew Yeh-Ching","booktitle":"2009 International Conference on CyberWorlds","pages":"13--20","year":"2009","organization":"IEEE","doi":"10.1109/CW.2009.14","authors":["Hongchao Fang","Soh Khim Ong","Andrew Yeh-Ching Nee"],"dblp":"conf/vw/FangON09","venue":"2009 International Conference on CyberWorlds","abstract":"Human-robot interaction issues, especially for industrial robots, have largely been confined to finding better ways to reconfigure or program the robots. In this paper, an Augmented Reality based Robot Programming (RPAR-II) system is proposed. A virtual robot, which is a replicate of a real robot, is used in a real environment to perform and simulate the robot trajectory planning process. The RPAR-II system assists the users during the robot programming process from task planning to execution. Stereo vision-based methodologies for virtual objects registration as well as interactive device position tracking are employed in this system. Practical issues concerning the system implementation are discussed.","paperId":"4f50a54f544dc44721ad4b0377e1d89a96b981da","paperTitle":"Robot Programming Using Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/4f50a54f544dc44721ad4b0377e1d89a96b981da","images":["https://d3i71xaburhd42.cloudfront.net/4f50a54f544dc44721ad4b0377e1d89a96b981da/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/4f50a54f544dc44721ad4b0377e1d89a96b981da/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/4f50a54f544dc44721ad4b0377e1d89a96b981da/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/4f50a54f544dc44721ad4b0377e1d89a96b981da/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/4f50a54f544dc44721ad4b0377e1d89a96b981da/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/4f50a54f544dc44721ad4b0377e1d89a96b981da/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/4f50a54f544dc44721ad4b0377e1d89a96b981da/6-Figure7-1.png"],"pdf":"http://doi.ieeecomputersociety.org/10.1109/CW.2009.14"},{"type":"inproceedings","key":"ro2019projection","title":"Projection-Based Augmented Reality Robot Prototype with Human-Awareness","author":"Ro, Hyocheol and Byun, Jung-Hyun and Kim, Inhwan and Park, Yoon Jung and Kim, Kyuri and Han, Tack-Don","booktitle":"2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","pages":"598--599","year":"2019","organization":"IEEE","doi":"10.1109/HRI.2019.8673173","authors":["Hyocheol Ro","Jung-Hyun Byun","Inhwan Kim","Yoon Jung Park","Kyuri Kim","Tack-Don Han"],"dblp":"conf/hri/RoBKPKH19","venue":"2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","abstract":"Since projection augmented reality (AR) robot can provide a lot of information through projector, it can be useful in museums and art galleries that need to provide information to the crowd. Therefore, it is necessary to continue to interact with people, and human-aware path planning is also needed. We prototyped projection AR mobile robot implemented human-aware path planning and wrote about future research direction.","paperId":"b60d64e969988bf3caba7cf6b42ef458278e69ea","paperTitle":"Projection-Based Augmented Reality Robot Prototype with Human-Awareness","paperUrl":"https://www.semanticscholar.org/paper/b60d64e969988bf3caba7cf6b42ef458278e69ea","images":["https://d3i71xaburhd42.cloudfront.net/b60d64e969988bf3caba7cf6b42ef458278e69ea/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/b60d64e969988bf3caba7cf6b42ef458278e69ea/2-Figure2-1.png"],"pdf":"https://doi.org/10.1109/HRI.2019.8673173"},{"type":"inproceedings","key":"wen2012robot","title":"Robot-Assisted RF Ablation with Interactive Planning and Mixed Reality Guidance","author":"Wen, Rong and Chng, Chin-Boon and Chui, Chee-Kong and Lim, Kah-Bin and Ong, Sim-Heng and Chang, Stephen Kin-Yong","booktitle":"2012 IEEE/SICE International Symposium on System Integration (SII)","pages":"31--36","year":"2012","organization":"IEEE","doi":"10.1109/SII.2012.6426963","authors":["Rong Wen","Chin-Boon Chng","Chee-Kong Chui","Kah-Bin Lim","Sim-Heng Ong","Stephen Kin-Yong Chang"],"dblp":"conf/sii/WenCCLOC12","venue":"2012 IEEE/SICE International Symposium on System Integration (SII)","abstract":"Radiofrequency (RF) ablation delivered through interventional procedure is a good alternative to hepatic resection or liver transplant for treatment of liver tumor. However, inefficient RF needle navigation and limited thermal ablation region challenge surgeons\' manipulations. This paper presents robot-assisted RF ablation system for liver tumor treatment with interactive planning and mixed reality guidance. Interactive planning involves ablation model planning and surgeon\'s supervisory feedback via projector-based augmented reality (AR). The preoperatively defined surgical planning data is visualized directly on the patient body through the AR display, supplying information of insertion points, ablation points as well as preplanned trajectories, before execution by robot. The needle\'s spatial insertion is intraoperatively navigated by stereoscopic tracking and simulated in a model based virtual environment comprising anatomic and RF needle models. The robotic execution benefited from improved accuracy on needle\'s tip tracking and model based simulation. Ex vivo model and in vivo animal studies were conducted to validate and assess the performance of proposed mechanism. This solution is promising in overcoming current technological limitations and practical constraints of precise transcutaneous ablation therapy.","paperId":"80727be21422dabb9a65882771dd3bb0f56136f2","paperTitle":"Robot-assisted RF ablation with interactive planning and mixed reality guidance","paperUrl":"https://www.semanticscholar.org/paper/80727be21422dabb9a65882771dd3bb0f56136f2","images":["https://d3i71xaburhd42.cloudfront.net/80727be21422dabb9a65882771dd3bb0f56136f2/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/80727be21422dabb9a65882771dd3bb0f56136f2/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/80727be21422dabb9a65882771dd3bb0f56136f2/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/80727be21422dabb9a65882771dd3bb0f56136f2/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/80727be21422dabb9a65882771dd3bb0f56136f2/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/80727be21422dabb9a65882771dd3bb0f56136f2/5-Figure6-1.png"],"pdf":"https://doi.org/10.1109/SII.2012.6426963"},{"type":"inproceedings","key":"bolano2020planning","title":"Planning and Evaluation of Robotic Solutions in a Logistic Line through Augmented Reality","author":"Bolano, Gabriele and Roennau, Arne and Dillmann, Ruediger","booktitle":"2020 Fourth IEEE International Conference on Robotic Computing (IRC)","pages":"422--423","year":"2020","organization":"IEEE","doi":"10.1109/IRC.2020.00075","authors":["Gabriele Bolano","Arne Roennau","Ruediger Dillmann"],"dblp":"conf/irc/BolanoRD20","venue":"2020 Fourth IEEE International Conference on Robotic Computing (IRC)","abstract":"The planning and evaluation of robotics solutions in logistic environments is a time consuming process. It usually requires to plan on paper or with 2D CAD software the setup and flow of the parts in a line, with the need to buy and set up expensive equipment in order to test and validate the planned solution. Augmented Reality (AR) is a promising technology for these applications. The required equipment and parts can be simulated in order to evaluate the system beforehand, computing the resulting performance information automatically. The simulated hardware can be superimposed at the desired position in the real scenario, giving the user a more precise information on how the components in the setup will behave. In this work we propose an AR system to plan and configure a line consisting of a robot and a conveyor belt. The proposed system architecture allows to adapt the behavior of the existing hardware components accordingly to the changes introduced in the simulation. The performances of the picking task are computed by the system and visualized to the user, as well as the simulated objects. A GUI allows the worker to change and adjust the speed of the robotics components and the position and flow of the parts. An additional robot can be placed and visualized in order to evaluate the improvements in the line performance deploying additional equipment.","paperId":"1bcce425e7043e0750c351929e0d5ab0cb938396","paperTitle":"Planning and Evaluation of Robotic Solutions in a Logistic Line Through Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/1bcce425e7043e0750c351929e0d5ab0cb938396","images":["https://d3i71xaburhd42.cloudfront.net/1bcce425e7043e0750c351929e0d5ab0cb938396/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/1bcce425e7043e0750c351929e0d5ab0cb938396/2-Figure2-1.png"],"pdf":"https://doi.org/10.1109/IRC.2020.00075"},{"type":"inproceedings","key":"kobayashi2007overlay","title":"Overlay What Humanoid Robot Perceives and Thinks to the Real-World by Mixed Reality System","author":"Kobayashi, Kazuhiko and Nishiwaki, Koichi and Uchiyama, Shinji and Yamamoto, Hiroyuki and Kagami, Satoshi and Kanade, Takeo","booktitle":"2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality","pages":"275--276","year":"2007","organization":"IEEE","doi":"10.1109/ISMAR.2007.4538864","authors":["Kazuhiko Kobayashi","Koichi Nishiwaki","Shinji Uchiyama","Hiroyuki Yamamoto","Satoshi Kagami","Takeo Kanade"],"dblp":"conf/ismar/KobayashiNUYKK07","venue":"2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality","abstract":"One of the problems in developing a humanoid robot is caused by the fact that intermediate results, such as what the robot perceives the environment, and how it plans its moving path are hard to be observed online in the physical environment. What developers can see is only the behavior. Therefore, they usually investigate logged data afterwards, to analyze how well each component worked, or which component was wrong in the total system. In this paper, we present a novel environment for robot development, in which intermediate results of the system are overlaid on physical space using mixed reality technology. Real-time observation enables the developers to see intuitively, in what situation the specific intermediate results are generated, and to understand how results of a component affected the total system. This feature makes the development efficient and precise. This environment also gives a human-robot interface that shows the robot internal state intuitively, not only in development, but also in operation.","paperId":"3cffa1a18f52ab8d79b059b84ad049b5584a98d1","paperTitle":"Overlay what Humanoid Robot Perceives and Thinks to the Real-world by Mixed Reality System","paperUrl":"https://www.semanticscholar.org/paper/3cffa1a18f52ab8d79b059b84ad049b5584a98d1","images":["https://d3i71xaburhd42.cloudfront.net/3cffa1a18f52ab8d79b059b84ad049b5584a98d1/1-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/3cffa1a18f52ab8d79b059b84ad049b5584a98d1/2-Figure3-1.png"],"pdf":"http://doi.ieeecomputersociety.org/10.1109/ISMAR.2007.4538864"},{"type":"inproceedings","key":"wu2018omnidirectional","title":"Omnidirectional Mobile Robot Control Based on Mixed Reality and Semg Signals","author":"Wu, Mulun and Xu, Yanbin and Yang, Chenguang and Feng, Ying","booktitle":"2018 Chinese Automation Congress (CAC)","pages":"1867--1872","year":"2018","organization":"IEEE","doi":"10.1109/CAC.2018.8623114","authors":["Mulun Wu","Yanbin Xu","Chenguang Yang","Ying Feng"],"venue":"2018 Chinese Automation Congress (CAC)","abstract":"Robots are an important part of social production now, and their control methods have become an important research topic. There is a greater need for remote control and richer ways of interacting. In this paper, virtual reality and the surface electromyography(sEMG) signals are combined with the control of the omnidirectional mobile robot(OMR), and the programs of two devices, the HoloLens and the MYO armband, are respectively programmed and developed, so that they can work together to control the OMR. The basic control method is that HoloLens wearer controls the movement direction of the OMR through gesture or gaze. MYO armband wearer generate different sEMG signals through different gesture to control the velocity of the OMR. When two devices are used together, they can control all the motion of the OMR.","paperId":"13cbecd65547839af3b2b74782739ba037f62d8a","paperTitle":"Omnidirectional Mobile Robot Control based on Mixed Reality and sEMG Signals","paperUrl":"https://www.semanticscholar.org/paper/13cbecd65547839af3b2b74782739ba037f62d8a","images":["https://d3i71xaburhd42.cloudfront.net/13cbecd65547839af3b2b74782739ba037f62d8a/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/13cbecd65547839af3b2b74782739ba037f62d8a/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/13cbecd65547839af3b2b74782739ba037f62d8a/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/13cbecd65547839af3b2b74782739ba037f62d8a/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/13cbecd65547839af3b2b74782739ba037f62d8a/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/13cbecd65547839af3b2b74782739ba037f62d8a/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/13cbecd65547839af3b2b74782739ba037f62d8a/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/13cbecd65547839af3b2b74782739ba037f62d8a/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/13cbecd65547839af3b2b74782739ba037f62d8a/5-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/13cbecd65547839af3b2b74782739ba037f62d8a/5-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/13cbecd65547839af3b2b74782739ba037f62d8a/6-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/13cbecd65547839af3b2b74782739ba037f62d8a/6-Figure12-1.png"],"pdf":"https://doi.org/10.1109/CAC.2018.8623114"},{"type":"inproceedings","key":"liu2020mobile","title":"Mobile Delivery Robots: Mixed Reality-Based Simulation Relying on ROS and Unity 3D","author":"Liu, Yuzhou and Novotny, Georg and Smirnov, Nikita and Morales-Alvarez, Walter and Olaverri-Monreal, Cristina","booktitle":"2020 IEEE Intelligent Vehicles Symposium (IV)","pages":"15--20","year":"2020","organization":"IEEE","doi":"10.1109/IV47402.2020.9304701","authors":["Yuzhou Liu","Georg Novotny","Nikita Smirnov","Walter Morales-Alvarez","Cristina Olaverri-Monreal"],"dblp":"journals/corr/abs-2006-09002","venue":"2020 IEEE Intelligent Vehicles Symposium (IV)","abstract":"In the context of Intelligent Transportation Systems and the delivery of goods, new technology approaches need to be developed in order to cope with certain challenges that last mile delivery entails, such as navigation in an urban environment. Autonomous delivery robots can help overcome these challenges. We propose a method for performing mixed reality (MR) simulation with ROS-based robots using Unity, which synchronizes the real and virtual environment, and simultaneously uses the sensor information of the real robots to locate themselves and project them into the virtual environment, so that they can use their virtual doppelganger to perceive the virtual world. Using this method, real and virtual robots can perceive each other and the environment in which the other party is located, thereby enabling the exchange of information between virtual and real objects. Through this approach a more realistic and reliable simulation can be obtained. Results of the demonstrated use-cases verified the feasibility and efficiency as well as the stability of implementing MR using Unity for Robot Operating System (ROS)-based robots.","paperId":"4f4c4952fd0197d06e5740e10e08571294430678","paperTitle":"Mobile Delivery Robots: Mixed Reality-Based Simulation Relying on ROS and Unity 3D","paperUrl":"https://www.semanticscholar.org/paper/4f4c4952fd0197d06e5740e10e08571294430678","images":["https://d3i71xaburhd42.cloudfront.net/5773cdca5cf7f9f56721f5acc6f89d387bf36dda/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/5773cdca5cf7f9f56721f5acc6f89d387bf36dda/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/5773cdca5cf7f9f56721f5acc6f89d387bf36dda/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/5773cdca5cf7f9f56721f5acc6f89d387bf36dda/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/5773cdca5cf7f9f56721f5acc6f89d387bf36dda/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/5773cdca5cf7f9f56721f5acc6f89d387bf36dda/5-Figure6-1.png"],"pdf":"http://arxiv.org/pdf/2006.09002"},{"type":"inproceedings","key":"caruso2010robotic","title":"Robotic Arm for Car Dashboard Layout Assessment in Mixed Reality Environment","author":"Caruso, Giandomenico and Belluco, Paolo","booktitle":"19th International Symposium in Robot and Human Interactive Communication","pages":"62--68","year":"2010","organization":"IEEE","doi":"10.1109/ROMAN.2010.5598685","authors":["Giandomenico Caruso","Paolo Belluco"],"dblp":"conf/ro-man/CarusoB10","venue":"19th International Symposium in Robot and Human Interactive Communication","abstract":"Mixed Reality (MR) technologies allow us to create different environments, by merging real and virtual objects, and can be successfully used for the assessment of industrial products during the product development phases. This paper describes the integration of a MR environment including an industrial robotic arm for the ergonomics assessment of the driving seat in the automotive field. The robotic arm has been used to configure the MR environment automatically while guaranteeing the correct merging between real and virtual objects and the repeatability of the testing sessions. The user\'s presence, near an industrial robot, arises some issues on the safety and on the human-robot interaction that we have been addressed in order to improve the performing of the ergonomic test. Finally, the developed system has been validated through some testing sessions with end users to verify the effectiveness of our solution.","paperId":"2204ca92484fd1585696837ebdb48c6172c9c35a","paperTitle":"Robotic arm for car dashboard layout assessment in Mixed Reality environment","paperUrl":"https://www.semanticscholar.org/paper/2204ca92484fd1585696837ebdb48c6172c9c35a","images":["https://d3i71xaburhd42.cloudfront.net/2204ca92484fd1585696837ebdb48c6172c9c35a/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/2204ca92484fd1585696837ebdb48c6172c9c35a/4-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/2204ca92484fd1585696837ebdb48c6172c9c35a/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/2204ca92484fd1585696837ebdb48c6172c9c35a/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/2204ca92484fd1585696837ebdb48c6172c9c35a/5-Figure4-1.png"],"pdf":"https://doi.org/10.1109/ROMAN.2010.5598685"},{"type":"inproceedings","key":"arpaia2020robotic","title":"Robotic Autism Rehabilitation by Wearable Brain-Computer Interface and Augmented Reality","author":"Arpaia, Pasquale and Bravaccio, Carmela and Corrado, Giuseppina and Duraccio, Luigi and Moccaldi, Nicola and Rossi, Silvia","booktitle":"2020 IEEE International Symposium on Medical Measurements and Applications (MeMeA)","pages":"1--6","year":"2020","organization":"IEEE","doi":"10.1109/MeMeA49120.2020.9137144","authors":["Pasquale Arpaia","Carmela Bravaccio","Giuseppina Corrado","Luigi Duraccio","Nicola Moccaldi","Silvia Rossi"],"dblp":"conf/memea/ArpaiaBCDMR20","venue":"2020 IEEE International Symposium on Medical Measurements and Applications (MeMeA)","abstract":"An instrument based on the integration of Brain Computer Interface (BCI) and Augmented Reality (AR) is proposed for robotic autism rehabilitation. Flickering stimuli at fixed frequencies appear on the display of Augmented Reality (AR) glasses. When the user focuses on one of the stimuli a Steady State Visual Evoked Potentials (SSVEP) occurs on his occipital region. A single-channel electroencephalographic Brain Computer Interface detects the elicited SSVEP and sends the corresponding commands to a mobile robot. The device\u2019s high wearability (single channel and dry electrodes), and the trainingless usability are fundamental for the acceptance by Autism Spectrum Disorder (ASD) children. Effectively controlling the movements of a robot through a new channel enhances rehabilitation engagement and effectiveness. A case study at an accredited rehabilitation center on 10 healthy adult subjects highlighted an average accuracy higher than 83%. Preliminary further tests at the Department of Translational Medical Sciences of University of Naples Federico II on 3 ASD patients between 8 and 10 years old provided positive feedback on device acceptance and attentional performance.","paperId":"479ac1289b2dd4b60bca1e671b6eb35721d27502","paperTitle":"Robotic Autism Rehabilitation by Wearable Brain-Computer Interface and Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/479ac1289b2dd4b60bca1e671b6eb35721d27502","images":["https://d3i71xaburhd42.cloudfront.net/ce977f139ee043a9523f2af75710bca191dca23f/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/ce977f139ee043a9523f2af75710bca191dca23f/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/ce977f139ee043a9523f2af75710bca191dca23f/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/ce977f139ee043a9523f2af75710bca191dca23f/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/ce977f139ee043a9523f2af75710bca191dca23f/4-TableI-1.png","https://d3i71xaburhd42.cloudfront.net/ce977f139ee043a9523f2af75710bca191dca23f/4-TableII-1.png","https://d3i71xaburhd42.cloudfront.net/ce977f139ee043a9523f2af75710bca191dca23f/5-TableIII-1.png"],"pdf":"https://doi.org/10.1109/MeMeA49120.2020.9137144"},{"type":"inproceedings","key":"shimizu2008mixed","title":"Mixed Reality Robotic User Interface: Virtual Kinematics to Enhance Robot Motion","author":"Shimizu, Noriyoshi and Sugimoto, Maki and Sekiguchi, Dairoku and Hasegawa, Shoichi and Inami, Masahiko","booktitle":"Proceedings of the 2008 International Conference on Advances in Computer Entertainment Technology","pages":"166--169","year":"2008","doi":"10.1145/1501750.1501789","authors":["Noriyoshi Shimizu","Maki Sugimoto","Dairoku Sekiguchi","Shoichi Hasegawa","Masahiko Inami"],"dblp":"conf/ACMace/ShimizuSSHI08","venue":"ACE \'08","abstract":"A Robotic User Interface (RUI) is part of a concept in which a robot is used as an interface for human behavior. By combining the RUI with Mixed Reality (MR) technology, we propose a MR RUI system that enables the presentation of enhanced visual information of a robot existing in the real world. In this paper, we propose the virtual kinematics to enhance robot motion. A MR RUI system with virtual kinematics can present a selection of visual information by controlling the robot through physical simulation and by changing the parameter dynamically.","paperId":"10e226166592c7396620e7645d251be5911a0c21","paperTitle":"Mixed reality robotic user interface: virtual kinematics to enhance robot motion","paperUrl":"https://www.semanticscholar.org/paper/10e226166592c7396620e7645d251be5911a0c21","images":["https://d3i71xaburhd42.cloudfront.net/10e226166592c7396620e7645d251be5911a0c21/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/10e226166592c7396620e7645d251be5911a0c21/3-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/10e226166592c7396620e7645d251be5911a0c21/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/10e226166592c7396620e7645d251be5911a0c21/4-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/10e226166592c7396620e7645d251be5911a0c21/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/10e226166592c7396620e7645d251be5911a0c21/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/10e226166592c7396620e7645d251be5911a0c21/4-Figure5-1.png"],"pdf":"https://star.rcast.u-tokyo.ac.jp/old/content/files/pdf/conference/workshop/21.%20Mixed%20Reality%20Robotic%20User%20Interface.pdf"},{"type":"inproceedings","key":"lamberti2018robotquest","title":"Robotquest: A Robotic Game Based on Projected Mixed Reality and Proximity Interaction","author":"Lamberti, Fabrizio and Calandra, Davide and Bazzano, Federica and Prattico, Filippo G and Destefanis, Davide M","booktitle":"2018 IEEE Games, Entertainment, Media Conference (GEM)","pages":"1--9","year":"2018","organization":"IEEE","doi":"10.1109/GEM.2018.8516501","authors":["Fabrizio Lamberti","Davide Calandra","Federica Bazzano","Filippo G Prattico","Davide M Destefanis"],"dblp":"conf/gamesem/LambertiCBPD18","venue":"2018 IEEE Games, Entertainment, Media Conference (GEM)","abstract":"The appearance of video games in modern society introduced a number of modifications in the recreational and socialization habits of both youths and adults. In particular, various studies have associated the excessive use of this media with health and social problems, being the \u201cclassical\u201d video game often a sedentary and solitary activity. The purpose of this work is to propose a possible way to deal with the above issues, which consists in exploiting a platform for robotic gaming based on consumer hardware that is being developed with the aim to reintroduce the physical and social dimensions in digital games. The proposed solution encompasses a floor-projected Mixed Reality (MR) environment, an autonomous toy robot and a set of tangible interfaces created using proximity beacons, which are combined in a robotic game concept named RobotQuest that is meant to show how to favor an engaging room-scale interaction between players and real/virtual game elements.","paperId":"e0ebf934f8b90827da1f9c11f82f3fb62ba5db7c","paperTitle":"RobotQuest: A Robotic Game Based on Projected Mixed Reality and Proximity Interaction","paperUrl":"https://www.semanticscholar.org/paper/e0ebf934f8b90827da1f9c11f82f3fb62ba5db7c","images":["https://d3i71xaburhd42.cloudfront.net/e0ebf934f8b90827da1f9c11f82f3fb62ba5db7c/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/e0ebf934f8b90827da1f9c11f82f3fb62ba5db7c/4-Figure2-1.png"],"pdf":"https://doi.org/10.1109/GEM.2018.8516501"},{"type":"inproceedings","key":"nishiwaki2008mixed","title":"Mixed Reality Environment for Autonomous Robot Development","author":"Nishiwaki, Koichi and Kobayashi, Kazuhiko and Uchiyama, Shinji and Yamamoto, Hiroyuki and Kagami, Satoshi","booktitle":"2008 IEEE International Conference on Robotics and Automation","pages":"2211--2212","year":"2008","organization":"IEEE","doi":"10.1109/ROBOT.2008.4543538","authors":["Koichi Nishiwaki","Kazuhiko Kobayashi","Shinji Uchiyama","Hiroyuki Yamamoto","Satoshi Kagami"],"dblp":"conf/icra/NishiwakiKUYK08","venue":"2008 IEEE International Conference on Robotics and Automation","abstract":"This video demonstrates a mixed reality (MR) environment which is constructed for development of autonomous behaviors of robots. Many kinds of functions are required to be integrated for realizing an autonomous behavior. For example, autonomous navigation of humanoid robots needs functions, such as, recognition of environment, localization and mapping, path planning, gait planning, dynamically stable biped walking pattern generation, and sensor feedback stabilization of walking. Technologies to realize each function are well investigated by many research works. However, another effort is required for constructing an autonomous behavior by integrating those functions. We demonstrate a MR environment in which internal status of a robot, such as, sensor status, recognition results, planning results, and motion control parameters, can be projected to the environment and its body. We can understand intuitively how each function works as a part of total system in the real environment by using the proposed system, and it helps solving the integration problems. The overview of the system, projection of each internal status, and the application to an autonomous locomotion experiment are presented in the video clip.","paperId":"d2ca5ab53b75ced36d2190ef5c314a2a062c9431","paperTitle":"Mixed reality environment for autonomous robot development","paperUrl":"https://www.semanticscholar.org/paper/d2ca5ab53b75ced36d2190ef5c314a2a062c9431","images":["https://d3i71xaburhd42.cloudfront.net/d2ca5ab53b75ced36d2190ef5c314a2a062c9431/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/d2ca5ab53b75ced36d2190ef5c314a2a062c9431/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/d2ca5ab53b75ced36d2190ef5c314a2a062c9431/2-Figure4-1.png"],"pdf":"http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_ICRA_2008/Video%20Session/Mixed%20Reality%20Environment%20for%20Autonomous%20Robot%20Development.pdf"},{"type":"inproceedings","key":"jost2018safe","title":"Safe Human-Robot-Interaction in Highly Flexible Warehouses Using Augmented Reality and Heterogenous Fleet Management System","author":"Jost, Jana and Kirks, Thomas and Gupta, Preity and L\\"unsch, Dennis and Stenzel, Jonas","booktitle":"2018 IEEE International Conference on Intelligence and Safety for Robotics (ISR)","pages":"256--260","year":"2018","organization":"IEEE","doi":"10.1109/IISR.2018.8535808","authors":["Jana Jost","Thomas Kirks","Preity Gupta","Dennis L\\"unsch","Jonas Stenzel"],"dblp":"conf/isr2/JostKGLS18","venue":"2018 IEEE International Conference on Intelligence and Safety for Robotics (ISR)","abstract":"Nowadays, production systems and warehouses still lack human-robot collaboration. Often due to safety issues robots are kept behind safety fences and the overall system is shutdown once a human enters. This paper is based on a new concept to overcome these issues. It consists of a safety concept which ensures a more efficient human-robot collaboration. The human worker can be localized via a safety vest which he/she wears and the nearby robots are also aware of this information. We propose a more efficient and safe overall system by using an Augmented Reality device which offers a simple interface between humans and robots and a Heterogeneous Fleet Management System which plans and provides collision-free paths for humans and robots and ensures that the resulting environment is safe for both robots and humans.","paperId":"9400a9ad2f181af0cd4a45c2eaa20fd962562a8a","paperTitle":"Safe Human-Robot-Interaction in Highly Flexible Warehouses using Augmented Reality and Heterogenous Fleet Management System","paperUrl":"https://www.semanticscholar.org/paper/9400a9ad2f181af0cd4a45c2eaa20fd962562a8a","images":["https://d3i71xaburhd42.cloudfront.net/9400a9ad2f181af0cd4a45c2eaa20fd962562a8a/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/9400a9ad2f181af0cd4a45c2eaa20fd962562a8a/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/9400a9ad2f181af0cd4a45c2eaa20fd962562a8a/4-Figure3-1.png"],"pdf":"https://doi.org/10.1109/IISR.2018.8535808"},{"type":"inproceedings","key":"lambrecht2012spatial","title":"Spatial Programming for Industrial Robots Based on Gestures and Augmented Reality","author":"Lambrecht, Jens and Kr\\"uger, J\\"org","booktitle":"2012 IEEE/RSJ International Conference on Intelligent Robots and Systems","pages":"466--472","year":"2012","organization":"IEEE","doi":"10.1109/IROS.2012.6385900","authors":["Jens Lambrecht","J\\"org Kr\\"uger"],"dblp":"conf/iros/LambrechtK12","venue":"2012 IEEE/RSJ International Conference on Intelligent Robots and Systems","abstract":"The presented spatial programming system provides an assistance system for online programming of industrial robots. A handheld device and a motion tracking system establish the basis for a modular 3D programming approach corresponding to different phases of robot programming: definition, evaluation and adaption. Static and dynamic gestures enable the program definition of poses, trajectories and tasks. The spatial evaluation is done using an Augmented Reality application on a handheld device. Therefore, the programmer is able to move freely within the robot cell and define the program spatially through gestures. The camera image of the handheld is simultaneously enhanced by virtual objects representing the robot program. Based on 3D motion tracking of human movements and a mobile Augmented Reality application, we introduce a novel kind of interaction for the adaption of robot programs. The programmer is enabled to interact with virtual program components through bare-hand gestures. Such sample forms of interaction include translation and rotation applicable to poses, trajectories or tasks representations. Finally, the program is adapted according to the gestural changes and can be transferred from the handheld device directly to the robot controler.","paperId":"9c5b91fd81603f428dd2fbe19964726544a2aeca","paperTitle":"Spatial programming for industrial robots based on gestures and Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/9c5b91fd81603f428dd2fbe19964726544a2aeca","images":["https://d3i71xaburhd42.cloudfront.net/9c5b91fd81603f428dd2fbe19964726544a2aeca/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/9c5b91fd81603f428dd2fbe19964726544a2aeca/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/9c5b91fd81603f428dd2fbe19964726544a2aeca/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/9c5b91fd81603f428dd2fbe19964726544a2aeca/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/9c5b91fd81603f428dd2fbe19964726544a2aeca/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/9c5b91fd81603f428dd2fbe19964726544a2aeca/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/9c5b91fd81603f428dd2fbe19964726544a2aeca/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/9c5b91fd81603f428dd2fbe19964726544a2aeca/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/9c5b91fd81603f428dd2fbe19964726544a2aeca/5-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/9c5b91fd81603f428dd2fbe19964726544a2aeca/6-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/9c5b91fd81603f428dd2fbe19964726544a2aeca/6-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/9c5b91fd81603f428dd2fbe19964726544a2aeca/7-Figure12-1.png"],"pdf":"https://doi.org/10.1109/IROS.2012.6385900"},{"type":"inproceedings","key":"matu2014stereoscopic","title":"Stereoscopic Augmented Reality System for Supervised Training on Minimal Invasive Surgery Robots","author":"Matu, Florin Octavian and Thogersen, Mikkel and Galsgaard, Bo and Jensen, Martin Moller and Kraus, Martin","booktitle":"Proceedings of the 2014 Virtual Reality International Conference","pages":"1--4","year":"2014","doi":"10.1145/2617841.2620722","authors":["Florin Octavian Matu","Mikkel Thogersen","Bo Galsgaard","Martin Moller Jensen","Martin Kraus"],"dblp":"conf/vric/MatuTGJK14","venue":"VRIC","abstract":"Training in the use of robot-assisted surgery systems is necessary before a surgeon is able to perform procedures using these systems because the setup is very different from manual procedures. In addition, surgery robots are highly expensive to both acquire and maintain --- thereby entailing the need for efficient training. When training with the robot, the communication between the trainer and the trainee is limited, since the trainee often cannot see the trainer. To overcome this issue, this paper proposes an Augmented Reality (AR) system where the trainer is controlling two virtual robotic arms. These arms are virtually superimposed on the video feed to the trainee, and can therefore be used to demonstrate and perform various tasks for the trainee. Furthermore, the trainer is presented with a 3D image through a stereoscopic display. Because of the added depth perception, this enables the trainer to better guide and help the trainee. A prototype has been developed using low-cost materials and the system has been evaluated by surgeons at Aalborg University Hospital. User feedback indicated that a 3D display for the trainer is very useful as it enables the trainer to better monitor the procedure, and thereby enhances the training experience. The virtual overlay was also found to work as a good and illustrative approach for enhanced communication. However, the delay of the prototype made it difficult to use for actual training.","paperId":"2d21da621bb20620ee977baf9cac893dea9d3817","paperTitle":"Stereoscopic augmented reality system for supervised training on minimal invasive surgery robots","paperUrl":"https://www.semanticscholar.org/paper/2d21da621bb20620ee977baf9cac893dea9d3817","images":[],"pdf":"https://doi.org/10.1145/2617841.2620722"},{"type":"inproceedings","key":"linder2010luminar","title":"LuminAR: Portable Robotic Augmented Reality Interface Design and Prototype","author":"Linder, Natan and Maes, Pattie","booktitle":"Adjunct proceedings of the 23nd annual ACM symposium on User interface software and technology","pages":"395--396","year":"2010","doi":"10.1145/1866218.1866237","authors":["Natan Linder","Pattie Maes"],"dblp":"conf/uist/LinderM10","venue":"UIST \'10","abstract":"In this paper we introduce LuminAR: a prototype for a new portable and compact projector-camera system designed to use the traditional incandescent bulb interface as a power source, and a robotic desk lamp that carries it, enabling it with dynamic motion capabilities. We are exploring how the LuminAR system embodied in a familiar form factor of a classic Angle Poise lamp may evolve into a new class of robotic, digital information devices.","paperId":"c42a0ea5dbf059dce7321407dacca9a01a534097","paperTitle":"LuminAR: portable robotic augmented reality interface design and prototype","paperUrl":"https://www.semanticscholar.org/paper/c42a0ea5dbf059dce7321407dacca9a01a534097","images":["https://d3i71xaburhd42.cloudfront.net/c42a0ea5dbf059dce7321407dacca9a01a534097/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/c42a0ea5dbf059dce7321407dacca9a01a534097/3-Figure2-1.png"],"pdf":"https://dspace.mit.edu/bitstream/1721.1/66093/1/Maes_Luminar%20Portable.pdf"},{"type":"inproceedings","key":"ho2020supervised","title":"Supervised Control for Robot-Assisted Surgery Using Augmented Reality","author":"Ho, Tzu-Hsuan and Song, Kai-Tai","booktitle":"2020 20th International Conference on Control, Automation and Systems (ICCAS)","pages":"329--334","year":"2020","organization":"IEEE","doi":"10.23919/ICCAS50221.2020.9268278","authors":["Tzu-Hsuan Ho","Kai-Tai Song"],"venue":"2020 20th International Conference on Control, Automation and Systems (ICCAS)","abstract":"In this paper, we propose an AR-based robotic system that can plan and execute a trajectory based on a 3D medical model and allow the surgeon to supervise the execution of surgical process. In order to achieve image-guided surgery, a hand-eye calibration procedure is developed by using AprilTag to obtain the transformation between the workspace coordinate and the robot coordinate, and perform the image navigation task of the surgical robot to complete a drilling task. A procedure of robot supervised control is proposed to assign or modify the robot trajectory based on AR visualization and 3D model. A specified AR marker is used to project virtual objects in the AR glasses. We developed a robot registration algorithm to match the AR virtual coordinate system and the workspace coordinate system, and convert the planned trajectory into robot trajectory. Experimental results on the lab-built robotic system show that a user can adjust the position and orientation of the insertion point on a bone model, and transmit the trajectory information to the robot for execution. The proposed visualization-based robot navigation method has the potential to enhance the safety of surgical operation.","paperId":"8369c8225e2a98daac5590109e6601126ff4e603","paperTitle":"Supervised Control for Robot-Assisted Surgery Using Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/8369c8225e2a98daac5590109e6601126ff4e603","images":["https://d3i71xaburhd42.cloudfront.net/3632f944ef6ee9ae5b8c58de2c87a1e835fcc465/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/3632f944ef6ee9ae5b8c58de2c87a1e835fcc465/5-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/3632f944ef6ee9ae5b8c58de2c87a1e835fcc465/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/3632f944ef6ee9ae5b8c58de2c87a1e835fcc465/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/3632f944ef6ee9ae5b8c58de2c87a1e835fcc465/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/3632f944ef6ee9ae5b8c58de2c87a1e835fcc465/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/3632f944ef6ee9ae5b8c58de2c87a1e835fcc465/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/3632f944ef6ee9ae5b8c58de2c87a1e835fcc465/5-Figure8-1.png"],"pdf":"https://doi.org/10.23919/ICCAS50221.2020.9268278"},{"type":"inproceedings","key":"el2021teaching","title":"Teaching System for Multimodal Object Categorization by Human-Robot Interaction in Mixed Reality","author":"El Hafi, Lotfi and Nakamura, Hitoshi and Taniguchi, Akira and Hagiwara, Yoshinobu and Taniguchi, Tadahiro","booktitle":"2021 IEEE/SICE International Symposium on System Integration (SII)","pages":"320--324","year":"2021","organization":"IEEE","doi":"10.1109/IEEECONF49454.2021.9382607","authors":["Lotfi El Hafi","Hitoshi Nakamura","Akira Taniguchi","Yoshinobu Hagiwara","Tadahiro Taniguchi"],"dblp":"conf/sii/HafiNTHT21","venue":"2021 IEEE/SICE International Symposium on System Integration (SII)","abstract":"As service robots are becoming essential to support aging societies, teaching them how to perform general service tasks is still a major challenge preventing their deployment in daily-life environments. In addition, developing an artificial intelligence for general service tasks requires bottom-up, unsupervised approaches to let the robots learn from their own observations and interactions with the users. However, compared to the top-down, supervised approaches such as deep learning where the extent of the learning is directly related to the amount and variety of the pre-existing data provided to the robots, and thus relatively easy to understand from a human perspective, the learning status in bottom-up approaches is by their nature much harder to appreciate and visualize. To address these issues, we propose a teaching system for multimodal object categorization by human-robot interaction through Mixed Reality (MR) visualization. In particular, our proposed system enables a user to monitor and intervene in the robot\u2019s object categorization process based on Multimodal Latent Dirichlet Allocation (MLDA) to solve unexpected results and accelerate the learning. Our contribution is twofold by 1) describing the integration of a service robot, MR interactions, and MLDA object categorization in a unified system, and 2) proposing an MR user interface to teach robots through intuitive visualization and interactions.","paperId":"04ee958d57547856e654ccfabf1ce413a577ec83","paperTitle":"Teaching System for Multimodal Object Categorization by Human-Robot Interaction in Mixed Reality","paperUrl":"https://www.semanticscholar.org/paper/04ee958d57547856e654ccfabf1ce413a577ec83","images":["https://d3i71xaburhd42.cloudfront.net/04ee958d57547856e654ccfabf1ce413a577ec83/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/04ee958d57547856e654ccfabf1ce413a577ec83/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/04ee958d57547856e654ccfabf1ce413a577ec83/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/04ee958d57547856e654ccfabf1ce413a577ec83/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/04ee958d57547856e654ccfabf1ce413a577ec83/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/04ee958d57547856e654ccfabf1ce413a577ec83/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/04ee958d57547856e654ccfabf1ce413a577ec83/4-TableI-1.png"],"pdf":"https://doi.org/10.1109/IEEECONF49454.2021.9382607"},{"type":"article","key":"frank2017toward","title":"Toward Mobile Mixed-Reality Interaction with Multi-Robot Systems","author":"Frank, Jared Alan and Krishnamoorthy, Sai Prasanth and Kapila, Vikram","journal":"IEEE Robotics and Automation Letters","volume":"2","number":"4","pages":"1901--1908","year":"2017","publisher":"IEEE","doi":"10.1109/LRA.2017.2714128","authors":["Jared Alan Frank","Sai Prasanth Krishnamoorthy","Vikram Kapila"],"dblp":"journals/ral/FrankKK17","venue":"IEEE Robotics and Automation Letters","abstract":"Although human-multi-robot systems have received increased attention in recent years, current implementations rely on structured environments and utilize specialized, research-grade hardware to operate. This letter presents approaches that leverage the visual and inertial sensing of mobile devices to address the estimation and control challenges of multi-robot systems that function in shared spaces with human operators such that both the mobile device camera and robots can move freely in the environment. It is shown that a subset of robots in the system can be used to maintain a reference frame that facilitates tracking and control of the remaining robots to perform tasks, such as object retrieval, using an operator\'s mobile device as the only sensing and computational platform in the system. To evaluate the performance of the proposed approaches, experiments are conducted in which a system of mobile robots is commanded to retrieve objects in an environment. Results show that, compared to using the visual data alone, integrating both the visual and inertial data from mobile devices yields improvements in performance, flexibility, and computational efficiency in implementing human-multi-robot systems.","paperId":"474febfe2ac89d30b125f505a537e437ab92f0d5","paperTitle":"Toward Mobile Mixed-Reality Interaction With Multi-Robot Systems","paperUrl":"https://www.semanticscholar.org/paper/474febfe2ac89d30b125f505a537e437ab92f0d5","images":["https://d3i71xaburhd42.cloudfront.net/474febfe2ac89d30b125f505a537e437ab92f0d5/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/474febfe2ac89d30b125f505a537e437ab92f0d5/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/474febfe2ac89d30b125f505a537e437ab92f0d5/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/474febfe2ac89d30b125f505a537e437ab92f0d5/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/474febfe2ac89d30b125f505a537e437ab92f0d5/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/474febfe2ac89d30b125f505a537e437ab92f0d5/7-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/474febfe2ac89d30b125f505a537e437ab92f0d5/6-TableI-1.png"],"pdf":"https://doi.org/10.1109/lra.2017.2714128"},{"type":"inproceedings","key":"chan2020towards","title":"Towards a Multimodal System Combining Augmented Reality and Electromyography for Robot Trajectory Programming and Execution","author":"Chan, Wesley P and Sakr, Maram and Quintero, Camilo Perez and Croft, Elizabeth and Van der Loos, HF Machiel","booktitle":"2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","pages":"419--424","year":"2020","organization":"IEEE","doi":"10.1109/RO-MAN47096.2020.9223526","authors":["Wesley P Chan","Maram Sakr","Camilo Perez Quintero","Elizabeth Croft","HF Machiel Van der Loos"],"dblp":"conf/ro-man/ChanSQCL20","venue":"2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","abstract":"Programming and executing robot trajectories is a routine manufacturing procedure. However, current interfaces (i.e., teach pendants) are bulky, unintuitive, and interrupts task flow. Recently, augmented reality (AR) has been used to create alternative solutions. However, input modalities of such systems tend to be limited. By introducing the use of electromyography (EMG), we have created a novel multimodal wearable interface for online trajectory programming and execution. Through the use of EMG, our system aims to bridge the user\u2019s force activation to the robot arm force profile. Our proposed system provides two interaction methods for trajectory execution and force control using 1) arm EMG and 2) arm orientation. We compared these methods with a standard joystick in a user study to test their usability. Results show that proposed methods have increased physical demands but yield equivalent task performance, demonstrating the potential of our proposed interface to provide a wearable alternative solution.","paperId":"29c8fb308e6a0f1fd36eaf32c085ea4d36b00b64","paperTitle":"Towards a Multimodal System combining Augmented Reality and Electromyography for Robot Trajectory Programming and Execution","paperUrl":"https://www.semanticscholar.org/paper/29c8fb308e6a0f1fd36eaf32c085ea4d36b00b64","images":["https://d3i71xaburhd42.cloudfront.net/29c8fb308e6a0f1fd36eaf32c085ea4d36b00b64/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/29c8fb308e6a0f1fd36eaf32c085ea4d36b00b64/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/29c8fb308e6a0f1fd36eaf32c085ea4d36b00b64/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/29c8fb308e6a0f1fd36eaf32c085ea4d36b00b64/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/29c8fb308e6a0f1fd36eaf32c085ea4d36b00b64/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/29c8fb308e6a0f1fd36eaf32c085ea4d36b00b64/5-TableI-1.png"],"pdf":"https://doi.org/10.1109/RO-MAN47096.2020.9223526"},{"type":"inproceedings","key":"sita2017towards","title":"Towards Multimodal Interactions: Robot Jogging in Mixed Reality","author":"Sita, Enrico and Studley, Matthew and Dailami, Farid and Pipe, Anthony and Thomessen, Trygve","booktitle":"Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology","pages":"1--2","year":"2017","doi":"10.1145/3139131.3141200","authors":["Enrico Sita","Matthew Studley","Farid Dailami","Anthony Pipe","Trygve Thomessen"],"dblp":"conf/vrst/SitaSDPT17","venue":"VRST","abstract":"The recent progress made in the field of Augmented Reality/Mixed Reality (AR/MR) has opened new possibilities and approaches to research areas that can benefit from 3D visualization of digital content in the real world. In fact, human-robot interaction design and the design of user interfaces have very much to gain from MR technologies. Nonetheless, designing the user-robot interaction and processing multimodal feedbacks are very challenging tasks. In this paper we focus in particular on interactions in mixed reality. The main contribution of this paper is the implementation of a control system for an industrial manipulator through the user\'s interactions with MR content displayed with the Microsoft HoloLens. The system is based on the communication between Unity3D (used to design the user experience) and ROS, therefore extendible to any ROS-compatible robotic hardware.","paperId":"4caceb1258479ae9e744cbc8a0ed8838094bb0be","paperTitle":"Towards multimodal interactions: robot jogging in mixed reality","paperUrl":"https://www.semanticscholar.org/paper/4caceb1258479ae9e744cbc8a0ed8838094bb0be","images":["https://d3i71xaburhd42.cloudfront.net/4caceb1258479ae9e744cbc8a0ed8838094bb0be/2-Figure2-1.png"],"pdf":"http://eprints.uwe.ac.uk/33810/1/VRST-poster-SITA-final.pdf"},{"type":"inproceedings","key":"howard2012using","title":"Using Mixed Reality to Map Human Exercise Demonstrations to a Robot Exercise Coach","author":"Howard, Ayanna M and Roberts, Luke and Garcia, Sergio and Quarells, Rakale","booktitle":"2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)","pages":"291--292","year":"2012","organization":"IEEE","doi":"10.1109/ISMAR.2012.6402579","authors":["Ayanna M Howard","Luke Roberts","Sergio Garcia","Rakale Quarells"],"dblp":"conf/ismar/HowardRGQ12","venue":"2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)","abstract":"Obesity is a growing health problem in the United States, especially among children. Indicators show that the rate of obesity for children age 12-19 years old has risen from 5% percent to 18% over the last ten years. To deal with the obesity epidemic, a number of technology interventions, including the use of robotics and virtual reality games, have arisen to motivate youth to become physically active. The difficulty though lies in providing a tool for health professionals to embed established clinical health protocols into these technologies. As such, in this paper we present a mixed reality system that translates physical demonstrations of various exercise protocols into movements for a robotic agent. This is accomplished by mapping real-time data from an RGB-D sensor to a robotic exercise coach. Details of the system are discussed and results from evaluation with 20 human subjects are provided.","paperId":"6b40869aa014f8a5f7af272e091073fcb94043dc","paperTitle":"Using mixed reality to map human exercise demonstrations to a robot exercise coach","paperUrl":"https://www.semanticscholar.org/paper/6b40869aa014f8a5f7af272e091073fcb94043dc","images":["https://d3i71xaburhd42.cloudfront.net/6b40869aa014f8a5f7af272e091073fcb94043dc/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/6b40869aa014f8a5f7af272e091073fcb94043dc/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/6b40869aa014f8a5f7af272e091073fcb94043dc/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/6b40869aa014f8a5f7af272e091073fcb94043dc/2-Figure4-1.png"],"pdf":"http://doi.ieeecomputersociety.org/10.1109/ISMAR.2012.6402579"},{"type":"inproceedings","key":"hamilton2021s","title":"What\'s the Point? Tradeoffs between Effectiveness and Social Perception When Using Mixed Reality to Enhance Gesturally Limited Robots","author":"Hamilton, Jared and Phung, Thao and Tran, Nhan and Williams, Tom","booktitle":"Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction","pages":"177--186","year":"2021","doi":"10.1145/3434073.3444676","authors":["Jared Hamilton","Thao Phung","Nhan Tran","Tom Williams"],"dblp":"conf/hri/HamiltonPT021","venue":"HRI","abstract":"Mixed Reality visualizations provide a powerful new approach for enabling gestural capabilities on non-humanoid robots. This paper explores two different categories of mixed-reality deictic gestures for armless robots: a virtual arrow positioned over a target referent (a non-ego-sensitive allocentric gesture) and a virtual arm positioned over the gesturing robot (an ego-sensitive allocentric gesture). Specifically, we present the results of a within-subjects Mixed Reality HRI experiment (N=23) exploring the trade-offs between these two types of gestures with respect to both objective performance and subjective social perceptions. Our results show a clear trade-off between performance and social perception, with non-ego-sensitive allocentric gestures enabling faster reaction time and higher accuracy, but ego-sensitive gestures enabling higher perceived social presence, anthropomorphism, and likability.","paperId":"1820dee7d574a98b1bac0a3086e80a6012688eda","paperTitle":"What\'s The Point?: Tradeoffs Between Effectiveness and Social Perception When Using Mixed Reality to Enhance Gesturally Limited Robots","paperUrl":"https://www.semanticscholar.org/paper/1820dee7d574a98b1bac0a3086e80a6012688eda","images":["https://d3i71xaburhd42.cloudfront.net/1820dee7d574a98b1bac0a3086e80a6012688eda/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/1820dee7d574a98b1bac0a3086e80a6012688eda/5-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/1820dee7d574a98b1bac0a3086e80a6012688eda/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/1820dee7d574a98b1bac0a3086e80a6012688eda/7-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/1820dee7d574a98b1bac0a3086e80a6012688eda/7-Figure5-1.png"],"pdf":"https://doi.org/10.1145/3434073.3444676"},{"type":"article","key":"livatino20113","title":"3-D Integration of Robot Vision and Laser Data with Semiautomatic Calibration in Augmented Reality Stereoscopic Visual Interface","author":"Livatino, Salvatore and Banno, Filippo and Muscato, Giovanni","journal":"IEEE Transactions on Industrial Informatics","volume":"8","number":"1","pages":"69--77","year":"2011","publisher":"IEEE","doi":"10.1109/TII.2011.2174062","authors":["Salvatore Livatino","Filippo Banno","Giovanni Muscato"],"dblp":"journals/tii/LivatinoBM12","venue":"IEEE Transactions on Industrial Informatics","abstract":"This paper proposes an augmented reality visualization interface to simultaneously present visual and laser sensors information further enhanced by stereoscopic viewing and 3-D graphics. The use of graphic elements is proposed to represent laser measurements that are aligned to video information in 3-D space. This methodology enables an operator to intuitively comprehend scene layout and proximity information and so to respond in an accurate and timely manner. The use of graphic elements to assist teleoperation, sometime discussed in the literature, is here proposed following an innovative approach that aligns virtual and real objects in 3-D space and color them suitably to facilitate comprehension of objects proximity during navigation. This paper is developed based on authors\' previous experience on stereoscopic teleoperation. The approach is experimented on a real telerobotic system, where a user operates a mobile robot located several kilometers apart. The result showed simplicity and effectiveness of the proposed approach.","paperId":"d74ef922dd6c219dff8b20ddc1f4c1bfc7ac9e23","paperTitle":"3-D Integration of Robot Vision and Laser Data With Semiautomatic Calibration in Augmented Reality Stereoscopic Visual Interface","paperUrl":"https://www.semanticscholar.org/paper/d74ef922dd6c219dff8b20ddc1f4c1bfc7ac9e23","images":["https://d3i71xaburhd42.cloudfront.net/d74ef922dd6c219dff8b20ddc1f4c1bfc7ac9e23/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/d74ef922dd6c219dff8b20ddc1f4c1bfc7ac9e23/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/d74ef922dd6c219dff8b20ddc1f4c1bfc7ac9e23/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/d74ef922dd6c219dff8b20ddc1f4c1bfc7ac9e23/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/d74ef922dd6c219dff8b20ddc1f4c1bfc7ac9e23/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/d74ef922dd6c219dff8b20ddc1f4c1bfc7ac9e23/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/d74ef922dd6c219dff8b20ddc1f4c1bfc7ac9e23/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/d74ef922dd6c219dff8b20ddc1f4c1bfc7ac9e23/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/d74ef922dd6c219dff8b20ddc1f4c1bfc7ac9e23/5-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/d74ef922dd6c219dff8b20ddc1f4c1bfc7ac9e23/5-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/d74ef922dd6c219dff8b20ddc1f4c1bfc7ac9e23/6-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/d74ef922dd6c219dff8b20ddc1f4c1bfc7ac9e23/6-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/d74ef922dd6c219dff8b20ddc1f4c1bfc7ac9e23/6-Figure13-1.png"],"pdf":"https://doi.org/10.1109/TII.2011.2174062"},{"type":"inproceedings","key":"ng2010intuitive","title":"Intuitive Robot Tool Path Teaching Using Laser and Camera in Augmented Reality Environment","author":"Ng, Chuen Leong and Ng, Teck Chew and Nguyen, Thi Anh Ngoc and Yang, Guilin and Chen, Wenjie","booktitle":"2010 11th International Conference on Control Automation Robotics & Vision","pages":"114--119","year":"2010","organization":"IEEE","doi":"10.1109/ICARCV.2010.5707399","authors":["Chuen Leong Ng","Teck Chew Ng","Thi Anh Ngoc Nguyen","Guilin Yang","Wenjie Chen"],"dblp":"conf/icarcv/NgNNYC10","venue":"2010 11th International Conference on Control Automation Robotics & Vision","abstract":"This paper presents a new intuitive method for robot tool path teaching in Augmented Reality (AR) environment. Conventional industrial robot teaching method is long known to be either tedious or require a highly accurate virtual representation of robot work cell. Our method targets to provide the user with a fast and easy way of programming an industrial robot for useful tasks in a safe environment. In our system, a human robot interaction (HRI) system has been designed by fusing information from a camera and a laser ranger finder. The video images provide visual information to the user to operate the system, whereas the laser range finder captures the Cartesian information of the user intended robot working paths and trajectories. Furthermore, an AR environment has been designed where the virtual tool is superimposed onto the live video. The user simply needs to point and click on the image of the workpiece to generate the tool path. User can also adjust virtual tool orientation and simulate the tool trajectory in the AR environment, thus simplifying the robot teaching task. The proposed system has been tested for robot laser welding application. It is intuitive as no prior knowledge of robotic control is required in order to use our system. Most importantly, the system is safe and the user does not need to be physically close to the robot during path teaching.","paperId":"2e28fb40ac4c7dc8036691419f3d1bcd56bc13f8","paperTitle":"Intuitive robot tool path teaching using laser and camera in Augmented Reality environment","paperUrl":"https://www.semanticscholar.org/paper/2e28fb40ac4c7dc8036691419f3d1bcd56bc13f8","images":["https://d3i71xaburhd42.cloudfront.net/2e28fb40ac4c7dc8036691419f3d1bcd56bc13f8/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/2e28fb40ac4c7dc8036691419f3d1bcd56bc13f8/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/2e28fb40ac4c7dc8036691419f3d1bcd56bc13f8/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/2e28fb40ac4c7dc8036691419f3d1bcd56bc13f8/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/2e28fb40ac4c7dc8036691419f3d1bcd56bc13f8/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/2e28fb40ac4c7dc8036691419f3d1bcd56bc13f8/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/2e28fb40ac4c7dc8036691419f3d1bcd56bc13f8/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/2e28fb40ac4c7dc8036691419f3d1bcd56bc13f8/5-Figure8-1.png"],"pdf":"https://doi.org/10.1109/ICARCV.2010.5707399"},{"type":"article","key":"livatino2021intuitive","title":"Intuitive Robot Teleoperation through Multi-Sensor Informed Mixed Reality Visual Aids","author":"Livatino, Salvatore and Guastella, Dario C and Muscato, Giovanni and Rinaldi, Vincenzo and Cantelli, Luciano and Melita, Carmelo D and Caniglia, Alessandro and Mazza, Riccardo and Padula, Gianluca","journal":"IEEE Access","volume":"9","pages":"25795--25808","year":"2021","publisher":"IEEE","doi":"10.1109/ACCESS.2021.3057808","authors":["Salvatore Livatino","Dario C Guastella","Giovanni Muscato","Vincenzo Rinaldi","Luciano Cantelli","Carmelo D Melita","Alessandro Caniglia","Riccardo Mazza","Gianluca Padula"],"dblp":"journals/access/LivatinoGMRCMCM21","venue":"IEEE Access","abstract":"Mobile robotic systems have evolved to include sensors capable of truthfully describing robot status and operating environment as accurately and reliably as never before. This possibility is challenged by effective sensor data exploitation, because of the cognitive load an operator is exposed to, due to the large amount of data and time-dependency constraints. This paper addresses this challenge in remote-vehicle teleoperation by proposing an intuitive way to present sensor data to users by means of using mixed reality and visual aids within the user interface. We propose a method for organizing information presentation and a set of visual aids to facilitate visual communication of data in teleoperation control panels. The resulting sensor-information presentation appears coherent and intuitive, making it easier for an operator to catch and comprehend information meaning. This increases situational awareness and speeds up decision-making. Our method is implemented on a real mobile robotic system operating outdoor equipped with on-board internal and external sensors, GPS, and a reconstructed 3D graphical model provided by an assistant drone. Experimentation verified feasibility while intuitive and comprehensive visual communication was confirmed through an assessment, which encourages further developments.","paperId":"4a7e044244a42f200fb13b6be01cbecc21602e04","paperTitle":"Intuitive Robot Teleoperation Through Multi-Sensor Informed Mixed Reality Visual Aids","paperUrl":"https://www.semanticscholar.org/paper/4a7e044244a42f200fb13b6be01cbecc21602e04","images":["https://d3i71xaburhd42.cloudfront.net/4a7e044244a42f200fb13b6be01cbecc21602e04/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/4a7e044244a42f200fb13b6be01cbecc21602e04/4-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/4a7e044244a42f200fb13b6be01cbecc21602e04/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/4a7e044244a42f200fb13b6be01cbecc21602e04/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/4a7e044244a42f200fb13b6be01cbecc21602e04/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/4a7e044244a42f200fb13b6be01cbecc21602e04/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/4a7e044244a42f200fb13b6be01cbecc21602e04/7-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/4a7e044244a42f200fb13b6be01cbecc21602e04/7-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/4a7e044244a42f200fb13b6be01cbecc21602e04/7-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/4a7e044244a42f200fb13b6be01cbecc21602e04/8-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/4a7e044244a42f200fb13b6be01cbecc21602e04/8-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/4a7e044244a42f200fb13b6be01cbecc21602e04/10-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/4a7e044244a42f200fb13b6be01cbecc21602e04/10-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/4a7e044244a42f200fb13b6be01cbecc21602e04/11-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/4a7e044244a42f200fb13b6be01cbecc21602e04/11-Figure14-1.png"],"pdf":"https://ieeexplore.ieee.org/ielx7/6287639/9312710/09349454.pdf"},{"type":"inproceedings","key":"akan2011intuitive","title":"Intuitive Industrial Robot Programming through Incremental Multimodal Language and Augmented Reality","author":"Akan, Batu and Ameri, Afshin and C\\"ur\\"ukl\\"u, Baran and Asplund, Lars","booktitle":"2011 IEEE International Conference on Robotics and Automation","pages":"3934--3939","year":"2011","organization":"IEEE","doi":"10.1109/ICRA.2011.5979887","authors":["Batu Akan","Afshin Ameri","Baran C\\"ur\\"ukl\\"u","Lars Asplund"],"dblp":"conf/icra/AkanACA11","venue":"2011 IEEE International Conference on Robotics and Automation","abstract":"Developing easy to use, intuitive interfaces is crucial to introduce robotic automation to many small medium sized enterprises (SMEs). Due to their continuously changing product lines, reprogramming costs exceed installation costs by a large margin. In addition, traditional programming methods for industrial robots is too complex for an inexperienced robot programmer, thus external assistance is often needed. In this paper a new incremental multimodal language, which uses augmented reality (AR) environment, is presented. The proposed language architecture makes it possible to manipulate, pick or place the objects in the scene. This approach shifts the focus of industrial robot programming from coordinate based programming paradigm, to object based programming scheme. This makes it possible for non-experts to program the robot in an intuitive way, without going through rigorous training in robot programming.","paperId":"a01b6495239314c083892aafff0f38ff275ff256","paperTitle":"Intuitive industrial robot programming through incremental multimodal language and augmented reality","paperUrl":"https://www.semanticscholar.org/paper/a01b6495239314c083892aafff0f38ff275ff256","images":["https://d3i71xaburhd42.cloudfront.net/a01b6495239314c083892aafff0f38ff275ff256/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/a01b6495239314c083892aafff0f38ff275ff256/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/a01b6495239314c083892aafff0f38ff275ff256/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/a01b6495239314c083892aafff0f38ff275ff256/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/a01b6495239314c083892aafff0f38ff275ff256/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/a01b6495239314c083892aafff0f38ff275ff256/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/a01b6495239314c083892aafff0f38ff275ff256/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/a01b6495239314c083892aafff0f38ff275ff256/6-Figure8-1.png"],"pdf":"http://www.es.mdh.se/pdf_publications/1403.pdf"},{"type":"article","key":"hernandez2020increasing","title":"Increasing Robot Autonomy via Motion Planning and an Augmented Reality Interface","author":"Hern\'andez, Juan David and Sobti, Shlok and Sciola, Anthony and Moll, Mark and Kavraki, Lydia E","journal":"IEEE Robotics and Automation Letters","volume":"5","number":"2","pages":"1017--1023","year":"2020","publisher":"IEEE","doi":"10.1109/LRA.2020.2967280","authors":["Juan David Hern\'andez","Shlok Sobti","Anthony Sciola","Mark Moll","Lydia E Kavraki"],"dblp":"journals/ral/HernandezSSMK20","venue":"IEEE Robotics and Automation Letters","abstract":"Recently, there has been a growing interest in robotic systems that are able to share workspaces and collaborate with humans. Such collaborative scenarios require efficient mechanisms to communicate human requests to a robot, as well as to transmit robot interpretations and intents to humans. Recent advances in augmented reality (AR) technologies have provided an alternative for such communication. Nonetheless, most of the existing work in human-robot interaction with AR devices is still limited to robot motion programming or teleoperation. In this paper, we present an alternative approach to command and collaborate with robots. Our approach uses an AR interface that allows a user to specify high-level requests to a robot, to preview, approve or modify the computed robot motions. The proposed approach exploits the robot\'s decision-making capabilities instead of requiring low-level motion specifications provided by the user. The latter is achieved by using a motion planner that can deal with high-level goals corresponding to regions in the robot configuration space. We present a proof of concept to validate our approach in different test scenarios, and we present a discussion of its applicability in collaborative environments.","paperId":"9e40111484c846a01b8b0def69c495b928a7fa0e","paperTitle":"Increasing Robot Autonomy via Motion Planning and an Augmented Reality Interface","paperUrl":"https://www.semanticscholar.org/paper/9e40111484c846a01b8b0def69c495b928a7fa0e","images":["https://d3i71xaburhd42.cloudfront.net/9e40111484c846a01b8b0def69c495b928a7fa0e/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/9e40111484c846a01b8b0def69c495b928a7fa0e/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/9e40111484c846a01b8b0def69c495b928a7fa0e/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/9e40111484c846a01b8b0def69c495b928a7fa0e/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/9e40111484c846a01b8b0def69c495b928a7fa0e/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/9e40111484c846a01b8b0def69c495b928a7fa0e/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/9e40111484c846a01b8b0def69c495b928a7fa0e/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/9e40111484c846a01b8b0def69c495b928a7fa0e/6-Figure8-1.png"],"pdf":"https://orca.cardiff.ac.uk/138124/1/Hern%C3%A1ndez%20et%20al.%20-%202020%20-%20Increasing%20Robot%20Autonomy%20via%20Motion%20Planning%20and%20an%20Augmented%20Reality%20Interface.pdf"},{"type":"inproceedings","key":"rotsidis2019improving","title":"Improving Robot Transparency: An Investigation with Mobile Augmented Reality","author":"Rotsidis, Alexandros and Theodorou, Andreas and Bryson, Joanna J and Wortham, Robert H","booktitle":"2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","pages":"1--8","year":"2019","organization":"IEEE","doi":"10.1109/RO-MAN46459.2019.8956390","authors":["Alexandros Rotsidis","Andreas Theodorou","Joanna J Bryson","Robert H Wortham"],"dblp":"conf/ro-man/RotsidisTBW19","venue":"2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","abstract":"Autonomous robots can be difficult to understand by their developers, let alone by end users. Yet, as they become increasingly integral parts of our societies, the need for affordable easy to use tools to provide transparency grows. The rise of the smartphone and the improvements in mobile computing performance have gradually allowed Augmented Reality (AR) to become more mobile and affordable. In this paper we review relevant robot systems architecture and propose a new software tool to provide robot transparency through the use of AR technology. Our new tool, ABOD3-AR provides real-time graphical visualisation and debugging of a robot\u2019s goals and priorities as a means for both designers and end users to gain a better mental model of the internal state and decision making processes taking place within a robot. We also report on our on-going research programme and planned studies to further understand the effects of transparency to naive users and experts.","paperId":"65f48dd32575ae0118cfdbcdc77c1e30ff8547c1","paperTitle":"Improving Robot Transparency: An Investigation With Mobile Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/65f48dd32575ae0118cfdbcdc77c1e30ff8547c1","images":["https://d3i71xaburhd42.cloudfront.net/1d2a6b31c75a120ec01348266b7019f5626b1483/4-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/1d2a6b31c75a120ec01348266b7019f5626b1483/5-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/1d2a6b31c75a120ec01348266b7019f5626b1483/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/1d2a6b31c75a120ec01348266b7019f5626b1483/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/1d2a6b31c75a120ec01348266b7019f5626b1483/6-TableI-1.png","https://d3i71xaburhd42.cloudfront.net/1d2a6b31c75a120ec01348266b7019f5626b1483/7-TableII-1.png","https://d3i71xaburhd42.cloudfront.net/1d2a6b31c75a120ec01348266b7019f5626b1483/7-TableV-1.png"],"pdf":"http://www.cs.bath.ac.uk/~jjb/ftp/Rotsidis19.pdf"},{"type":"inproceedings","key":"choi2013haptic","title":"A Haptic Augmented Reality Surgeon Console for a Laparoscopic Surgery Robot System","author":"Choi, Seung Wook and Kim, Hee Chan and Kang, Heung Sik and Kim, Seongjun and Choi, Jaesoon","booktitle":"2013 13th International Conference on Control, Automation and Systems (ICCAS 2013)","pages":"355--357","year":"2013","organization":"IEEE","doi":"10.1109/ICCAS.2013.6703923","authors":["Seung Wook Choi","Hee Chan Kim","Heung Sik Kang","Seongjun Kim","Jaesoon Choi"],"venue":"2013 13th International Conference on Control, Automation and Systems (ICCAS 2013)","abstract":"Robot surgery needs measures for safety and dexterous control of the surgical instrument for wider application, in spite of evident clinical efficacy in diverse surgery areas. Integration of the advanced human machine interface technologies including haptic rendering and augmented reality in surgeon console for robot-assisted laparoscopic surgery to provide enhanced safety and easier system control has been tried in this study. The surgeon console is composed of various hardware and software modules for endoscope video signal capture, image/vision signal processing, 3D deformable model handling, haptic and graphic rendering, and interface to displays and haptic devices. Intra-operative endoscopic video signal is processed to extract information for the tracking of \u201cObject-Of-Interest (OOI)\u201ds such as anatomic structure that needs cautious handling, bleeding site and the relative position of the surgical instruments, and displayed with overlaid image of 3D-reconstructed preoperative medical imaging data. Parts of the extracted or user-defined OOIs can be transformed into a deformable 3D model and interactively manipulated by the surgeon during the operation for intuitive information utilization. The haptic rendering provides virtual force field experience for the surgeon to have safer handling of the surgical instruments and dexterous execution of surgical task. The surgeon console framework has been implemented on a PC integrated in a laparoscopic surgery robot system under development. The system showed successfully the feasibility of the concept and further development for enhanced usability and graphical contents quality is underway.","paperId":"13499c5f75b11ed9b89e03866466d5a93aaf3970","paperTitle":"A haptic augmented reality surgeon console for a laparoscopic surgery robot system","paperUrl":"https://www.semanticscholar.org/paper/13499c5f75b11ed9b89e03866466d5a93aaf3970","images":["https://d3i71xaburhd42.cloudfront.net/13499c5f75b11ed9b89e03866466d5a93aaf3970/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/13499c5f75b11ed9b89e03866466d5a93aaf3970/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/13499c5f75b11ed9b89e03866466d5a93aaf3970/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/13499c5f75b11ed9b89e03866466d5a93aaf3970/3-Figure4-1.png"],"pdf":"https://doi.org/10.1109/ICCAS.2013.6703923"},{"type":"inproceedings","key":"costa2015mixed","title":"A Mixed Reality Game Using 3Pi Robots\u2014\\"PiTanks\\"","author":"Costa, Hugo and Cebola, Peter and Cunha, Tiago and Sousa, Armando","booktitle":"2015 10th Iberian Conference on Information Systems and Technologies (CISTI)","pages":"1--6","year":"2015","organization":"IEEE","doi":"10.1109/CISTI.2015.7170600","authors":["Hugo Costa","Peter Cebola","Tiago Cunha","Armando Sousa"],"venue":"2015 10th Iberian Conference on Information Systems and Technologies (CISTI)","abstract":"In the growing field of Robotics, one of the many possible paths to explore is the social aspect that it can influence upon the present society. The combination of the goal-oriented development of robots with the interactivity used in games while employing mixed reality is a promising route to take in regard to designing user-friendly robots and improving problem solving featured in artificial intelligence software. In this paper, we present a competitive team-based game using Pololu\'s 3Pi robots moving in a projected map, capable of human interaction via game controllers. The game engine was developed utilizing the framework Qt Creator with C++ and OpenCV for the image processing tasks. The technical framework uses the ROS framework for communications that may be, in the future, used to connect different modules. Various parameters of the implementation are tested, such as position tracking errors.","paperId":"8364c869d01173356bf2427e3058e253acc13c22","paperTitle":"A mixed reality game using 3Pi robots \u2014 \u201cPiTanks\u201d","paperUrl":"https://www.semanticscholar.org/paper/8364c869d01173356bf2427e3058e253acc13c22","images":["https://d3i71xaburhd42.cloudfront.net/8364c869d01173356bf2427e3058e253acc13c22/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/8364c869d01173356bf2427e3058e253acc13c22/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/8364c869d01173356bf2427e3058e253acc13c22/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/8364c869d01173356bf2427e3058e253acc13c22/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/8364c869d01173356bf2427e3058e253acc13c22/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/8364c869d01173356bf2427e3058e253acc13c22/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/8364c869d01173356bf2427e3058e253acc13c22/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/8364c869d01173356bf2427e3058e253acc13c22/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/8364c869d01173356bf2427e3058e253acc13c22/5-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/8364c869d01173356bf2427e3058e253acc13c22/5-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/8364c869d01173356bf2427e3058e253acc13c22/6-Figure11-1.png"],"pdf":"https://doi.org/10.1109/CISTI.2015.7170600"},{"type":"article","key":"park2021hands","title":"Hands-Free Human--Robot Interaction Using Multimodal Gestures and Deep Learning in Wearable Mixed Reality","author":"Park, Kyeong-Beom and Choi, Sung Ho and Lee, Jae Yeol and Ghasemi, Yalda and Mohammed, Mustafa and Jeong, Heejin","journal":"IEEE Access","volume":"9","pages":"55448--55464","year":"2021","publisher":"IEEE","doi":"10.1109/ACCESS.2021.3071364","authors":["Kyeong-Beom Park","Sung Ho Choi","Jae Yeol Lee","Yalda Ghasemi","Mustafa Mohammed","Heejin Jeong"],"dblp":"journals/access/ParkCLGMJ21","venue":"IEEE Access","abstract":"This study proposes a novel hands-free interaction method using multimodal gestures such as eye gazing and head gestures and deep learning for human-robot interaction (HRI) in mixed reality (MR) environments. Since human operators hold some objects for conducting tasks, there are many constrained situations where they cannot use their hands for HRI interactions. To provide more effective and intuitive task assistance, the proposed hands-free method supports coarse-to-fine interactions. Eye gazing-based interaction is used for coarse interactions such as searching and previewing of target objects, and head gesture interactions are used for fine interactions such as selection and 3D manipulation. In addition, deep learning-based object detection is applied to estimate the initial positioning of physical objects to be manipulated by the robot. The result of object detection is then combined with 3D spatial mapping in the MR environment for supporting accurate initial object positioning. Furthermore, virtual object-based indirect manipulation is proposed to support more intuitive and efficient control of the robot, compared with traditional direct manipulation (e.g., joint-based and end effector-based manipulations). In particular, a digital twin, the synchronized virtual robot of the real robot, is used to provide a preview and simulation of the real robot to manipulate it more effectively and accurately. Two case studies were conducted to confirm the originality and advantages of the proposed hands-free HRI: (1) performance evaluation of initial object positioning and (2) comparative analysis with traditional direct robot manipulations. The deep learning-based initial positioning reduces much effort for robot manipulation using eye gazing and head gestures. The object-based indirect manipulation also supports more effective HRI than previous direct interaction methods.","paperId":"01c08d9f95009b393f21c757f12468ebbcb0b061","paperTitle":"Hands-Free Human\u2013Robot Interaction Using Multimodal Gestures and Deep Learning in Wearable Mixed Reality","paperUrl":"https://www.semanticscholar.org/paper/01c08d9f95009b393f21c757f12468ebbcb0b061","images":["https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/5-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/9-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/6-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/12-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/7-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/15-Table3-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/7-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/8-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/9-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/9-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/10-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/10-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/10-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/11-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/11-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/12-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/12-Figure14-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/12-Figure15-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/13-Figure16-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/13-Figure17-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/14-Figure18-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/14-Figure19-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/14-Figure20-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/14-Figure21-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/15-Figure22-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/15-Figure23-1.png","https://d3i71xaburhd42.cloudfront.net/01c08d9f95009b393f21c757f12468ebbcb0b061/15-Figure24-1.png"],"pdf":"https://ieeexplore.ieee.org/ielx7/6287639/9312710/09395580.pdf"},{"type":"inproceedings","key":"fong2019robot","title":"A Robot with an Augmented-Reality Display for Functional Capacity Evaluation and Rehabilitation of Injured Workers","author":"Fong, Jason and Ocampo, Renz and Gross, Douglas P and Tavakoli, Mahdi","booktitle":"2019 IEEE 16th International Conference on Rehabilitation Robotics (ICORR)","pages":"181--186","year":"2019","organization":"IEEE","doi":"10.1109/ICORR.2019.8779417","authors":["Jason Fong","Renz Ocampo","Douglas P Gross","Mahdi Tavakoli"],"dblp":"conf/icorr/FongOGT19","venue":"2019 IEEE 16th International Conference on Rehabilitation Robotics (ICORR)","abstract":"Occupational rehabilitation is an integral part of the recovery process for workers who have sustained injuries at the workplace. It often requires the injured worker to engage in functional tasks that simulate the workplace environment to help regain their functional capabilities and allow for a return to employment. We present a system comprised of a robotic arm for recreating the physical dynamics of functional tasks and a 3D Augmented Reality (AR) display for immersive visualization of the tasks. While this system can be used to simulate a multitude of occupational tasks, we focus on one specific functional task. Participants perform a virtual version of the task using the robot-AR system, and a physical version of the same task without the system. This study shows the results for two able-bodied users to determine if the robot-AR system produces upper-limb movements similar to the real-life equivalent task. The similarity between relative joint positions, i.e., hand-to-elbow (H2E) and elbow-to-shoulder (E2S) displacements, is evaluated within clusters based on the spatial position of the user\u2019s hand. The H2E displacements for approximately 50% of hand position clusters were consistent between the robot-AR and real-world conditions and approximately 30% for E2S displacements. The similar clusters are distributed across the entire task space however, indicating the robot-AR system has the potential to properly simulate real-world equivalent tasks.","paperId":"8e7de1dd89017f4222e05e33d398ce6bbac72b1e","paperTitle":"A Robot with an Augmented-Reality Display for Functional Capacity Evaluation and Rehabilitation of Injured Workers","paperUrl":"https://www.semanticscholar.org/paper/8e7de1dd89017f4222e05e33d398ce6bbac72b1e","images":["https://d3i71xaburhd42.cloudfront.net/8e7de1dd89017f4222e05e33d398ce6bbac72b1e/4-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/8e7de1dd89017f4222e05e33d398ce6bbac72b1e/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/8e7de1dd89017f4222e05e33d398ce6bbac72b1e/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/8e7de1dd89017f4222e05e33d398ce6bbac72b1e/5-Figure4-1.png"],"pdf":"http://www.ece.ualberta.ca/~tbs/pmwiki/pdf/ICORR-Fong-2019.pdf"},{"type":"inproceedings","key":"luxenburger2019augmented","title":"Augmented Reality for Human-Robot Cooperation in Aircraft Assembly","author":"Luxenburger, Andreas and Mohr, Jonas and Spieldenner, Torsten and Merkel, Dieter and Espinosa, Fabio and Schwartz, Tim and Reinicke, Florian and Ahlers, Julian and Stoyke, Markus","booktitle":"2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)","pages":"263--2633","year":"2019","organization":"IEEE","doi":"10.1109/AIVR46125.2019.00061","authors":["Andreas Luxenburger","Jonas Mohr","Torsten Spieldenner","Dieter Merkel","Fabio Espinosa","Tim Schwartz","Florian Reinicke","Julian Ahlers","Markus Stoyke"],"dblp":"conf/aivr/LuxenburgerMSME19","venue":"2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)","abstract":"Augmented Reality (AR) is often discussed as one of the enabling technologies in Industrie 4.0. In this paper, we describe a practical application, where Augmented Reality glasses are used not only for assembly assistance, but also as a means of communication to enable the orchestration of a hybrid team consisting of a human worker and two mobile robotic systems. The task of the hybrid team is to rivet so-called stringers onto an aircraft hull. While the two robots do the physically demanding, unergonomic and possibly hazardous tasks (squeezing and sealing rivets), the human takes over those responsibilities that need experience, multi-sensory sensitiveness and specialist knowledge. We describe the working scenario, the overall architecture and give design and implementation details on the AR application.","paperId":"f0d46fe1c16b78746b9279c9fb657d16ce9363a9","paperTitle":"Augmented Reality for Human-Robot Cooperation in Aircraft Assembly","paperUrl":"https://www.semanticscholar.org/paper/f0d46fe1c16b78746b9279c9fb657d16ce9363a9","images":["https://d3i71xaburhd42.cloudfront.net/f0d46fe1c16b78746b9279c9fb657d16ce9363a9/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/f0d46fe1c16b78746b9279c9fb657d16ce9363a9/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/f0d46fe1c16b78746b9279c9fb657d16ce9363a9/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/f0d46fe1c16b78746b9279c9fb657d16ce9363a9/4-Figure4-1.png"],"pdf":"https://doi.org/10.1109/AIVR46125.2019.00061"},{"type":"inproceedings","key":"hartmann2020aar","title":"AAR: Augmenting a Wearable Augmented Reality Display with an Actuated Head-Mounted Projector","author":"Hartmann, Jeremy and Yeh, Yen-Ting and Vogel, Daniel","booktitle":"Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology","pages":"445--458","year":"2020","doi":"10.1145/3379337.3415849","authors":["Jeremy Hartmann","Yen-Ting Yeh","Daniel Vogel"],"dblp":"conf/uist/HartmannY020","venue":"UIST","abstract":"Current wearable AR devices create an isolated experience with a limited field of view, vergence-accommodation conflicts, and difficulty communicating the virtual environment to observers. To address these issues and enable new ways to visualize, manipulate, and share virtual content, we introduce Augmented Augmented Reality (AAR) by combining a wearable AR display with a wearable spatial augmented reality projector. To explore this idea, a system is constructed to combine a head-mounted actuated pico projector with a Hololens AR headset. Projector calibration uses a modified structure from motion pipeline to reconstruct the geometric structure of the pan-tilt actuator axes and offsets. A toolkit encapsulates a set of high-level functionality to manage content placement relative to each augmented display and the physical environment. Demonstrations showcase ways to utilize the projected and head-mounted displays together, such as expanding field of view, distributing content across depth surfaces, and enabling bystander collaboration.","paperId":"d5e7ef9baa7e27d05fa5af5afe3c318cd368d016","paperTitle":"AAR: Augmenting a Wearable Augmented Reality Display with an Actuated Head-Mounted Projector","paperUrl":"https://www.semanticscholar.org/paper/d5e7ef9baa7e27d05fa5af5afe3c318cd368d016","images":["https://d3i71xaburhd42.cloudfront.net/d5e7ef9baa7e27d05fa5af5afe3c318cd368d016/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/d5e7ef9baa7e27d05fa5af5afe3c318cd368d016/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/d5e7ef9baa7e27d05fa5af5afe3c318cd368d016/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/d5e7ef9baa7e27d05fa5af5afe3c318cd368d016/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/d5e7ef9baa7e27d05fa5af5afe3c318cd368d016/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/d5e7ef9baa7e27d05fa5af5afe3c318cd368d016/7-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/d5e7ef9baa7e27d05fa5af5afe3c318cd368d016/7-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/d5e7ef9baa7e27d05fa5af5afe3c318cd368d016/8-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/d5e7ef9baa7e27d05fa5af5afe3c318cd368d016/8-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/d5e7ef9baa7e27d05fa5af5afe3c318cd368d016/9-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/d5e7ef9baa7e27d05fa5af5afe3c318cd368d016/9-Figure12-1.png"],"pdf":"https://jjhartmann.github.io/AugmentedAugmentedReality/assets/Hartmann_AugmentedAugmentedReality_UIST2020.pdf"},{"type":"inproceedings","key":"corotan2019indoor","title":"An Indoor Navigation Robot Using Augmented Reality","author":"Corotan, Austin and Irgen-Gioro, Jianna Jian Zhang","booktitle":"2019 5th International Conference on Control, Automation and Robotics (ICCAR)","pages":"111--116","year":"2019","organization":"IEEE","doi":"10.1109/ICCAR.2019.8813348","authors":["Austin Corotan","Jianna Jian Zhang Irgen-Gioro"],"venue":"2019 5th International Conference on Control, Automation and Robotics (ICCAR)","abstract":"In this paper, we address the issue of autonomous robotic navigation in an indoor environment. An Augmented Reality (AR) based navigation system is created using the JAQL robot and an Google Pixel 2 Android smart phone. In developing this system, we aim to investigate the current capability of augmented reality as an all-in-one solution to indoor routing, localization, and object detection. Using Google\'s ARcore, we have developed a mobile application, which was used as both the sensor and the controller for the system. We highlight the features of ARcore and how they were applied to each of our navigation tasks. We evaluate the effectiveness of our system and present results from a set of experiments in a real environment under varying conditions. We conclude by discussing the strengths and limitations of our system and how we can make improvements in the future.","paperId":"bafa5db7e4db1d9fd2caa4ab710a4d74c1df472b","paperTitle":"An Indoor Navigation Robot Using Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/bafa5db7e4db1d9fd2caa4ab710a4d74c1df472b","images":["https://d3i71xaburhd42.cloudfront.net/bafa5db7e4db1d9fd2caa4ab710a4d74c1df472b/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/bafa5db7e4db1d9fd2caa4ab710a4d74c1df472b/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/bafa5db7e4db1d9fd2caa4ab710a4d74c1df472b/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/bafa5db7e4db1d9fd2caa4ab710a4d74c1df472b/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/bafa5db7e4db1d9fd2caa4ab710a4d74c1df472b/5-TableI-1.png","https://d3i71xaburhd42.cloudfront.net/bafa5db7e4db1d9fd2caa4ab710a4d74c1df472b/6-TableII-1.png"],"pdf":"https://doi.org/10.1109/ICCAR.2019.8813348"},{"type":"inproceedings","key":"hiroi2010evaluation","title":"Evaluation of Head Size of an Interactive Robot Using an Augmented Reality","author":"Hiroi, Yutaka and Hisano, Shuhei and Ito, Akinori","booktitle":"2010 World Automation Congress","pages":"1--6","year":"2010","organization":"IEEE","authors":["Yutaka Hiroi","Shuhei Hisano","Akinori Ito"],"venue":"2010 World Automation Congress","abstract":"In this paper, we propose a design methodology of robots based on augmented reality (AR). While robot design based on subjective evaluation is useful, the problem of subject-based design is that the developer should prepare all variation of robots to be evaluated. Using AR, a developer can prepare robots using computer graphics. In addition, the AR technology makes it possible to evaluate a robot in an environment in which the robot is supposed to work. We conducted experiments to evaluate a robot\'s head size using both AR and real robots, and compared the evaluation results. As a result, similar evaluation results were obtained from both evaluation experiments, which showed a possibility of AR-based robot evaluation.","paperId":"789fcd5dcb6a1ea069960e9b8bda154e67a203b6","paperTitle":"Evaluation of head size of an interactive robot using an augmented reality","paperUrl":"https://www.semanticscholar.org/paper/789fcd5dcb6a1ea069960e9b8bda154e67a203b6","images":["https://d3i71xaburhd42.cloudfront.net/789fcd5dcb6a1ea069960e9b8bda154e67a203b6/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/789fcd5dcb6a1ea069960e9b8bda154e67a203b6/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/789fcd5dcb6a1ea069960e9b8bda154e67a203b6/5-Figure9-1.png"],"pdf":null},{"type":"inproceedings","key":"chou2004augmented","title":"Augmented Reality Based Preoperative Planning for Robot Assisted Tele-Neurosurgery","author":"Chou, Wusheng and Wang, Tianmiao and Zhang, Yuru","booktitle":"2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No. 04CH37583)","volume":"3","pages":"2901--2906","year":"2004","organization":"IEEE","doi":"10.1109/ICSMC.2004.1400773","authors":["Wusheng Chou","Tianmiao Wang","Yuru Zhang"],"dblp":"conf/smc/ChouWZ04","venue":"2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)","abstract":"Robot assisted telesurgery allow the surgeon to deliver the neurosurgical expertise to remote sites by remotely controlling medical robot to do minimally invasive surgery. An augmented reality based preoperative planning method is proposed to improve the safety of tele-neurosurgery. The position of four marks, which are attached on the head of patient and are used for surgeon to do preliminary preoperative planning with the support of sophisticated navigation software, is obtained by stereovision. Besides of the establishment of bilateral audio, video and data communications, the virtual simulation environment is calibrated and overlapped with the real live images. Using the augmented reality, the surgeon at remote site can conduct virtual surgery to find and verify the most appropriate and safest planning and then deliver his planning to surgery site. This method has been successfully applied in clinical cases of tele-neurosurgery.","paperId":"b582b32f72ec29563823b15707ac1c72e8c31693","paperTitle":"Augmented reality based preoperative planning for robot assisted tele-neurosurgery","paperUrl":"https://www.semanticscholar.org/paper/b582b32f72ec29563823b15707ac1c72e8c31693","images":["https://d3i71xaburhd42.cloudfront.net/b582b32f72ec29563823b15707ac1c72e8c31693/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/b582b32f72ec29563823b15707ac1c72e8c31693/5-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/b582b32f72ec29563823b15707ac1c72e8c31693/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/b582b32f72ec29563823b15707ac1c72e8c31693/5-FigureI-1.png"],"pdf":"https://doi.org/10.1109/ICSMC.2004.1400773"},{"type":"inproceedings","key":"genccturk2019development","title":"Development of Augmented Reality Based Mobile Robot Maintenance Software","author":"GEN\xc7T\\"URK, Hakan and YAYAN, U\u011fur","booktitle":"2019 Innovations in Intelligent Systems and Applications Conference (ASYU)","pages":"1--5","organization":"IEEE","doi":"10.1109/ASYU48272.2019.8946359","authors":["Hakan GEN\xc7T\\"URK","U\u011fur YAYAN"],"venue":"2019 Innovations in Intelligent Systems and Applications Conference (ASYU)","abstract":"Augmented Reality (AR) helps to increase the perception of reality by adding digital data to the real world. Nowadays, AR is used effectively in many areas from entertainment sectors to health and education applications. In this study, a mobile application has been developed by using Augmented Reality technology for maintenance and repair of the mobile robot (evarobot). In this study, battery, motor, sensor and electronic card replacement scenarios of Evarobot and maintenance and repair steps were determined. These steps are supported with visuals in accordance with the instructions given and are intended to enable the user to carry out the maintenance process easily without the need for an expert or maintenance document. At the end of the study, the digitization of the experience and the transfer of the information without getting lost has been realized in an easy way.","paperId":"58f159cc268da7794a119c34b4e5ddd4a9ffe46c","paperTitle":"Development of Augmented Reality Based Mobile Robot Maintenance Software","paperUrl":"https://www.semanticscholar.org/paper/58f159cc268da7794a119c34b4e5ddd4a9ffe46c","images":[],"pdf":"https://doi.org/10.1109/ASYU48272.2019.8946359"},{"type":"inproceedings","key":"mohareri2011autonomous","title":"Autonomous Humanoid Robot Navigation Using Augmented Reality Technique","author":"Mohareri, Omid and Rad, Ahmad B","booktitle":"2011 IEEE International Conference on Mechatronics","pages":"463--468","year":"2011","organization":"IEEE","doi":"10.1109/ICMECH.2011.5971330","authors":["Omid Mohareri","Ahmad B Rad"],"venue":"2011 IEEE International Conference on Mechatronics","abstract":"This work presents a novel vision-based navigation strategy for autonomous humanoid robots using augmented reality (AR). In the first stage, a platform is developed for indoor and outdoor human location positioning and navigation using mobile augmented reality. The image sequence would be obtained by a smart phone\'s camera and the location information will be provided to the user in the form of 3D graphics and audio effects containing location information. To recognize a location, an image database and location model is pre-constructed to relate the detected AR-marker\'s position to the map of environment. The AR-markers basically act as active landmarks placed in undiscovered environments, sending out location information once detected by a camera. The second stage implements the same algorithm on an autonomous humanoid robot to be used as its navigation module. This is achieved by coupling the robot odometry and inertial sensing with the visual marker detection module. Using this system, the robot employs its vision system to enhance its localization robustness and allow quick recovery in lost situations by detecting the active landmarks or the so called AR-markers. The problem of motion blur resulting from the 6-DOF motion of humanoid\'s camera is solved using an adaptive thresholding technique developed to increase the robustness of the augmented reality marker detection under different illumination conditions and camera movements. For our experiments, we used the humanoid robot NAO and verified the performance of this navigation methodology in real-world scenarios.","paperId":"1b34ac92fad67b06b5b1378e15deae7189a93e6f","paperTitle":"Autonomous humanoid robot navigation using augmented reality technique","paperUrl":"https://www.semanticscholar.org/paper/1b34ac92fad67b06b5b1378e15deae7189a93e6f","images":["https://d3i71xaburhd42.cloudfront.net/1b34ac92fad67b06b5b1378e15deae7189a93e6f/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/1b34ac92fad67b06b5b1378e15deae7189a93e6f/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/1b34ac92fad67b06b5b1378e15deae7189a93e6f/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/1b34ac92fad67b06b5b1378e15deae7189a93e6f/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/1b34ac92fad67b06b5b1378e15deae7189a93e6f/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/1b34ac92fad67b06b5b1378e15deae7189a93e6f/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/1b34ac92fad67b06b5b1378e15deae7189a93e6f/3-FigureI-1.png"],"pdf":"https://doi.org/10.1109/ICMECH.2011.5971330"},{"type":"inproceedings","key":"fujii2020development","title":"Development and Evaluation of Mixed Reality Co-Eating System: Sharing the Behavior of Eating Food with a Robot Could Improve Our Dining Experience","author":"Fujii, Ayaka and Kochigami, Kanae and Kitagawa, Shingo and Okada, Kei and Inaba, Masayuki","booktitle":"2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","pages":"357--362","year":"2020","organization":"IEEE","doi":"10.1109/RO-MAN47096.2020.9223518","authors":["Ayaka Fujii","Kanae Kochigami","Shingo Kitagawa","Kei Okada","Masayuki Inaba"],"dblp":"conf/ro-man/FujiiKKOI20","venue":"2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","abstract":"Eating with others enhances our dining experience, improves socialization, and has some health benefits. Although many people do not want to eat alone, there is an increase in the number of people who eat alone in Japan due to difficulty in matching mealtimes and places with others.In this paper, we develop a mixed reality (MR) system for coeating with a robot. In this system, a robot and a MR headset are connected enabling users to observe a robot putting food image into its mouth, as if eating. We conducted an experiment to evaluate the developed system with users that are at least 13 years old. Experimental results show that the users enjoyed their meal and felt more delicious when the robot ate with them than when the robot only talked without eating. Furthermore, they eat more when a robot eats, suggesting that a robot could influence people\u2019s eating behavior.","paperId":"3917e0f5b6000af297797782eb1ffff01abefb57","paperTitle":"Development and Evaluation of Mixed Reality Co-eating System: Sharing the Behavior of Eating Food with a Robot Could Improve Our Dining Experience","paperUrl":"https://www.semanticscholar.org/paper/3917e0f5b6000af297797782eb1ffff01abefb57","images":["https://d3i71xaburhd42.cloudfront.net/3917e0f5b6000af297797782eb1ffff01abefb57/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/3917e0f5b6000af297797782eb1ffff01abefb57/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/3917e0f5b6000af297797782eb1ffff01abefb57/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/3917e0f5b6000af297797782eb1ffff01abefb57/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/3917e0f5b6000af297797782eb1ffff01abefb57/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/3917e0f5b6000af297797782eb1ffff01abefb57/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/3917e0f5b6000af297797782eb1ffff01abefb57/4-Figure7-1.png"],"pdf":"https://doi.org/10.1109/RO-MAN47096.2020.9223518"},{"type":"inproceedings","key":"luipers2021concept","title":"Concept of an Intuitive Human-Robot-Collaboration via Motion Tracking and Augmented Reality","author":"Luipers, Dario and Richert, Anja","booktitle":"2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)","pages":"423--427","year":"2021","organization":"IEEE","doi":"10.1109/ICAICA52286.2021.9498091","authors":["Dario Luipers","Anja Richert"],"venue":"2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)","abstract":"Human-cobot interaction is one of the main aspects of the 4th industrial revolution. One goal of current robotic research is to enhance safety and efficiency by designing the collaboration in a more intuitive way. The following work introduces two concepts to improve human-cobot collaboration on the basis of deep learning and augmented reality to achieve a more efficient and pleasant working environment. The first concept uses meta learning and Gaussian process to predict the movement of the human worker. The second concept enables the worker to see the next assembly step of the robotic arm via augmented reality.","paperId":"d27ac6d65177fb95523fdfb46e1232133a48ae3f","paperTitle":"Concept of an Intuitive Human-Robot-Collaboration via Motion Tracking and Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/d27ac6d65177fb95523fdfb46e1232133a48ae3f","images":[],"pdf":"https://doi.org/10.1109/ICAICA52286.2021.9498091"},{"type":"inproceedings","key":"dias2020deep","title":"Deep Learning of Augmented Reality Based Human Interactions for Automating a Robot Team","author":"Dias, Adhitha and Wellaboda, Hasitha and Rasanka, Yasod and Munasinghe, Menusha and Rodrigo, Ranga and Jayasekara, Peshala","booktitle":"2020 6th International Conference on Control, Automation and Robotics (ICCAR)","pages":"175--182","year":"2020","organization":"IEEE","doi":"10.1109/ICCAR49639.2020.9108004","authors":["Adhitha Dias","Hasitha Wellaboda","Yasod Rasanka","Menusha Munasinghe","Ranga Rodrigo","Peshala Jayasekara"],"venue":"2020 6th International Conference on Control, Automation and Robotics (ICCAR)","abstract":"Getting a team of robots to achieve a relatively complex task using manual manipulation through augmented reality (AR) is interesting. However, the true potential of such an approach manifests when the system can learn from humans. We propose a system comprising a team of robots that performs a previously unseen task\u2014a variant, to be specific\u2014by learning from the sequences of actions taken by multiple human beings doing this task in various ways using deep learning (DL). The training inputs can be through actual manipulation of the team of robots using an augmented-reality tablet or through a simulator. Results indicate that the system is able to fulfill the specified variant of the task more than 80% of the time, inaccuracies mainly owing to unrealistic specifications of tasks. This opens up an avenue of training a team of robots, instead of crafting a rule base.","paperId":"2ee4f8a996929182872eedbc4655c68e106a6ba8","paperTitle":"Deep Learning of Augmented Reality based Human Interactions for Automating a Robot Team","paperUrl":"https://www.semanticscholar.org/paper/2ee4f8a996929182872eedbc4655c68e106a6ba8","images":["https://d3i71xaburhd42.cloudfront.net/2ee4f8a996929182872eedbc4655c68e106a6ba8/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/2ee4f8a996929182872eedbc4655c68e106a6ba8/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/2ee4f8a996929182872eedbc4655c68e106a6ba8/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/2ee4f8a996929182872eedbc4655c68e106a6ba8/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/2ee4f8a996929182872eedbc4655c68e106a6ba8/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/2ee4f8a996929182872eedbc4655c68e106a6ba8/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/2ee4f8a996929182872eedbc4655c68e106a6ba8/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/2ee4f8a996929182872eedbc4655c68e106a6ba8/6-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/2ee4f8a996929182872eedbc4655c68e106a6ba8/7-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/2ee4f8a996929182872eedbc4655c68e106a6ba8/7-TableI-1.png"],"pdf":"https://doi.org/10.1109/ICCAR49639.2020.9108004"},{"type":"inproceedings","key":"lamberti2019designing","title":"Designing Interactive Robotic Games Based on Mixed Reality Technology","author":"Lamberti, Fabrizio and Cannav`o, Alberto and Pirone, Paolo","booktitle":"2019 IEEE International Conference on Consumer Electronics (ICCE)","pages":"1--4","year":"2019","organization":"IEEE","doi":"10.1109/ICCE.2019.8661911","authors":["Fabrizio Lamberti","Alberto Cannav`o","Paolo Pirone"],"dblp":"conf/iccel/LambertiCP19","venue":"2019 IEEE International Conference on Consumer Electronics (ICCE)","abstract":"This paper focuses on an emerging research area represented by robotic gaming and aims to explore the design space of interactive games that combine commercial-off-the-shelf robots and mixed reality. To this purpose, a software platform is developed which allows players to interact with both physical elements and virtual content projected on the ground. A game is then created to show designers how to maximize opportunities offered by such a technology and to build playful experiences.","paperId":"baa970933084e88a00db2fbfa24e171309460ba0","paperTitle":"Designing Interactive Robotic Games based on Mixed Reality Technology","paperUrl":"https://www.semanticscholar.org/paper/baa970933084e88a00db2fbfa24e171309460ba0","images":["https://d3i71xaburhd42.cloudfront.net/baa970933084e88a00db2fbfa24e171309460ba0/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/baa970933084e88a00db2fbfa24e171309460ba0/4-Figure2-1.png"],"pdf":"https://doi.org/10.1109/ICCE.2019.8661911"},{"type":"inproceedings","key":"cousins2017development","title":"Development of a Mixed Reality Based Interface for Human Robot Interaciotn","author":"Cousins, Matthew and Yang, Chenguang and Chen, Junshen and He, Wei and Ju, Zhaojie","booktitle":"2017 International Conference on Machine Learning and Cybernetics (ICMLC)","volume":"1","pages":"27--34","year":"2017","organization":"IEEE","doi":"10.1109/ICMLC.2017.8107738","authors":["Matthew Cousins","Chenguang Yang","Junshen Chen","Wei He","Zhaojie Ju"],"dblp":"conf/icmlc/CousinsYC0J17","venue":"2017 International Conference on Machine Learning and Cybernetics (ICMLC)","abstract":"This research paper presents a novel interface of programming by demonstration (PbD) for Human-Robot Interaction (HRI). The designed mixed reality (MR) based interface is providing users a more immersive user experience (UX) while teleoperating the robot. The operator\'s hand gestures are captured and be used to control the robot. Users are able to see their own hands in the virtual environment. Experimental test was carried out with a dual-arm robot to make the robot react to a gesture made by an operator viewing the robot workspace through a virtual reality (VR) headset.","paperId":"a28b6c85d75943978a6799e44e29be01a3a706f9","paperTitle":"Development of a mixed reality based interface for human robot interaciotn","paperUrl":"https://www.semanticscholar.org/paper/a28b6c85d75943978a6799e44e29be01a3a706f9","images":["https://d3i71xaburhd42.cloudfront.net/a28b6c85d75943978a6799e44e29be01a3a706f9/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/a28b6c85d75943978a6799e44e29be01a3a706f9/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/a28b6c85d75943978a6799e44e29be01a3a706f9/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/a28b6c85d75943978a6799e44e29be01a3a706f9/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/a28b6c85d75943978a6799e44e29be01a3a706f9/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/a28b6c85d75943978a6799e44e29be01a3a706f9/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/a28b6c85d75943978a6799e44e29be01a3a706f9/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/a28b6c85d75943978a6799e44e29be01a3a706f9/6-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/a28b6c85d75943978a6799e44e29be01a3a706f9/7-Figure9-1.png"],"pdf":"https://doi.org/10.1109/ICMLC.2017.8107738"},{"type":"inproceedings","key":"christensen2017depth","title":"Depth Cues in Augmented Reality for Training of Robot-Assisted Minimally Invasive Surgery","author":"Christensen, Nicklas H and Hjermitslev, Oliver G and Falk, Frederik and Madsen, Marco B and Ostergaard, Frederik H and Kibsgaard, Martin and Kraus, Martin and Poulsen, Johan and Petersson, Jane","booktitle":"Proceedings of the 21st International Academic Mindtrek Conference","pages":"120--126","year":"2017","doi":"10.1145/3131085.3131123","authors":["Nicklas H Christensen","Oliver G Hjermitslev","Frederik Falk","Marco B Madsen","Frederik H Ostergaard","Martin Kibsgaard","Martin Kraus","Johan Poulsen","Jane Petersson"],"dblp":"conf/mindtrek/ChristensenHFMO17","venue":"MindTrek","abstract":"Training of robot-assisted minimally invasive surgery often includes supervised practice with a robotic surgical system. In this case, augmented reality can improve the communication between instructor and trainee, for example, by allowing the instructor to demonstrate skills with virtual surgical instruments that are shown to the trainee by means of augmented reality. However, virtual instruments are more difficult to handle than the real instruments---partly due to the lack of depth cues. In order to improve the usability of virtual surgical instruments, we compared five depth cues. Results showed both a preference for the artificial highlight cue and an aversion to transparency and depth of field. The highlight cue was therefore reviewed by experienced surgery instructors. These experts agreed that the highlight cue was beneficial and that the prototype could be used to some extent already and fully upon further development.","paperId":"b908fe780a1f93326be5a8d40fa1a768caf0464c","paperTitle":"Depth cues in augmented reality for training of robot-assisted minimally invasive surgery","paperUrl":"https://www.semanticscholar.org/paper/b908fe780a1f93326be5a8d40fa1a768caf0464c","images":["https://d3i71xaburhd42.cloudfront.net/b908fe780a1f93326be5a8d40fa1a768caf0464c/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/b908fe780a1f93326be5a8d40fa1a768caf0464c/5-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/b908fe780a1f93326be5a8d40fa1a768caf0464c/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/b908fe780a1f93326be5a8d40fa1a768caf0464c/5-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/b908fe780a1f93326be5a8d40fa1a768caf0464c/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/b908fe780a1f93326be5a8d40fa1a768caf0464c/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/b908fe780a1f93326be5a8d40fa1a768caf0464c/5-Figure5-1.png"],"pdf":"https://doi.org/10.1145/3131085.3131123"},{"type":"inproceedings","key":"shao2019development","title":"Development of Robot Design Evaluating System Using Augmented Reality for Affinity Robots","author":"Shao, Shyang and Muramatsu, Satoshi and Inagaki, Katsuhiko and Chugo, Daisuke and Yokota, Syo and Hashimoto, Hiroshi","booktitle":"2019 IEEE 17th International Conference on Industrial Informatics (INDIN)","volume":"1","pages":"815--820","year":"2019","organization":"IEEE","doi":"10.1109/INDIN41052.2019.8972057","authors":["Shyang Shao","Satoshi Muramatsu","Katsuhiko Inagaki","Daisuke Chugo","Syo Yokota","Hiroshi Hashimoto"],"dblp":"conf/indin/ShaoMICYH19","venue":"2019 IEEE 17th International Conference on Industrial Informatics (INDIN)","abstract":"In this research, in order to clarify robot design elements with high affinity with people, we developed an evaluation system that enables users to evaluate impressions from a highly flexible perspective from augmented reality. Human investigated impression evaluation on robot and identified 5 factors which are the evaluation index","paperId":"a213f671cea25b319d43b1f8ee7544ee9481fa9a","paperTitle":"Development of robot design evaluating system using Augmented Reality for affinity robots","paperUrl":"https://www.semanticscholar.org/paper/a213f671cea25b319d43b1f8ee7544ee9481fa9a","images":["https://d3i71xaburhd42.cloudfront.net/a213f671cea25b319d43b1f8ee7544ee9481fa9a/1-Figure1-1.png"],"pdf":"https://doi.org/10.1109/INDIN41052.2019.8972057"},{"type":"inproceedings","key":"akinbiyi2006dynamic","title":"Dynamic Augmented Reality for Sensory Substitution in Robot-Assisted Surgical Systems","author":"Akinbiyi, Takintope and Reiley, Carol E and Saha, Sunipa and Burschka, Darius and Hasser, Christopher J and Yuh, David D and Okamura, Allison M","booktitle":"2006 International Conference of the IEEE Engineering in Medicine and Biology Society","pages":"567--570","year":"2006","organization":"IEEE","doi":"10.1109/IEMBS.2006.259707","authors":["Takintope Akinbiyi","Carol E Reiley","Sunipa Saha","Darius Burschka","Christopher J Hasser","David D Yuh","Allison M Okamura"],"dblp":"conf/embc/AkinbiyiRSBHYO06","venue":"2006 International Conference of the IEEE Engineering in Medicine and Biology Society","abstract":"Teleoperated robot-assisted surgical systems provide surgeons with improved precision, dexterity, and visualization over traditional minimally invasive surgery. The addition of haptic (force and/or tactile) feedback has been proposed as a way to further enhance the performance of these systems. However, due to limitations in sensing and control technologies, implementing direct haptic feedback to the surgeon\'s hands remains impractical for clinical application. A new, intuitive augmented reality system for presentation of force information through sensory substitution has been developed and evaluated. The augmented reality system consists of force-sensing robotic instruments, a kinematic tool tracker, and a graphic display that overlays a visual representation of force levels on top of the moving instrument tips. The system is integrated with the da Vinci Surgical System (Intuitive Surgical, Inc.) and tested by several users in a phantom knot tying task. The augmented reality system decreases the number of broken sutures, decreases the number of loose knots, and results in more consistent application of forces","paperId":"b771c22eee42a8dcffd9cc5f40dbe95b0d3d0084","paperTitle":"Dynamic Augmented Reality for Sensory Substitution in Robot-Assisted Surgical Systems","paperUrl":"https://www.semanticscholar.org/paper/b771c22eee42a8dcffd9cc5f40dbe95b0d3d0084","images":["https://d3i71xaburhd42.cloudfront.net/b771c22eee42a8dcffd9cc5f40dbe95b0d3d0084/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/b771c22eee42a8dcffd9cc5f40dbe95b0d3d0084/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/b771c22eee42a8dcffd9cc5f40dbe95b0d3d0084/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/b771c22eee42a8dcffd9cc5f40dbe95b0d3d0084/2-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/b771c22eee42a8dcffd9cc5f40dbe95b0d3d0084/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/b771c22eee42a8dcffd9cc5f40dbe95b0d3d0084/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/b771c22eee42a8dcffd9cc5f40dbe95b0d3d0084/3-TableI-1.png","https://d3i71xaburhd42.cloudfront.net/b771c22eee42a8dcffd9cc5f40dbe95b0d3d0084/3-TableII-1.png","https://d3i71xaburhd42.cloudfront.net/b771c22eee42a8dcffd9cc5f40dbe95b0d3d0084/4-TableIII-1.png"],"pdf":"http://mediatum.ub.tum.de/doc/1292020/document.pdf"},{"type":"inproceedings","key":"xue2020enabling","title":"Enabling Human-Robot-Interaction for Remote Robotic Operation via Augmented Reality","author":"Xue, Chung and Qiao, Yuansong and Murray, Niall","booktitle":"2020 IEEE 21st International Symposium on\\" A World of Wireless, Mobile and Multimedia Networks\\"(WoWMoM)","pages":"194--196","year":"2020","organization":"IEEE","doi":"10.1109/WoWMoM49955.2020.00046","authors":["Chung Xue","Yuansong Qiao","Niall Murray"],"dblp":"conf/wowmom/XueQM20","venue":"2020 IEEE 21st International Symposium on \\"A World of Wireless, Mobile and Multimedia Networks\\" (WoWMoM)","abstract":"Human-Robot Interaction (HRI) will be a crucial component of smart factories of the future (FoF). This demo presents a reliable and cost-effective HRI system based on Augmented Reality (AR). In this demonstration, we offer concepts and methods currently being developed to enable high-level human-robot collaboration and interaction. The user interaction and instructions are captured via AR and communicated to a Robot Operating System (ROS) powered robotic arm.","paperId":"e30afe793ee992c7050456ccb34a1dd5f95c4cd8","paperTitle":"Enabling Human-Robot-Interaction for Remote Robotic Operation via Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/e30afe793ee992c7050456ccb34a1dd5f95c4cd8","images":["https://d3i71xaburhd42.cloudfront.net/e30afe793ee992c7050456ccb34a1dd5f95c4cd8/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/e30afe793ee992c7050456ccb34a1dd5f95c4cd8/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/e30afe793ee992c7050456ccb34a1dd5f95c4cd8/3-Figure4-1.png"],"pdf":"https://doi.org/10.1109/WoWMoM49955.2020.00046"},{"type":"inproceedings","key":"kapinus2020end","title":"End-User Robot Programming Case Study: Augmented Reality vs. Teach Pendant","author":"Kapinus, Michal and Materna, Zden\u0117k and Bambu\u0161ek, Daniel and Beran, Vit\u0117zslav","booktitle":"Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction","pages":"281--283","year":"2020","doi":"10.1145/3371382.3378266","authors":["Michal Kapinus","Zden\u0117k Materna","Daniel Bambu\u0161ek","Vit\u0117zslav Beran"],"dblp":"conf/hri/KapinusMBB20","venue":"HRI","abstract":"The work presents a preliminary experiment aimed for comparing a traditional method of programming an industrial collaborative robot using a teach pendant, with a novel method based on augmented reality and interaction on a high-level of abstraction. In the experiment, three participants programmed a visual inspection task. Subjective and objective metrics are reported as well as selected usability-related issues of both interfaces. The main purpose of the experiment was to get initial insight into the problematic of comparing highly different user interfaces and to provide a basis for a more rigorous comparison, that is going to be taken out.","paperId":"fa410694c2fcf7f20020f52e3c5cbd1b0aab8752","paperTitle":"End-User Robot Programming Case Study: Augmented Reality vs. Teach Pendant","paperUrl":"https://www.semanticscholar.org/paper/fa410694c2fcf7f20020f52e3c5cbd1b0aab8752","images":["https://d3i71xaburhd42.cloudfront.net/df513d2b2056fbb0cadf88f7636b12a7251813ad/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/df513d2b2056fbb0cadf88f7636b12a7251813ad/2-Table1-1.png"],"pdf":"https://doi.org/10.1145/3371382.3378266"},{"type":"inproceedings","key":"krupke2018comparison","title":"Comparison of Multimodal Heading and Pointing Gestures for Co-Located Mixed Reality Human-Robot Interaction","author":"Krupke, Dennis and Steinicke, Frank and Lubos, Paul and Jonetzko, Yannick and G\\"orner, Michael and Zhang, Jianwei","booktitle":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","pages":"1--9","year":"2018","organization":"IEEE","doi":"10.1109/IROS.2018.8594043","authors":["Dennis Krupke","Frank Steinicke","Paul Lubos","Yannick Jonetzko","Michael G\\"orner","Jianwei Zhang"],"dblp":"conf/iros/KrupkeSLJG018","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","abstract":"Mixed reality (MR)opens up new vistas for human-robot interaction (HRI)scenarios in which a human operator can control and collaborate with co-located robots. For instance, when using a see-through head-mounted-display (HMD)such as the Microsoft HoloLens, the operator can see the real robots and additional virtual information can be superimposed over the real-world view to improve security, acceptability and predictability in HRI situations. In particular, previewing potential robot actions in-situ before they are executed has enormous potential to reduce the risks of damaging the system or injuring the human operator. In this paper, we introduce the concept and implementation of such an MR human-robot collaboration system in which a human can intuitively and naturally control a co-located industrial robot arm for pick-and-place tasks. In addition, we compared two different, multimodal HRI techniques to select the pick location on a target object using (i)head orientation (aka heading)or (ii)pointing, both in combination with speech. The results show that heading-based interaction techniques are more precise, require less time and are perceived as less physically, temporally and mentally demanding for MR-based pick-and-place scenarios. We confirmed these results in an additional usability study in a delivery-service task with a multi-robot system. The developed MR interface shows a preview of the current robot programming to the operator, e. g., pick selection or trajectory. The findings provide important implications for the design of future MR setups.","paperId":"38f6dedb6cdcd33ce1735e73e8fb90d7336d85b1","paperTitle":"Comparison of Multimodal Heading and Pointing Gestures for Co-Located Mixed Reality Human-Robot Interaction","paperUrl":"https://www.semanticscholar.org/paper/38f6dedb6cdcd33ce1735e73e8fb90d7336d85b1","images":["https://d3i71xaburhd42.cloudfront.net/38f6dedb6cdcd33ce1735e73e8fb90d7336d85b1/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/38f6dedb6cdcd33ce1735e73e8fb90d7336d85b1/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/38f6dedb6cdcd33ce1735e73e8fb90d7336d85b1/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/38f6dedb6cdcd33ce1735e73e8fb90d7336d85b1/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/38f6dedb6cdcd33ce1735e73e8fb90d7336d85b1/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/38f6dedb6cdcd33ce1735e73e8fb90d7336d85b1/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/38f6dedb6cdcd33ce1735e73e8fb90d7336d85b1/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/38f6dedb6cdcd33ce1735e73e8fb90d7336d85b1/6-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/38f6dedb6cdcd33ce1735e73e8fb90d7336d85b1/3-TableI-1.png","https://d3i71xaburhd42.cloudfront.net/38f6dedb6cdcd33ce1735e73e8fb90d7336d85b1/5-TableII-1.png"],"pdf":"https://basilic.informatik.uni-hamburg.de/Publications/2018/KSLJGZ18/IROS2018_final2.pdf"},{"type":"inproceedings","key":"calandra2021evaluating","title":"Evaluating an Augmented Reality-Based Partially Assisted Approach to Remote Assistance in Heterogeneous Robotic Applications","author":"Calandra, Davide and Cannav`o, Alberto and Lamberti, Fabrizio","booktitle":"2021 IEEE 7th International Conference on Virtual Reality (ICVR)","pages":"380--387","year":"2021","organization":"IEEE","doi":"10.1109/ICVR51878.2021.9483849","authors":["Davide Calandra","Alberto Cannav`o","Fabrizio Lamberti"],"venue":"2021 IEEE 7th International Conference on Virtual Reality (ICVR)","abstract":"Among the countless applications of Augmented Reality (AR) in the industry, remote assistance represents one of the most prominent and widely studied use cases. Recently, the way in which assistance can be delivered started to evolve, unleashing the full potential of such technology. New methodologies have been proposed able to foster operators\u2019 autonomy and reduce under-utilization of skilled human resources. This paper studies the effectiveness of a recently proposed approach to AR-based remote assistance, referred to as partially assisted, which differs from the traditional step-by-step guidance in the way the AR hints are conveyed by the expert to the operator. The suitability of this approach has been proved already for a number of simple industrial tasks, but a comprehensive study has yet to be performed for validating its effectiveness in complex use cases. This paper addresses this lack by considering as a case study the mastering of a robotic manipulator, a procedure involving a number of heterogeneous operations. The performance of the partially assisted approach is compared with step-by-step guidance based on both objective and subjective metrics. Results showed that the former approach could be particularly effective in reducing the time investment for the expert, allowing the operator to autonomously complete the assigned task in a time comparable to traditional assistance with a negligible need for further support.","paperId":"7f150539f781c1125d4f6a04907f0ce8ec69b1b2","paperTitle":"Evaluating an Augmented Reality-Based Partially Assisted Approach to Remote Assistance in Heterogeneous Robotic Applications","paperUrl":"https://www.semanticscholar.org/paper/7f150539f781c1125d4f6a04907f0ce8ec69b1b2","images":[],"pdf":"https://doi.org/10.1109/ICVR51878.2021.9483849"},{"type":"inproceedings","key":"aikawa2018comparison","title":"Comparison of Gesture Inputs for Robot System Using Mixed Reality to Encourage Driving Review","author":"Aikawa, Yuya and Kanoh, Masayoshi and Jimenez, Felix and Hayase, Mitsuhiro and Tanaka, Takahiro and Kanamori, Hitoshi","booktitle":"2018 Joint 10th International Conference on Soft Computing and Intelligent Systems (SCIS) and 19th International Symposium on Advanced Intelligent Systems (ISIS)","pages":"62--66","year":"2018","organization":"IEEE","doi":"10.1109/SCIS-ISIS.2018.00020","authors":["Yuya Aikawa","Masayoshi Kanoh","Felix Jimenez","Mitsuhiro Hayase","Takahiro Tanaka","Hitoshi Kanamori"],"dblp":"conf/scisisis/AikawaKJHTK18","venue":"2018 Joint 10th International Conference on Soft Computing and Intelligent Systems (SCIS) and 19th International Symposium on Advanced Intelligent Systems (ISIS)","abstract":"We have been developing a robot system that encourages people to review their driving at home in order to improve it. Mixed reality technology is used to implement a function in the system for reviewing one\'s driving. Unintuitive input interface used in an existing mixed reality system makes it difficult to operate the system. In this paper, we propose two gestures. One uses a swiping motion, and the other acts like a laser pointer. The gesture that imitates a laser pointer performed best in experiments.","paperId":"cd16dac5c20a03032483463844f498f24c60f0c4","paperTitle":"Comparison of Gesture Inputs for Robot System Using Mixed Reality to Encourage Driving Review","paperUrl":"https://www.semanticscholar.org/paper/cd16dac5c20a03032483463844f498f24c60f0c4","images":["https://d3i71xaburhd42.cloudfront.net/cd16dac5c20a03032483463844f498f24c60f0c4/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/cd16dac5c20a03032483463844f498f24c60f0c4/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/cd16dac5c20a03032483463844f498f24c60f0c4/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/cd16dac5c20a03032483463844f498f24c60f0c4/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/cd16dac5c20a03032483463844f498f24c60f0c4/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/cd16dac5c20a03032483463844f498f24c60f0c4/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/cd16dac5c20a03032483463844f498f24c60f0c4/4-TableI-1.png"],"pdf":"https://doi.org/10.1109/SCIS-ISIS.2018.00020"},{"type":"inproceedings","key":"nuzzi2020hands","title":"Hands-Free: A Robot Augmented Reality Teleoperation System","author":"Nuzzi, Cristina and Ghidini, Stefano and Pagani, Roberto and Pasinetti, Simone and Coffetti, Gabriele and Sansoni, Giovanna","booktitle":"2020 17th International Conference on Ubiquitous Robots (UR)","pages":"617--624","year":"2020","organization":"IEEE","doi":"10.1109/UR49135.2020.9144841","authors":["Cristina Nuzzi","Stefano Ghidini","Roberto Pagani","Simone Pasinetti","Gabriele Coffetti","Giovanna Sansoni"],"dblp":"conf/urai/NuzziGPPCS20","venue":"2020 17th International Conference on Ubiquitous Robots (UR)","abstract":"In this paper the novel teleoperation method \\"Hands-Free\\" is presented. Hands-Free is a vision-based augmented reality system that allows users to teleoperate a robot end-effector with their hands in real time. The system leverages OpenPose neural network to detect the human operator hand in a given workspace, achieving an average inference time of 0.15 s. The user index position is extracted from the image and converted in real world coordinates to move the robot end-effector in a different workspace.The user hand skeleton is visualized in real-time moving in the actual robot workspace, allowing the user to teleoperate the robot intuitively, regardless of the differences between the user workspace and the robot workspace.Since a set of calibration procedures is involved to convert the index position to the robot end-effector position, we designed three experiments to determine the different errors introduced by conversion. A detailed explanation of the mathematical principles adopted in this work is provided in the paper.Finally, the proposed system has been developed using ROS and is publicly available at the following GitHub repository: https://github.com/Krissy93/hands-free-project.","paperId":"29205be1860924a28e98f89639039a9cbf3435e5","paperTitle":"Hands-Free: a robot augmented reality teleoperation system","paperUrl":"https://www.semanticscholar.org/paper/29205be1860924a28e98f89639039a9cbf3435e5","images":["https://d3i71xaburhd42.cloudfront.net/d57b543be5aacfdec68a0f2015da927d3b6aefc7/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/d57b543be5aacfdec68a0f2015da927d3b6aefc7/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/d57b543be5aacfdec68a0f2015da927d3b6aefc7/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/d57b543be5aacfdec68a0f2015da927d3b6aefc7/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/d57b543be5aacfdec68a0f2015da927d3b6aefc7/7-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/d57b543be5aacfdec68a0f2015da927d3b6aefc7/6-TableI-1.png","https://d3i71xaburhd42.cloudfront.net/d57b543be5aacfdec68a0f2015da927d3b6aefc7/6-TableII-1.png","https://d3i71xaburhd42.cloudfront.net/d57b543be5aacfdec68a0f2015da927d3b6aefc7/7-TableIII-1.png"],"pdf":"https://doi.org/10.1109/UR49135.2020.9144841"},{"type":"inproceedings","key":"zolotas2018head","title":"Head-Mounted Augmented Reality for Explainable Robotic Wheelchair Assistance","author":"Zolotas, Mark and Elsdon, Joshua and Demiris, Yiannis","booktitle":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","pages":"1823--1829","year":"2018","organization":"IEEE","doi":"10.1109/IROS.2018.8594002","authors":["Mark Zolotas","Joshua Elsdon","Yiannis Demiris"],"dblp":"conf/iros/ZolotasED18","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","abstract":"Robotic wheelchairs with built-in assistive features, such as shared control, are an emerging means of providing independent mobility to severely disabled individuals. However, patients often struggle to build a mental model of their wheelchair\'s behaviour under different environmental conditions. Motivated by the desire to help users bridge this gap in perception, we propose a novel augmented reality system using a Microsoft Hololens as a head-mounted aid for wheelchair navigation. The system displays visual feedback to the wearer as a way of explaining the underlying dynamics of the wheelchair\'s shared controller and its predicted future states. To investigate the influence of different interface design options, a pilot study was also conducted. We evaluated the acceptance rate and learning curve of an immersive wheelchair training regime, revealing preliminary insights into the potential beneficial and adverse nature of different augmented reality cues for assistive navigation. In particular, we demonstrate that care should be taken in the presentation of information, with effort-reducing cues for augmented information acquisition (for example, a rear-view display) being the most appreciated.","paperId":"2162e684d671263a8bea9cb3c4fc61d8d129e272","paperTitle":"Head-Mounted Augmented Reality for Explainable Robotic Wheelchair Assistance","paperUrl":"https://www.semanticscholar.org/paper/2162e684d671263a8bea9cb3c4fc61d8d129e272","images":["https://d3i71xaburhd42.cloudfront.net/2162e684d671263a8bea9cb3c4fc61d8d129e272/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/2162e684d671263a8bea9cb3c4fc61d8d129e272/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/2162e684d671263a8bea9cb3c4fc61d8d129e272/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/2162e684d671263a8bea9cb3c4fc61d8d129e272/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/2162e684d671263a8bea9cb3c4fc61d8d129e272/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/2162e684d671263a8bea9cb3c4fc61d8d129e272/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/2162e684d671263a8bea9cb3c4fc61d8d129e272/5-TableI-1.png","https://d3i71xaburhd42.cloudfront.net/2162e684d671263a8bea9cb3c4fc61d8d129e272/6-TableII-1.png"],"pdf":"https://doi.org/10.1109/IROS.2018.8594002"},{"type":"inproceedings","key":"sosa2015imperfect","title":"Imperfect Robot Control in a Mixed Reality Game to Teach Hybrid Human-Robot Team Coordination","author":"Sosa, Adam and Stanton, Richard and Perez, Stepheny and Keyes-Garcia, Christian and Gonzalez, Sara and Toups, Zachary O","booktitle":"Proceedings of the 2015 Annual Symposium on Computer-Human Interaction in Play","pages":"697--702","year":"2015","doi":"10.1145/2793107.2810288","authors":["Adam Sosa","Richard Stanton","Stepheny Perez","Christian Keyes-Garcia","Sara Gonzalez","Zachary O Toups"],"dblp":"conf/chiplay/SosaSPKGT15","venue":"CHI PLAY","abstract":"A key component of team coordination in real-world practice involves communication and work execution from multiple perspectives; this is especially true for unmanned robotic system operators. Previously, such communication skills have been challenging to train, requiring many users and/or high-fidelity simulations. The present research develops a mixed reality game using a small robot and alternate roles to engage two players in team communication through information distribution. The robot features imperfect controls, creating challenge and reflecting the real-world context. This paper presents the game design and development challenges, reflecting on the value of mixed reality for training hybrid human-robot teams.","paperId":"7abeb1ae77b110daa95fe9bc11add309a71b8ae0","paperTitle":"Imperfect Robot Control in a Mixed Reality Game to Teach Hybrid Human-Robot Team Coordination","paperUrl":"https://www.semanticscholar.org/paper/7abeb1ae77b110daa95fe9bc11add309a71b8ae0","images":["https://d3i71xaburhd42.cloudfront.net/7abeb1ae77b110daa95fe9bc11add309a71b8ae0/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/7abeb1ae77b110daa95fe9bc11add309a71b8ae0/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/7abeb1ae77b110daa95fe9bc11add309a71b8ae0/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/7abeb1ae77b110daa95fe9bc11add309a71b8ae0/4-Figure6-1.png"],"pdf":"https://doi.org/10.1145/2793107.2810288"},{"type":"inproceedings","key":"williams2019mixed","title":"Mixed Reality Deictic Gesture for Multi-Modal Robot Communication","author":"Williams, Tom and Bussing, Matthew and Cabrol, Sebastian and Boyle, Elizabeth and Tran, Nhan","booktitle":"2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","pages":"191--201","year":"2019","organization":"IEEE","doi":"10.1109/HRI.2019.8673275","authors":["Tom Williams","Matthew Bussing","Sebastian Cabrol","Elizabeth Boyle","Nhan Tran"],"dblp":"conf/hri/0001BCBT19","venue":"2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","abstract":"In previous work, researchers have repeatedly demonstrated that robots\' use of deictic gestures enables effective and natural human-robot interaction. However, new technologies such as augmented reality head mounted displays enable environments in which mixed-reality becomes possible, and in such environments, physical gestures become but one category among many different types of mixed reality deictic gestures. In this paper, we present the first experimental exploration of the effectiveness of mixed reality deictic gestures beyond physical gestures. Specifically, we investigate human perception of videos simulating the display of allocentric gestures, in which robots circle their targets in users\' fields of view. Our results suggest that this is an effective communication strategy, both in terms of objective accuracy and subjective perception, especially when paired with complex natural language references.","paperId":"350a226c434446dbfe73e1041761f7365ea765fc","paperTitle":"Mixed Reality Deictic Gesture for Multi-Modal Robot Communication","paperUrl":"https://www.semanticscholar.org/paper/350a226c434446dbfe73e1041761f7365ea765fc","images":["https://d3i71xaburhd42.cloudfront.net/350a226c434446dbfe73e1041761f7365ea765fc/4-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/350a226c434446dbfe73e1041761f7365ea765fc/6-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/350a226c434446dbfe73e1041761f7365ea765fc/6-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/350a226c434446dbfe73e1041761f7365ea765fc/7-Figure4-1.png"],"pdf":"http://inside.mines.edu/~twilliams/pdfs/williams2019hri.pdf"},{"type":"inproceedings","key":"nilwong2020outdoor","title":"Outdoor Robot Navigation System Using Game-Based DQN and Augmented Reality","author":"Nilwong, Sivapong and Capi, Genci","booktitle":"2020 17th International Conference on Ubiquitous Robots (UR)","pages":"74--80","year":"2020","organization":"IEEE","doi":"10.1109/UR49135.2020.9144838","authors":["Sivapong Nilwong","Genci Capi"],"dblp":"conf/urai/NilwongC20","venue":"2020 17th International Conference on Ubiquitous Robots (UR)","abstract":"This paper presents a deep reinforcement learning based robot outdoor navigation method using visual information. The deep q network (DQN) maps the visual data to robot action in a goal location reaching task. An advantage of the proposed method is that the implemented DQN is trained in the first-person shooter (FPS) game-based simulated environment provided by ViZDoom. The FPS simulated environment reduces the differences between the training and the real environments resulting in a good performance of trained DQNs. In our implementation a marker-based augmented reality algorithm with a simple object detection method is used to train the DQN. The proposed outdoor navigation system is tested in the simulation and real robot implementation, with no additional training. Experimental results showed that the navigation system trained inside the game-based simulation can guide the real robot in outdoor goal directed navigation tasks.","paperId":"4c47b03121500c6f56895bfa0124f3e4937a96e1","paperTitle":"Outdoor Robot Navigation System using Game-Based DQN and Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/4c47b03121500c6f56895bfa0124f3e4937a96e1","images":["https://d3i71xaburhd42.cloudfront.net/be3687195deb2e997396a8537235e4b4924a76a7/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/be3687195deb2e997396a8537235e4b4924a76a7/6-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/be3687195deb2e997396a8537235e4b4924a76a7/6-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/be3687195deb2e997396a8537235e4b4924a76a7/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/be3687195deb2e997396a8537235e4b4924a76a7/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/be3687195deb2e997396a8537235e4b4924a76a7/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/be3687195deb2e997396a8537235e4b4924a76a7/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/be3687195deb2e997396a8537235e4b4924a76a7/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/be3687195deb2e997396a8537235e4b4924a76a7/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/be3687195deb2e997396a8537235e4b4924a76a7/5-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/be3687195deb2e997396a8537235e4b4924a76a7/3-TableI-1.png","https://d3i71xaburhd42.cloudfront.net/be3687195deb2e997396a8537235e4b4924a76a7/3-TableII-1.png","https://d3i71xaburhd42.cloudfront.net/be3687195deb2e997396a8537235e4b4924a76a7/4-TableIII-1.png","https://d3i71xaburhd42.cloudfront.net/be3687195deb2e997396a8537235e4b4924a76a7/6-TableIV-1.png"],"pdf":"https://doi.org/10.1109/UR49135.2020.9144838"},{"type":"inproceedings","key":"aschenbrenner2018comparing","title":"Comparing Different Augmented Reality Support Applications for Cooperative Repair of an Industrial Robot","author":"Aschenbrenner, Doris and Rojkov, Michael and Leutert, Florian and Verlinden, Jouke and Lukosch, Stephan and Latoschik, Marc Erich and Schilling, Klaus","booktitle":"2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)","pages":"69--74","year":"2018","organization":"IEEE","doi":"10.1109/ISMAR-ADJUNCT.2018.00036","authors":["Doris Aschenbrenner","Michael Rojkov","Florian Leutert","Jouke Verlinden","Stephan Lukosch","Marc Erich Latoschik","Klaus Schilling"],"dblp":"conf/ismar/AschenbrennerRL18","venue":"2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)","abstract":"Digitization and the growing capabilities of data networks enable companies to perform tasks via remote support, which previously required service personnel to travel. But which mixed reality method leads to better results regarding human factors, grounding and performance criteria? This paper reports on a collaborative user study, in which a local worker is guided by a remote expert with the help of different augmented reality methods, specifically see-through HMD, spatial projection, and video-mixing tablet. The task to perform is the exchange of a controller in a switch cabinet of an industrial robot, a task rather typical for failure detection within the field. Our study was conducted in collaboration with a technician school, where 50 technician apprentices participated in our study. Our results show clear advantages of using augmented reality (AR) versus traditional conditions (audio, video, screenshot) to enable remote support. It further gives significant indications for using a projection based AR method.","paperId":"156a166dd3c01cc32542da7b56550e16ea5f2fcb","paperTitle":"Comparing Different Augmented Reality Support Applications for Cooperative Repair of an Industrial Robot","paperUrl":"https://www.semanticscholar.org/paper/156a166dd3c01cc32542da7b56550e16ea5f2fcb","images":["https://d3i71xaburhd42.cloudfront.net/156a166dd3c01cc32542da7b56550e16ea5f2fcb/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/156a166dd3c01cc32542da7b56550e16ea5f2fcb/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/156a166dd3c01cc32542da7b56550e16ea5f2fcb/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/156a166dd3c01cc32542da7b56550e16ea5f2fcb/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/156a166dd3c01cc32542da7b56550e16ea5f2fcb/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/156a166dd3c01cc32542da7b56550e16ea5f2fcb/3-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/156a166dd3c01cc32542da7b56550e16ea5f2fcb/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/156a166dd3c01cc32542da7b56550e16ea5f2fcb/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/156a166dd3c01cc32542da7b56550e16ea5f2fcb/5-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/156a166dd3c01cc32542da7b56550e16ea5f2fcb/5-Figure10-1.png"],"pdf":"https://doi.org/10.1109/ISMAR-ADJUNCT.2018.00036"},{"type":"inproceedings","key":"reardon2018come","title":"Come See This! Augmented Reality to Enable Human-Robot Cooperative Search","author":"Reardon, Christopher and Lee, Kevin and Fink, Jonathan","booktitle":"2018 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)","pages":"1--7","year":"2018","organization":"IEEE","doi":"10.1109/SSRR.2018.8468622","authors":["Christopher Reardon","Kevin Lee","Jonathan Fink"],"dblp":"conf/ssrr/ReardonLF18","venue":"2018 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)","abstract":"Ahstract-Robots operating alongside humans in field environments have the potential to greatly increase the situational awareness of their human teammates. A significant challenge, however, is the efficient conveyance of what the robot perceives to the human in order to achieve improved situational awareness. We believe augmented reality (AR), which allows a human to simultaneously perceive the real world and digital information situated virtually in the real world, has the potential to address this issue. Motivated by the emerging prevalence of practical human-wearable AR devices, we present a system that enables a robot to perform cooperative search with a human teammate, where the robot can both share search results and assist the human teammate in navigation to the search target. We demonstrate this ability in a search task in an uninstrumented environment where the robot identifies and localizes targets and provides navigation direction via AR to bring the human to the correct target.","paperId":"334ae614cc3fa76d9e983f82e3a1826a06739b3b","paperTitle":"Come See This! Augmented Reality to Enable Human-Robot Cooperative Search","paperUrl":"https://www.semanticscholar.org/paper/334ae614cc3fa76d9e983f82e3a1826a06739b3b","images":["https://d3i71xaburhd42.cloudfront.net/334ae614cc3fa76d9e983f82e3a1826a06739b3b/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/334ae614cc3fa76d9e983f82e3a1826a06739b3b/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/334ae614cc3fa76d9e983f82e3a1826a06739b3b/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/334ae614cc3fa76d9e983f82e3a1826a06739b3b/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/334ae614cc3fa76d9e983f82e3a1826a06739b3b/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/334ae614cc3fa76d9e983f82e3a1826a06739b3b/6-Figure6-1.png"],"pdf":"https://doi.org/10.1109/SSRR.2018.8468622"},{"type":"article","key":"chen2020combination","title":"Combination of Augmented Reality Based Brain-Computer Interface and Computer Vision for High-Level Control of a Robotic Arm","author":"Chen, Xiaogang and Huang, Xiaoshan and Wang, Yijun and Gao, Xiaorong","journal":"IEEE Transactions on Neural Systems and Rehabilitation Engineering","volume":"28","number":"12","pages":"3140--3147","year":"2020","publisher":"IEEE","doi":"10.1109/TNSRE.2020.3038209","authors":["Xiaogang Chen","Xiaoshan Huang","Yijun Wang","Xiaorong Gao"],"venue":"IEEE Transactions on Neural Systems and Rehabilitation Engineering","abstract":"Recent advances in robotics, neuroscience, and signal processing make it possible to operate a robot through electroencephalography (EEG)-based brain-computer interface (BCI). Although some successful attempts have been made in recent years, the practicality of the entire system still has much room for improvement. The present study designed and realized a robotic arm control system by combing augmented reality (AR), computer vision, and steady-state visual evoked potential (SSVEP)-BCI. AR environment was implemented by a Microsoft HoloLens. Flickering stimuli for eliciting SSVEPs were presented on the HoloLens, which allowed users to see both the robotic arm and the user interface of the BCI. Thus users did not need to switch attention between the visual stimulator and the robotic arm. A four-command SSVEP-BCI was built for users to choose the specific object to be operated by the robotic arm. Once an object was selected, the computer vision would provide the location and color of the object in the workspace. Subsequently, the object was autonomously picked up and placed by the robotic arm. According to the online results obtained from twelve participants, the mean classification accuracy of the proposed system was 93.96 \xb1 5.05%. Moreover, all subjects could utilize the proposed system to successfully pick and place objects in a specific order. These results demonstrated the potential of combining AR-BCI and computer vision to control robotic arms, which is expected to further promote the practicality of BCI-controlled robots.","paperId":"28aa453e45479959693624e45ef54e40373a981d","paperTitle":"Combination of Augmented Reality Based Brain- Computer Interface and Computer Vision for High-Level Control of a Robotic Arm","paperUrl":"https://www.semanticscholar.org/paper/28aa453e45479959693624e45ef54e40373a981d","images":["https://d3i71xaburhd42.cloudfront.net/08a00420b5973356df927993b4e37cc458432024/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/08a00420b5973356df927993b4e37cc458432024/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/08a00420b5973356df927993b4e37cc458432024/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/08a00420b5973356df927993b4e37cc458432024/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/08a00420b5973356df927993b4e37cc458432024/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/08a00420b5973356df927993b4e37cc458432024/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/08a00420b5973356df927993b4e37cc458432024/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/08a00420b5973356df927993b4e37cc458432024/6-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/08a00420b5973356df927993b4e37cc458432024/7-TableI-1.png","https://d3i71xaburhd42.cloudfront.net/08a00420b5973356df927993b4e37cc458432024/7-TableII-1.png"],"pdf":"https://doi.org/10.1109/TNSRE.2020.3038209"},{"type":"inproceedings","key":"gong2017real","title":"Real-Time Human-in-the-Loop Remote Control for a Life-Size Traffic Police Robot with Multiple Augmented Reality Aided Display Terminals","author":"Gong, Liang and Gong, Changyang and Ma, Zhao and Zhao, Lujie and Wang, Zhenyu and Li, Xudong and Jing, Xiaolong and Yang, Haozhe and Liu, Chengliang","booktitle":"2017 2nd International Conference on Advanced Robotics and Mechatronics (ICARM)","pages":"420--425","year":"2017","organization":"IEEE","doi":"10.1109/ICARM.2017.8273199","authors":["Liang Gong","Changyang Gong","Zhao Ma","Lujie Zhao","Zhenyu Wang","Xudong Li","Xiaolong Jing","Haozhe Yang","Chengliang Liu"],"dblp":"conf/icarm/GongGMZWLJYL17","venue":"2017 2nd International Conference on Advanced Robotics and Mechatronics (ICARM)","abstract":"Policing of road traffic is listed in the most hazardous tasks since many traffic police personnel injured in a series of accidents at intersections. A life-size traffic cop robot called \u201cIWI\u201d is invented as an alternative to human traffic police in the street for directing traffic on point duty, and this paper proposes a human-robot cooperation scheme with wearable augmented glasses as an HMI to obtain a more immersing teleoperation experience with the aid of wearable augmented reality glasses. First, a humanoid robot is tailored for mobile surveillance with raspberry pi camera based eyes and omnidirectional Mecanum wheels. Second, the real-time video stream acquired by the on-site robot is distributed to multiple terminals, such as the central sever, the wearable augmented reality glasses and the mobile control tablet. Third, with a depth of focus estimation the augmented indications are displayed to aid to understand the remote scenario. Finally, the human policing results, such as STOP, PULL OVER, TURN-LEFT, are compiled and programmed as simplified patterns to control the robot body/hand pantomime. Experimental results show that the proposed methodology and control scheme is feasible in real-time application with high real-time performance of less than 0.5s latency, and open possibilities of easing the traffic jam via simultaneously scheduling multiple traffic cop robots.","paperId":"a4dce82179e9b36140c6c07d66f06e885516af27","paperTitle":"Real-time human-in-the-loop remote control for a life-size traffic police robot with multiple augmented reality aided display terminals","paperUrl":"https://www.semanticscholar.org/paper/a4dce82179e9b36140c6c07d66f06e885516af27","images":["https://d3i71xaburhd42.cloudfront.net/a4dce82179e9b36140c6c07d66f06e885516af27/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/a4dce82179e9b36140c6c07d66f06e885516af27/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/a4dce82179e9b36140c6c07d66f06e885516af27/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/a4dce82179e9b36140c6c07d66f06e885516af27/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/a4dce82179e9b36140c6c07d66f06e885516af27/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/a4dce82179e9b36140c6c07d66f06e885516af27/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/a4dce82179e9b36140c6c07d66f06e885516af27/6-Figure8-1.png"],"pdf":"https://doi.org/10.1109/ICARM.2017.8273199"},{"type":"inproceedings","key":"estevez2015robot","title":"Robot Devastation: Using DIY Low-Cost Platforms for Multiplayer Interaction in an Augmented Reality Game","author":"Estevez, David and Victores, Juan G and Morante, Santiago and Balaguer, Carlos","booktitle":"2015 7th International Conference on Intelligent Technologies for Interactive Entertainment (INTETAIN)","pages":"32--36","year":"2015","organization":"IEEE","doi":"10.4108/icst.intetain.2015.259753","authors":["David Estevez","Juan G Victores","Santiago Morante","Carlos Balaguer"],"dblp":"journals/eetcc/EstevezVMB15","venue":"2015 7th International Conference on Intelligent Technologies for Interactive Entertainment (INTETAIN)","abstract":"We present Robot Devastation, a multiplayer augmented reality game using low-cost robots. Players can assemble their low-cost robotic platforms and connect them to the central server, commanding them through their home PCs. Several low-cost platforms were developed and tested inside the game.","paperId":"7f9ed51740caa9d9f874d303f47cf3fd187c4cca","paperTitle":"Robot devastation: Using DIY low-cost platforms for multiplayer interaction in an augmented reality game","paperUrl":"https://www.semanticscholar.org/paper/7f9ed51740caa9d9f874d303f47cf3fd187c4cca","images":["https://d3i71xaburhd42.cloudfront.net/7f9ed51740caa9d9f874d303f47cf3fd187c4cca/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/7f9ed51740caa9d9f874d303f47cf3fd187c4cca/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/7f9ed51740caa9d9f874d303f47cf3fd187c4cca/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/7f9ed51740caa9d9f874d303f47cf3fd187c4cca/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/7f9ed51740caa9d9f874d303f47cf3fd187c4cca/5-Figure5-1.png"],"pdf":"http://eudl.eu/pdf/10.4108/icst.intetain.2015.259753"},{"type":"inproceedings","key":"jia2020collision","title":"Collision Detection Based on Augmented Reality for Construction Robot","author":"Jia, Chun and Liu, Zhenzhong","booktitle":"2020 5th International Conference on Advanced Robotics and Mechatronics (ICARM)","pages":"194--197","year":"2020","organization":"IEEE","doi":"10.1109/ICARM49381.2020.9195301","authors":["Chun Jia","Zhenzhong Liu"],"dblp":"conf/icarm/JiaL20","venue":"2020 5th International Conference on Advanced Robotics and Mechatronics (ICARM)","abstract":"With the industrialization of construction, the safe and accurate task of construction robots is faced with higher requirements. Collision detection of construction robots based on human-robot cooperation has become a key technology to be solved urgently. This paper discusses a collision detection system based on augmented reality technology and applied it to tablets. The task scene of the construction robot is converted into an augmented reality (AR) environment in this system, and the user can control the virtual robot for collision detection with the interactive interface. The glass installation simulation experiment proves its effectiveness.","paperId":"e421bfdcd9ea7d16bfdd580bdda74ce941dff82d","paperTitle":"Collision Detection Based on Augmented Reality for Construction Robot","paperUrl":"https://www.semanticscholar.org/paper/e421bfdcd9ea7d16bfdd580bdda74ce941dff82d","images":["https://d3i71xaburhd42.cloudfront.net/e421bfdcd9ea7d16bfdd580bdda74ce941dff82d/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/e421bfdcd9ea7d16bfdd580bdda74ce941dff82d/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/e421bfdcd9ea7d16bfdd580bdda74ce941dff82d/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/e421bfdcd9ea7d16bfdd580bdda74ce941dff82d/2-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/e421bfdcd9ea7d16bfdd580bdda74ce941dff82d/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/e421bfdcd9ea7d16bfdd580bdda74ce941dff82d/3-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/e421bfdcd9ea7d16bfdd580bdda74ce941dff82d/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/e421bfdcd9ea7d16bfdd580bdda74ce941dff82d/4-TableI-1.png"],"pdf":"https://doi.org/10.1109/ICARM49381.2020.9195301"},{"type":"inproceedings","key":"liang2019robot","title":"Robot Teleoperation System Based on Mixed Reality","author":"Liang, Congyuan and Liu, Chao and Liu, Xiaofeng and Cheng, Long and Yang, Chenguang","booktitle":"2019 IEEE 4Th international conference on advanced robotics and mechatronics (ICARM)","pages":"384--389","year":"2019","organization":"IEEE","doi":"10.1109/ICARM.2019.8834302","authors":["Congyuan Liang","Chao Liu","Xiaofeng Liu","Long Cheng","Chenguang Yang"],"dblp":"conf/icarm/LiangLLCY19","venue":"2019 IEEE 4th International Conference on Advanced Robotics and Mechatronics (ICARM)","abstract":"This work develops a novel robot teleoperation system based on mixed reality. Combined with Leap Motion and HoloLens, the system is able to offer a better operate experience by allowing the operator to teleoperate a robot within a mixed reality scene. Besides, we also design a simple writing task in this paper, which verifies the proposed system\u2019s validity.","paperId":"e0d5ba1c4e350cc2a9821741284878eda19cacd0","paperTitle":"Robot Teleoperation System Based on Mixed Reality","paperUrl":"https://www.semanticscholar.org/paper/e0d5ba1c4e350cc2a9821741284878eda19cacd0","images":["https://d3i71xaburhd42.cloudfront.net/e0d5ba1c4e350cc2a9821741284878eda19cacd0/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/e0d5ba1c4e350cc2a9821741284878eda19cacd0/4-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/e0d5ba1c4e350cc2a9821741284878eda19cacd0/5-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/e0d5ba1c4e350cc2a9821741284878eda19cacd0/5-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/e0d5ba1c4e350cc2a9821741284878eda19cacd0/6-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/e0d5ba1c4e350cc2a9821741284878eda19cacd0/1-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/e0d5ba1c4e350cc2a9821741284878eda19cacd0/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/e0d5ba1c4e350cc2a9821741284878eda19cacd0/2-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/e0d5ba1c4e350cc2a9821741284878eda19cacd0/2-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/e0d5ba1c4e350cc2a9821741284878eda19cacd0/2-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/e0d5ba1c4e350cc2a9821741284878eda19cacd0/3-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/e0d5ba1c4e350cc2a9821741284878eda19cacd0/3-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/e0d5ba1c4e350cc2a9821741284878eda19cacd0/3-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/e0d5ba1c4e350cc2a9821741284878eda19cacd0/5-TableI-1.png"],"pdf":"https://hal-lirmm.ccsd.cnrs.fr/lirmm-02315582/file/08834302.pdf"},{"type":"inproceedings","key":"zalud2007augmented","title":"Augmented Reality User Interface for Reconnaissance Robotic Missions","author":"Zalud, Ludek","booktitle":"RO-MAN 2007-The 16th IEEE International Symposium on Robot and Human Interactive Communication","pages":"974--979","year":"2007","organization":"IEEE","doi":"10.1109/ROMAN.2007.4415224","authors":["Ludek Zalud"],"dblp":"conf/ro-man/Zalud07","venue":"RO-MAN 2007 - The 16th IEEE International Symposium on Robot and Human Interactive Communication","abstract":"The problem of visual telepresence and augmented reality control of reconnaissance mobile robots is described. ARGOS (advanced robotic graphical operation system) for teleoperation of various mobile robots through sensory supported visual telepresence is presented. Two robots - Orpheus and Hermes, and one embedded system Orpheus EB - made on Department of Control and Instrumentation (DCI) are described as examples of systems with different features and capabilities that may be controlled through ARGOS. Data fusion of CCD color camera data, thermovision data and 3D proximity data through extended 3D evidence grids is described.","paperId":"a1b5606b0b6303928f3e98df1c7644bcbbef6fa8","paperTitle":"Augmented Reality User Interface for Reconnaissance Robotic Missions","paperUrl":"https://www.semanticscholar.org/paper/a1b5606b0b6303928f3e98df1c7644bcbbef6fa8","images":["https://d3i71xaburhd42.cloudfront.net/a1b5606b0b6303928f3e98df1c7644bcbbef6fa8/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/a1b5606b0b6303928f3e98df1c7644bcbbef6fa8/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/a1b5606b0b6303928f3e98df1c7644bcbbef6fa8/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/a1b5606b0b6303928f3e98df1c7644bcbbef6fa8/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/a1b5606b0b6303928f3e98df1c7644bcbbef6fa8/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/a1b5606b0b6303928f3e98df1c7644bcbbef6fa8/5-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/a1b5606b0b6303928f3e98df1c7644bcbbef6fa8/5-Figure10-1.png"],"pdf":"https://doi.org/10.1109/ROMAN.2007.4415224"},{"type":"inproceedings","key":"park2007teleoperation","title":"Teleoperation of a Multi-Purpose Robot over the Internet Using Augmented Reality","author":"Park, Hyeshin and Lim, Yo-An and Pervez, Aslam and Lee, Beom-Chan and Lee, Sang-Goog and Ryu, Jeha","booktitle":"2007 International Conference on Control, Automation and Systems","pages":"2456--2461","year":"2007","organization":"IEEE","doi":"10.1109/ICCAS.2007.4406776","authors":["Hyeshin Park","Yo-An Lim","Aslam Pervez","Beom-Chan Lee","Sang-Goog Lee","Jeha Ryu"],"venue":"2007 International Conference on Control, Automation and Systems","abstract":"Bilateral teleoperation using augmented reality is proposed for a multi-purpose robot called SpiderBot-II that is an indoor-installed wire-driven parallel manipulator. It is intended to be used for various applications, including in-house rehabilitation training and daily life assistance such as walking assistance and health monitoring, especially for the elderly or the handicapped that spends most of time at home. Through the teleoperation over the Internet, a therapist or an attendant in a remote site can give help to the physically disabled by directly manipulating the robot. For easy recognition of the obstacle, predefined markers are attached to each obstacle in the workspace. For better teleoperation, moreover, reaction force between obstacles and the end-effector, which is calculated using force field, is given to a remote operator and this enables the operator to perform the teleoperation more effectively. Force field, which is proportional to proximity between obstacles and the end-effector, is generated to facilitate the obstacle avoidance and visually augmented on the operator\'s screen for better recognition of obstacles.","paperId":"dd3e30d1c6c8fd1a99acc041ada4e47d465cdb2d","paperTitle":"Teleoperation of a multi-purpose robot over the internet using augmented reality","paperUrl":"https://www.semanticscholar.org/paper/dd3e30d1c6c8fd1a99acc041ada4e47d465cdb2d","images":["https://d3i71xaburhd42.cloudfront.net/dd3e30d1c6c8fd1a99acc041ada4e47d465cdb2d/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/dd3e30d1c6c8fd1a99acc041ada4e47d465cdb2d/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/dd3e30d1c6c8fd1a99acc041ada4e47d465cdb2d/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/dd3e30d1c6c8fd1a99acc041ada4e47d465cdb2d/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/dd3e30d1c6c8fd1a99acc041ada4e47d465cdb2d/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/dd3e30d1c6c8fd1a99acc041ada4e47d465cdb2d/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/dd3e30d1c6c8fd1a99acc041ada4e47d465cdb2d/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/dd3e30d1c6c8fd1a99acc041ada4e47d465cdb2d/5-Figure9-1.png"],"pdf":"https://doi.org/10.1109/ICCAS.2007.4406776"},{"type":"inproceedings","key":"bentz2019unsupervised","title":"Unsupervised Learning of Assistive Camera Views by an Aerial Co-Robot in Augmented Reality Multitasking Environments","author":"Bentz, William and Dhanjal, Sahib and Panagou, Dimitra","booktitle":"2019 International Conference on Robotics and Automation (ICRA)","pages":"3003--3009","year":"2019","organization":"IEEE","doi":"10.1109/ICRA.2019.8793587","authors":["William Bentz","Sahib Dhanjal","Dimitra Panagou"],"dblp":"conf/icra/BentzDP19","venue":"2019 International Conference on Robotics and Automation (ICRA)","abstract":"This paper presents a novel method by which an assistive aerial robot can learn the relevant camera views within a task domain through tracking the head motions of a human collaborator. The human\u2019s visual field is modeled as an anisotropic spherical sensor, which decays in acuity towards the periphery, and is integrated in time throughout the domain. This data is resampled and fed into an expectation maximization solver in order to estimate the environment\u2019s visual interest as a mixture of Gaussians. A dynamic coverage control law directs the robot to capture camera views of the peaks of these Gaussians which is broadcast to an augmented reality display worn by the human operator. An experimental study is presented that assesses the influence of the assitive robot on reflex time, head motion, and task completion time.","paperId":"7cc61ba78a541862650342737766bb565ef2b410","paperTitle":"Unsupervised Learning of Assistive Camera Views by an Aerial Co-robot in Augmented Reality Multitasking Environments","paperUrl":"https://www.semanticscholar.org/paper/7cc61ba78a541862650342737766bb565ef2b410","images":["https://d3i71xaburhd42.cloudfront.net/7cc61ba78a541862650342737766bb565ef2b410/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/7cc61ba78a541862650342737766bb565ef2b410/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/7cc61ba78a541862650342737766bb565ef2b410/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/7cc61ba78a541862650342737766bb565ef2b410/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/7cc61ba78a541862650342737766bb565ef2b410/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/7cc61ba78a541862650342737766bb565ef2b410/5-TableI-1.png"],"pdf":"http://www-personal.umich.edu/~dpanagou/assets/documents/WBentz_ICRA19.pdf"},{"type":"inproceedings","key":"zhang2020vision","title":"Vision Tracking Algorithm for Augmented Reality System of Teleoperation Mobile Robots","author":"Zhang, Dongpu and Tian, Lin and Huang, Kewu and Wang, Jiwu","booktitle":"2020 3rd International Conference on Unmanned Systems (ICUS)","pages":"1047--1052","year":"2020","organization":"IEEE","doi":"10.1109/ICUS50048.2020.9274917","authors":["Dongpu Zhang","Lin Tian","Kewu Huang","Jiwu Wang"],"venue":"2020 3rd International Conference on Unmanned Systems (ICUS)","abstract":"Teleoperation mobile robot is an effective solution to the operation in unfamiliar, dangerous or unreachable environment in the deep space. With Augmented Reality(AR), teleoperation mobile robots can reach high accuracy and efficiency. To get perfect AR effect, the feature extraction and matching algorithm are achieved based on reconstruction model and k-d tree. The pose estimation used LM algorithm is discussed. A new vision tracking algorithm is built-up based on 3D reconstruction model for AR teleoperation mobile robot. With experiments, it can proved that the new vision tracking algorithm can meet the requirement of teleoperation mobile robot systems in manned deep space exploration.","paperId":"3d06d91115d4b22149721fea2f071c712be66797","paperTitle":"Vision Tracking Algorithm for Augmented Reality System of Teleoperation Mobile Robots","paperUrl":"https://www.semanticscholar.org/paper/3d06d91115d4b22149721fea2f071c712be66797","images":[],"pdf":"https://doi.org/10.1109/ICUS50048.2020.9274917"},{"type":"inproceedings","key":"renner2018wysiwicd","title":"Wysiwicd: What You See Is What I Can Do","author":"Renner, Patrick and Lier, Florian and Friese, Felix and Pfeiffer, Thies and Wachsmuth, Sven","booktitle":"Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction","pages":"382--382","year":"2018","doi":"10.1145/3173386.3177533","authors":["Patrick Renner","Florian Lier","Felix Friese","Thies Pfeiffer","Sven Wachsmuth"],"dblp":"conf/hri/RennerLFPW18a","venue":"HRI","abstract":"Mobile robots start to appear in everyday life, e.g., in shopping malls or nursing homes. Often, they are operated by staff with no prior experience in robotics. False expectations regarding the capabilities of robots, however, may lead to disappointments and reservations when it comes to accepting robots in ones personal space. We make use of state-of-the-art Mixed Reality (MR) technology by integrating a Microsoft HoloLens into the robot\xbbs operating space to facilitate acceptance and interaction. The MR device is used to increase situation awareness by (a) externalizing the robot\xbbs behavior-state and sensor data and (b) projecting planned behavior. In addition to that, MR technology is (c) used as satellite sensing and display device for human-robot-communication.","paperId":"fa9ce7962e1ee803642c5f041482e0c4c46535c8","paperTitle":"WYSIWICD: What You See is What I Can Do","paperUrl":"https://www.semanticscholar.org/paper/fa9ce7962e1ee803642c5f041482e0c4c46535c8","images":["https://d3i71xaburhd42.cloudfront.net/fa9ce7962e1ee803642c5f041482e0c4c46535c8/1-Figure1-1.png"],"pdf":"https://doi.org/10.1145/3173386.3177533"},{"type":"inproceedings","key":"lee2018augmented","title":"Augmented Reality in Human-Robot Cooperative Search","author":"Lee, Kevin and Reardon, Christopher and Fink, Jonathan","booktitle":"2018 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)","pages":"1--1","year":"2018","organization":"IEEE","doi":"10.1109/SSRR.2018.8468659","authors":["Kevin Lee","Christopher Reardon","Jonathan Fink"],"dblp":"conf/ssrr/LeeRF18","venue":"2018 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)","abstract":"Robots operating alongside humans in field environments have the potential to greatly increase the situational awareness of their human teammates. A significant challenge, however, is the efficient conveyance of what the robot perceives to the human in order to achieve improved situational awareness. We believe augmented reality (AR), which allows a human to simultaneously perceive the real world and digital information situated virtually in the real world, has the potential to address this issue. We propose to demonstrate that augmented reality can be used to enable human-robot cooperative search, where the robot can both share search results and assist the human teammate in navigating to a search target.","paperId":"5a4d682391640bff7e159ab49e19bb62f501e53a","paperTitle":"Augmented Reality in Human-Robot Cooperative Search","paperUrl":"https://www.semanticscholar.org/paper/5a4d682391640bff7e159ab49e19bb62f501e53a","images":["https://d3i71xaburhd42.cloudfront.net/5a4d682391640bff7e159ab49e19bb62f501e53a/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/5a4d682391640bff7e159ab49e19bb62f501e53a/1-Figure2-1.png"],"pdf":"https://doi.org/10.1109/SSRR.2018.8468659"},{"type":"inproceedings","key":"korthauer2020watch","title":"Watch Your Vehicle Driving at the City: Interior HMI with Augmented Reality for Automated Driving","author":"Korthauer, Andreas and Guenther, Clemens and Hinrichs, Andreas and Ren, Wen and Yang, Yiwen","booktitle":"22nd International Conference on Human-Computer Interaction with Mobile Devices and Services","pages":"1--5","year":"2020","doi":"10.1145/3406324.3425895","authors":["Andreas Korthauer","Clemens Guenther","Andreas Hinrichs","Wen Ren","Yiwen Yang"],"dblp":"conf/mhci/KorthauerGHRY20","venue":"MobileHCI","abstract":"It is very likely that automated vehicles will have to operate in mixed traffic environments for a longer period in the future. To deploy highly automated vehicles (HAV) in urban mixed traffic, it is essential to be able to communicate externally with other human road users (drivers of non-automated vehicles, cyclists, pedestrians) and internally with the passenger of the HAV. Internal communication through an interior human machine interface (HMI) keeps the passenger informed about the HAV\'s automation state and actions, in order to build trust and enable a more comfortable ride experience. In this study, we explore augmented reality (AR) as part of an interior HMI for this purpose. We follow a user-centric approach with user research, prototyping, and user experience evaluation. Results suggest that the passengers of HAVs have specific information needs concerning surrounding traffic and the current driving situation. AR could help to serve this need, for instance by highlighting relevant objects in the driving scene.","paperId":"7e7e9e22870e08e2be69a2ba54a955a6a8e76d81","paperTitle":"Watch Your Vehicle Driving at the City: Interior HMI with Augmented Reality for Automated Driving","paperUrl":"https://www.semanticscholar.org/paper/7e7e9e22870e08e2be69a2ba54a955a6a8e76d81","images":["https://d3i71xaburhd42.cloudfront.net/7e7e9e22870e08e2be69a2ba54a955a6a8e76d81/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/7e7e9e22870e08e2be69a2ba54a955a6a8e76d81/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/7e7e9e22870e08e2be69a2ba54a955a6a8e76d81/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/7e7e9e22870e08e2be69a2ba54a955a6a8e76d81/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/7e7e9e22870e08e2be69a2ba54a955a6a8e76d81/5-Figure5-1.png"],"pdf":"https://doi.org/10.1145/3406324.3425895"},{"type":"inproceedings","key":"dietrich2010visualization","title":"Visualization of Robot\'s Awareness and Perception","author":"Dietrich, Andr\'e and Schulze, Michael and Zug, Sebastian and Kaiser, J\\"org","booktitle":"Proceedings of the First International Workshop on Digital Engineering","pages":"38--44","year":"2010","doi":"10.1145/1837154.1837160","authors":["Andr\'e Dietrich","Michael Schulze","Sebastian Zug","J\\"org Kaiser"],"dblp":"conf/iwde/DietrichSZK10","venue":"IWDE \'10","abstract":"Today, direct interaction between humans and robots is limited, although the combination of human flexibility and robots power enables a growing productivity. The problem for humans lies in the nearly unpredictable behavior and motion of the robot itself. However, we can enhance human\'s view with more information to get knowledge about robot\'s perception and awareness. We use Augmented Reality methods for providing the information in an adaptable visualization for different user types. We show that our approach leads to shorter development cycles as well as to safer human-robot interaction.","paperId":"27902c623aec0229930956725a8d64b8633cb589","paperTitle":"Visualization of robot\'s awareness and perception","paperUrl":"https://www.semanticscholar.org/paper/27902c623aec0229930956725a8d64b8633cb589","images":["https://d3i71xaburhd42.cloudfront.net/27902c623aec0229930956725a8d64b8633cb589/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/27902c623aec0229930956725a8d64b8633cb589/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/27902c623aec0229930956725a8d64b8633cb589/4-Figure3-1.png"],"pdf":"http://famouso.sourceforge.net/publications/2010/IWDE10DietrichSchulzeZugKaiser.pdf"},{"type":"inproceedings","key":"zhu2016virtually","title":"Virtually Adapted Reality and Algorithm Visualization for Autonomous Robots","author":"Zhu, Danny and Veloso, Manuela","booktitle":"Robot World Cup","pages":"452--464","year":"2016","organization":"Springer","doi":"10.1007/978-3-319-68792-6_38","authors":["Danny Zhu","Manuela Veloso"],"dblp":"conf/robocup/ZhuV16","venue":"RoboCup","abstract":"Autonomous mobile robots are often videotaped during operation, whether for later evaluation by their developers or for demonstration of the robots to others. Watching such videos is engaging and interesting. However, clearly the plain videos do not show detailed information about the algorithms running on the moving robots, leading to a rather limited visual understanding of the underlying autonomy. Researchers have resorted to following the autonomous robots algorithms through a variety of methods, most commonly graphical user interfaces running on offboard screens and separated from the captured videos. Such methods enable considerable debugging, but still have limited effectiveness, as there is an inevitable visual mismatch with the video capture. In this work, we aim to break this disconnect, and we contribute the ability to overlay visualizations onto a video, to extract the robot\u2019s algorithms, in particular to follow its route planning and execution. We further provide mechanisms to create and visualize virtual adaptations of the real environment to enable the exploration of the behavior of the algorithms in new situations. We demonstrate the complete implementation with an autonomous quadrotor navigating in a lab environment using the rapidly-exploring random tree algorithm. We briefly motivate and discuss our follow-up visualization work for our complex small-size robot soccer team.","paperId":"f9a502baba68e5d1f321de0daa9c1c6abb29cca3","paperTitle":"Virtually Adapted Reality and Algorithm Visualization for Autonomous Robots","paperUrl":"https://www.semanticscholar.org/paper/f9a502baba68e5d1f321de0daa9c1c6abb29cca3","images":["https://d3i71xaburhd42.cloudfront.net/f9a502baba68e5d1f321de0daa9c1c6abb29cca3/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/f9a502baba68e5d1f321de0daa9c1c6abb29cca3/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/f9a502baba68e5d1f321de0daa9c1c6abb29cca3/7-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/f9a502baba68e5d1f321de0daa9c1c6abb29cca3/7-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/f9a502baba68e5d1f321de0daa9c1c6abb29cca3/7-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/f9a502baba68e5d1f321de0daa9c1c6abb29cca3/10-Figure6-1.png"],"pdf":"http://www.cs.cmu.edu/afs/cs/user/mmv/www/papers/16robocup-danny.pdf"},{"type":"inproceedings","key":"boateng2021virtual","title":"Virtual Shadow Rendering for Maintaining Situation Awareness in Proximal Human-Robot Teaming","author":"Boateng, Andrew and Zhang, Yu","booktitle":"Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction","pages":"494--498","year":"2021","doi":"10.1145/3434074.3447221","authors":["Andrew Boateng","Yu Zhang"],"dblp":"conf/hri/BoatengZ21","venue":"HRI","abstract":"One focus of augmented reality (AR) in robotics has been on enriching the interface for human-robot interaction. While such an interface is often made intuitive to interact with, it invariably imposes novel objects into the environment. In situations where the human already has a focus, such as in a human-robot teaming task, these objects can potentially overload our senses and lead to degraded teaming performance. In this paper, we propose using AR objects to solely augment natural objects to avoid disrupting our natural senses while adding critical information about the current situation. In particular, our case study focuses on addressing the limited field of view of humans by incorporating persistent virtual shadows of robots for maintaining situation awareness in proximal human-robot teaming tasks.","paperId":"7f1afbb0cf53ab833723c623d4cb1e016f9090e7","paperTitle":"Virtual Shadow Rendering for Maintaining Situation Awareness in Proximal Human-Robot Teaming","paperUrl":"https://www.semanticscholar.org/paper/7f1afbb0cf53ab833723c623d4cb1e016f9090e7","images":["https://d3i71xaburhd42.cloudfront.net/7f1afbb0cf53ab833723c623d4cb1e016f9090e7/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/7f1afbb0cf53ab833723c623d4cb1e016f9090e7/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/7f1afbb0cf53ab833723c623d4cb1e016f9090e7/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/7f1afbb0cf53ab833723c623d4cb1e016f9090e7/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/7f1afbb0cf53ab833723c623d4cb1e016f9090e7/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/7f1afbb0cf53ab833723c623d4cb1e016f9090e7/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/7f1afbb0cf53ab833723c623d4cb1e016f9090e7/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/7f1afbb0cf53ab833723c623d4cb1e016f9090e7/4-Figure8-1.png"],"pdf":"https://doi.org/10.1145/3434074.3447221"},{"type":"article","key":"pai2016virtual","title":"Virtual Planning, Control, and Machining for a Modular-Based Automated Factory Operation in an Augmented Reality Environment","author":"Pai, Yun Suen and Yap, Hwa Jen and Dawal, Siti Zawiah Md and Ramesh, S and Phoon, Sin Ye","journal":"Scientific reports","volume":"6","number":"1","pages":"1--19","year":"2016","publisher":"Nature Publishing Group","doi":"10.1038/srep27380","authors":["Yun Suen Pai","Hwa Jen Yap","Siti Zawiah Md Dawal","S Ramesh","Sin Ye Phoon"],"venue":"Scientific reports","abstract":"This study presents a modular-based implementation of augmented reality to provide an immersive experience in learning or teaching the planning phase, control system, and machining parameters of a fully automated work cell. The architecture of the system consists of three code modules that can operate independently or combined to create a complete system that is able to guide engineers from the layout planning phase to the prototyping of the final product. The layout planning module determines the best possible arrangement in a layout for the placement of various machines, in this case a conveyor belt for transportation, a robot arm for pick-and-place operations, and a computer numerical control milling machine to generate the final prototype. The robotic arm module simulates the pick-and-place operation offline from the conveyor belt to a computer numerical control (CNC) machine utilising collision detection and inverse kinematics. Finally, the CNC module performs virtual machining based on the Uniform Space Decomposition method and axis aligned bounding box collision detection. The conducted case study revealed that given the situation, a semi-circle shaped arrangement is desirable, whereas the pick-and-place system and the final generated G-code produced the highest deviation of 3.83\u2009mm and 5.8\u2009mm respectively.","paperId":"60c1ac15e82148a6167a8da250dd85aa41fd3ee5","paperTitle":"Virtual Planning, Control, and Machining for a Modular-Based Automated Factory Operation in an Augmented Reality Environment","paperUrl":"https://www.semanticscholar.org/paper/60c1ac15e82148a6167a8da250dd85aa41fd3ee5","images":["https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/3-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/4-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/7-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/11-Table3-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/12-Table4-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/7-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/13-Table5-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/7-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/8-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/8-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/8-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/9-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/9-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/10-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/11-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/11-Figure14-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/12-Figure15-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/13-Figure16-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/13-Figure17-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/14-Figure18-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/14-Figure19-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/14-Figure20-1.png","https://d3i71xaburhd42.cloudfront.net/e3966745d7796a6a46569da59f7b49e186fb39ef/15-Figure21-1.png"],"pdf":"https://www.nature.com/articles/srep27380.pdf"},{"type":"inproceedings","key":"takashima2013transformtable","title":"TransformTable: A Self-Actuated Shape-Changing Digital Table","author":"Takashima, Kazuki and Aida, Naohiro and Yokoyama, Hitomi and Kitamura, Yoshifumi","booktitle":"Proceedings of the 2013 ACM international conference on Interactive tabletops and surfaces","pages":"179--188","year":"2013","doi":"10.1145/2512349.2512818","authors":["Kazuki Takashima","Naohiro Aida","Hitomi Yokoyama","Yoshifumi Kitamura"],"dblp":"conf/tabletop/TakashimaAYK13","venue":"ITS","abstract":"This paper proposes TransformTable, an interactive digital table, whose shape can be physically and dynamically deformed. Shape transformations are mechanically and electrically actuated by wireless signals from a host computer. TransfomTable represents digital information in a physically changeable screen shape and simultaneously produces different spatial arrangements of users around the table. This provides visual information while changing the physical workspace to allow users to effectively handle their tasks. We implemented the first TransformTable prototype that can deform from/into one of three typical shapes: round, square, or rectangular. We also discuss implementation methods and further application designs and scenarios. A preliminary study shows fundamental and potential social impacts of the table transformation on users\' subjective views in a group conversation.","paperId":"6348a414c266ac960b18eb1d6194218b83a40d73","paperTitle":"TransformTable: a self-actuated shape-changing digital table","paperUrl":"https://www.semanticscholar.org/paper/6348a414c266ac960b18eb1d6194218b83a40d73","images":["https://d3i71xaburhd42.cloudfront.net/6348a414c266ac960b18eb1d6194218b83a40d73/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/6348a414c266ac960b18eb1d6194218b83a40d73/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/6348a414c266ac960b18eb1d6194218b83a40d73/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/6348a414c266ac960b18eb1d6194218b83a40d73/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/6348a414c266ac960b18eb1d6194218b83a40d73/6-Figure6-1.png"],"pdf":"http://www.interaction-ipsj.org/proceedings/2014/data/20140220/14INT006.pdf"},{"type":"inproceedings","key":"caiza2020augmented","title":"Augmented Reality for Robot Control in Low-Cost Automation Context and IoT","author":"Caiza, Gustavo and Bonilla-Vasconez, Pablo and Garcia, Carlos A and Garcia, Marcelo V","booktitle":"2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)","volume":"1","pages":"1461--1464","year":"2020","organization":"IEEE","doi":"10.1109/ETFA46521.2020.9212056","authors":["Gustavo Caiza","Pablo Bonilla-Vasconez","Carlos A Garcia","Marcelo V Garcia"],"dblp":"conf/etfa/CaizaBGG20","venue":"2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)","abstract":"Augmented Reality (AR) is one of the key technology in the Industry 4.0 development. The robotic manufacturing is very useful because of their versatility and repeatability in industrial tasks. However, as technologies advance, complex and high-cost control systems are required, hence the need to create low-cost and efficient control systems, with low complexity when operating them. A virtual control proposed in Unity 3D allowing the application to be much more interactive and very easy for the user. The authors describe the working scenario, the overall architecture, communication protocol in the context of the Internet of Things (IoT) and give design and implementation details on the AR application.","paperId":"85ee47c3e24b9967e69323112022c80c116443dc","paperTitle":"Augmented Reality for Robot Control in Low-cost Automation Context and IoT","paperUrl":"https://www.semanticscholar.org/paper/85ee47c3e24b9967e69323112022c80c116443dc","images":["https://d3i71xaburhd42.cloudfront.net/85ee47c3e24b9967e69323112022c80c116443dc/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/85ee47c3e24b9967e69323112022c80c116443dc/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/85ee47c3e24b9967e69323112022c80c116443dc/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/85ee47c3e24b9967e69323112022c80c116443dc/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/85ee47c3e24b9967e69323112022c80c116443dc/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/85ee47c3e24b9967e69323112022c80c116443dc/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/85ee47c3e24b9967e69323112022c80c116443dc/4-Figure7-1.png"],"pdf":"https://doi.org/10.1109/ETFA46521.2020.9212056"},{"type":"inproceedings","key":"araiza2019augmented","title":"Augmented Reality for Quick and Intuitive Robotic Packing Re-Programming","author":"Araiza-Illan, Dejanira and De San Bernabe, Alberto and Hongchao, Fang and Shin, Leong Yong","booktitle":"2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","pages":"664--664","year":"2019","organization":"IEEE","doi":"10.1109/HRI.2019.8673327","authors":["Dejanira Araiza-Illan","Alberto De San Bernabe","Fang Hongchao","Leong Yong Shin"],"dblp":"conf/hri/Araiza-IllanBFS19","venue":"2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","abstract":"Current manufacturing applications are subject to constant changes in production orders for their robotic systems to adapt to the dynamic nature of the market. Hence, reprogramming robots needs to be a fast, easy and effective process. In this demonstration, we present an augmented reality (AR) interface using HoloLens. Our interface provides an intuitive platform to re-program a robotic packing application through simple hand gestures and the information gathered by the HoloLens\' spatial mapping functionality.","paperId":"8f9b1e610f82a1279cc30ee781e65423c4843c76","paperTitle":"Augmented Reality for Quick and Intuitive Robotic Packing Re-Programming","paperUrl":"https://www.semanticscholar.org/paper/8f9b1e610f82a1279cc30ee781e65423c4843c76","images":["https://d3i71xaburhd42.cloudfront.net/8f9b1e610f82a1279cc30ee781e65423c4843c76/1-Figure1-1.png"],"pdf":"https://doi.org/10.1109/HRI.2019.8673327"},{"type":"article","key":"sugimoto2005time","title":"Time Follower\'s Vision: A Teleoperation Interface with past Images","author":"Sugimoto, Maki and Kagotani, Georges and Nii, Hideaki and Shiroma, Naoji and Matsuno, Fumitoshi and Inami, Masahiko","journal":"IEEE Computer Graphics and Applications","volume":"25","number":"1","pages":"54--63","year":"2005","publisher":"IEEE","doi":"10.1109/MCG.2005.23","authors":["Maki Sugimoto","Georges Kagotani","Hideaki Nii","Naoji Shiroma","Fumitoshi Matsuno","Masahiko Inami"],"dblp":"journals/cga/SugimotoKNSIM05","venue":"IEEE Computer Graphics and Applications","abstract":"Time-Followers\'s Vision is a mixed-reality-based visual presentation system that captures a robotic vehicle\'s size, position, and environment, allowing even inexperienced operators to easily control it. The technique produces a virtual image using mixed reality technology and presents the vehicle\'s surrounding environment and status to the operator. Therefore, even for inexperienced operators, the vehicle\'s position and orientation and the surrounding situation can be readily understood. The authors implement a prototype system and evaluate its feasibility. This article is available with a short video documentary on CD-ROM.","paperId":"568da261f44fd833291fbd8fb18ac384fb964c83","paperTitle":"Time Follower\'s Vision: a teleoperation interface with past images","paperUrl":"https://www.semanticscholar.org/paper/568da261f44fd833291fbd8fb18ac384fb964c83","images":["https://d3i71xaburhd42.cloudfront.net/568da261f44fd833291fbd8fb18ac384fb964c83/4-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/568da261f44fd833291fbd8fb18ac384fb964c83/1-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/568da261f44fd833291fbd8fb18ac384fb964c83/5-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/568da261f44fd833291fbd8fb18ac384fb964c83/1-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/568da261f44fd833291fbd8fb18ac384fb964c83/6-Table3-1.png","https://d3i71xaburhd42.cloudfront.net/568da261f44fd833291fbd8fb18ac384fb964c83/7-Table4-1.png","https://d3i71xaburhd42.cloudfront.net/568da261f44fd833291fbd8fb18ac384fb964c83/4-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/568da261f44fd833291fbd8fb18ac384fb964c83/7-Figure15-1.png"],"pdf":"http://www.researchgate.net/profile/Fumitoshi_Matsuno/publication/8041847_Time_follower\'s_vision_a_teleoperation_interface_with_past_images/links/02e7e524fd902f404b000000.pdf"},{"type":"inproceedings","key":"kim2014task","title":"Task-Oriented Teleoperation through Natural 3D User Interaction","author":"Kim, Hyoungnyoun and Kim, Jun-Sik and Ryu, Kwanghyun and Cheon, Seyoung and Oh, Yonghwan and Park, Ji-Hyung","booktitle":"2014 11th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)","pages":"335--338","year":"2014","organization":"IEEE","doi":"10.1109/URAI.2014.7057536","authors":["Hyoungnyoun Kim","Jun-Sik Kim","Kwanghyun Ryu","Seyoung Cheon","Yonghwan Oh","Ji-Hyung Park"],"dblp":"conf/urai/KimKRCOP14","venue":"2014 11th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)","abstract":"Teleoperation provides a technical means to perform desired tasks in remote environments. Teleoperation using direct control and haptic-mediated interactions requires significant effort for unskilled users to understand how to operate the robot. Task-oriented teleoperation supports human-level understanding to directly transfer the meaning of tasks from the user to the robot. In this paper, we propose a natural 3D interface to transfer task knowledge to a remote robot. We design a 3D manipulation system in a mixed reality environment that combines human hand gestures and a 3D view of the remote robot. We demonstrate that the remote robot successfully executes the ordered task even in dynamic environments.","paperId":"9421b1831edeab5fcc50a88ce1598a566b9dc1f9","paperTitle":"Task-oriented teleoperation through natural 3D user interaction","paperUrl":"https://www.semanticscholar.org/paper/9421b1831edeab5fcc50a88ce1598a566b9dc1f9","images":["https://d3i71xaburhd42.cloudfront.net/9421b1831edeab5fcc50a88ce1598a566b9dc1f9/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/9421b1831edeab5fcc50a88ce1598a566b9dc1f9/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/9421b1831edeab5fcc50a88ce1598a566b9dc1f9/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/9421b1831edeab5fcc50a88ce1598a566b9dc1f9/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/9421b1831edeab5fcc50a88ce1598a566b9dc1f9/4-Figure5-1.png"],"pdf":"https://doi.org/10.1109/URAI.2014.7057536"},{"type":"inproceedings","key":"schmitt2021assisted","title":"Assisted Human-Robot-Interaction for Industrial Assembly: Application of Spatial Augmented Reality (SAR) for Collaborative Assembly Tasks","author":"Schmitt, Jan and Hillenbrand, Andreas and Kranz, Philipp and Kaupp, Tobias","booktitle":"Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction","pages":"52--56","year":"2021","doi":"10.1145/3434074.3447127","authors":["Jan Schmitt","Andreas Hillenbrand","Philipp Kranz","Tobias Kaupp"],"dblp":"conf/hri/SchmittHKK21","venue":"HRI","abstract":"Human-robot collaboration is increasingly applied to industrial assembly sequences due to the growing need for flexibility in manufacturing. Assistant systems are able help to support shared assembly sequences to facilitate collaboration. This contribution shows a workplace installation of a collaborative robot (Cobot) and a spatial augmented reality (SAR) assistant system applied to an assembly use-case. We demonstrate a methodology for the distribution of the assembly sequence between the worker, the Cobot, and the SAR.","paperId":"d6f84f428255ccb63ee46231094172d03319a211","paperTitle":"Assisted Human-Robot-Interaction for Industrial Assembly: Application of Spatial Augmented Reality (SAR) for Collaborative Assembly Tasks","paperUrl":"https://www.semanticscholar.org/paper/d6f84f428255ccb63ee46231094172d03319a211","images":["https://d3i71xaburhd42.cloudfront.net/d6f84f428255ccb63ee46231094172d03319a211/2-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/d6f84f428255ccb63ee46231094172d03319a211/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/d6f84f428255ccb63ee46231094172d03319a211/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/d6f84f428255ccb63ee46231094172d03319a211/3-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/d6f84f428255ccb63ee46231094172d03319a211/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/d6f84f428255ccb63ee46231094172d03319a211/4-Figure4-1.png"],"pdf":"https://dl.acm.org/doi/pdf/10.1145/3434074.3447127"},{"type":"inproceedings","key":"sugimoto2011robotable2","title":"RoboTable2: A Novel Programming Environment Using Physical Robots on a Tabletop Platform","author":"Sugimoto, Masanori and Fujita, Tomoki and Mi, Haipeng and Krzywinski, Aleksander","booktitle":"Proceedings of the 8th International Conference on Advances in Computer Entertainment Technology","pages":"1--8","year":"2011","doi":"10.1145/2071423.2071436","authors":["Masanori Sugimoto","Tomoki Fujita","Haipeng Mi","Aleksander Krzywinski"],"dblp":"conf/ACMace/SugimotoFMK11","venue":"Advances in Computer Entertainment Technology","abstract":"In this paper, we propose a novel environment called RoboTable2 that supports users with limited programming knowledge or experience in conducting robot programming. We have devised a tabletop platform that can simultaneously recognize multi-touch input and track physical robots, enabling users to conduct robot-programming tasks in an intuitive manner. A user study comparing the proposed and a conventional graphical programming environment was conducted, involving ten university students with no programming experience. The effects of the proposed environment were clarified via video analysis, questionnaires and usage logs. A pilot study was also conducted to verify how easily users could design and develop applications on RoboTable2. The lessons learned with respect to design guidelines for the proposed programming environment and issues for investigation are discussed.","paperId":"d6d78863de3f842d8c2d7e57d002864791a04550","paperTitle":"RoboTable2: a novel programming environment using physical robots on a tabletop platform","paperUrl":"https://www.semanticscholar.org/paper/d6d78863de3f842d8c2d7e57d002864791a04550","images":["https://d3i71xaburhd42.cloudfront.net/d6d78863de3f842d8c2d7e57d002864791a04550/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/d6d78863de3f842d8c2d7e57d002864791a04550/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/d6d78863de3f842d8c2d7e57d002864791a04550/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/d6d78863de3f842d8c2d7e57d002864791a04550/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/d6d78863de3f842d8c2d7e57d002864791a04550/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/d6d78863de3f842d8c2d7e57d002864791a04550/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/d6d78863de3f842d8c2d7e57d002864791a04550/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/d6d78863de3f842d8c2d7e57d002864791a04550/6-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/d6d78863de3f842d8c2d7e57d002864791a04550/7-Figure11-1.png"],"pdf":"https://doi.org/10.1145/2071423.2071436"},{"type":"inproceedings","key":"krzywinski2009robotable","title":"RoboTable: A Tabletop Framework for Tangible Interaction with Robots in a Mixed Reality","author":"Krzywinski, Aleksander and Mi, Haipeng and Chen, Weiqin and Sugimoto, Masanori","booktitle":"proceedings of the international conference on advances in computer Enterntainment technology","pages":"107--114","year":"2009","doi":"10.1145/1690388.1690407","authors":["Aleksander Krzywinski","Haipeng Mi","Weiqin Chen","Masanori Sugimoto"],"dblp":"conf/ACMace/KrzywinskiMCS09","venue":"Advances in Computer Entertainment Technology","abstract":"Combining table top and tangible user interfaces is a relatively new research area. Many problems remain to be solved before users can benefit from tangible interactive table top systems. This paper presents our effort in this direction. RoboTable is an interactive table top system that enables users to naturally and intuitively manipulate robots. The goal of this research is to develop a software framework for human-robot interaction which combines table top, tangible objects, artificial intelligence and physics simulations and demonstrate the framework with game applications.","paperId":"d1fc035042fc032aa57892181f1538e251136d30","paperTitle":"RoboTable: a tabletop framework for tangible interaction with robots in a mixed reality","paperUrl":"https://www.semanticscholar.org/paper/d1fc035042fc032aa57892181f1538e251136d30","images":["https://d3i71xaburhd42.cloudfront.net/d1fc035042fc032aa57892181f1538e251136d30/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/d1fc035042fc032aa57892181f1538e251136d30/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/d1fc035042fc032aa57892181f1538e251136d30/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/d1fc035042fc032aa57892181f1538e251136d30/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/d1fc035042fc032aa57892181f1538e251136d30/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/d1fc035042fc032aa57892181f1538e251136d30/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/d1fc035042fc032aa57892181f1538e251136d30/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/d1fc035042fc032aa57892181f1538e251136d30/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/d1fc035042fc032aa57892181f1538e251136d30/6-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/d1fc035042fc032aa57892181f1538e251136d30/6-Figure10-1.png"],"pdf":"https://doi.org/10.1145/1690388.1690407"},{"type":"inproceedings","key":"leithinger2010relief","title":"Relief: A Scalable Actuated Shape Display","author":"Leithinger, Daniel and Ishii, Hiroshi","booktitle":"Proceedings of the fourth international conference on Tangible, embedded, and embodied interaction","pages":"221--222","year":"2010","doi":"10.1145/1709886.1709928","authors":["Daniel Leithinger","Hiroshi Ishii"],"dblp":"conf/tei/LeithingerI10","venue":"TEI \'10","abstract":"Relief is an actuated tabletop display, which is able to render and animate three-dimensional shapes with a malleable surface. It allows users to experience and form digital models like geographical terrain in an intuitive manner. The tabletop surface is actuated by an array of 120 motorized pins, which are controlled with a low-cost, scalable platform built upon open-source hardware and software tools. Each pin can be addressed individually and senses user input like pulling and pushing.","paperId":"880dec32b10d579637d259c69c448070318fed30","paperTitle":"Relief: a scalable actuated shape display","paperUrl":"https://www.semanticscholar.org/paper/880dec32b10d579637d259c69c448070318fed30","images":["https://d3i71xaburhd42.cloudfront.net/880dec32b10d579637d259c69c448070318fed30/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/880dec32b10d579637d259c69c448070318fed30/2-Figure2-1.png"],"pdf":"https://dspace.mit.edu/bitstream/1721.1/60866/1/Ishii_Relief%20A.pdf"},{"type":"inproceedings","key":"de2019intuitive","title":"An Intuitive Augmented Reality Interface for Task Scheduling, Monitoring, and Work Performance Improvement in Human-Robot Collaboration","author":"De Franco, Alessandro and Lamon, Edoardo and Balatti, Pietro and De Momi, Elena and Ajoudani, Arash","booktitle":"2019 IEEE International Work Conference on Bioinspired Intelligence (IWOBI)","pages":"75--80","year":"2019","organization":"IEEE","doi":"10.1109/IWOBI47054.2019.9114472","authors":["Alessandro De Franco","Edoardo Lamon","Pietro Balatti","Elena De Momi","Arash Ajoudani"],"dblp":"conf/iwobi/FrancoLBMA19","venue":"2019 IEEE International Work Conference on Bioinspired Intelligence (IWOBI)","abstract":"One of the open challenges in Human-Robot collaborative tasks is to provide a simple way for humans to understand robotic systems\' plans and actions and to interact with them in the most natural way. Towards the direction of a natural interaction between human and robots, we propose a simple and intuitive interface exploiting the emergent Augmented Reality (AR) technology. This interface aims not only to enhance human awareness of the robot status and planned actions during collaborative tasks, but also to improve the quality of the work. The presented interface enables the human operators to interact with the system sending commands (gesture or vocal) and receive instant feedbacks (holograms and sound) through an AR device, enabling an intuitive way to coordinate human and robot actions. The interface validation is performed through two industrial scenario experiments, i.e., a collaborative assembly of a metallic structure and a collaborative polishing. The experimental results showed that the presented AR interface could be exploited to improve human operators\' performance while performing an industrial task. At the same time the usage is also perceived as beneficial by ten subjects that tested the interface in the experiments.","paperId":"bde3d965a598b8ee3dab4df081bb32e3fc23a879","paperTitle":"An Intuitive Augmented Reality Interface for Task Scheduling, Monitoring, and Work Performance Improvement in Human-Robot Collaboration","paperUrl":"https://www.semanticscholar.org/paper/bde3d965a598b8ee3dab4df081bb32e3fc23a879","images":["https://d3i71xaburhd42.cloudfront.net/bde3d965a598b8ee3dab4df081bb32e3fc23a879/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/bde3d965a598b8ee3dab4df081bb32e3fc23a879/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/bde3d965a598b8ee3dab4df081bb32e3fc23a879/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/bde3d965a598b8ee3dab4df081bb32e3fc23a879/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/bde3d965a598b8ee3dab4df081bb32e3fc23a879/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/bde3d965a598b8ee3dab4df081bb32e3fc23a879/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/bde3d965a598b8ee3dab4df081bb32e3fc23a879/6-Figure7-1.png"],"pdf":"https://doi.org/10.1109/IWOBI47054.2019.9114472"},{"type":"inproceedings","key":"cardenas2021reducing","title":"Reducing Cognitive Workload in Telepresence Lunar-Martian Environments through Audiovisual Feedback in Augmented Reality","author":"Cardenas, Irvin Steve and Powlison, Kaleb and Kim, Jong-Hoon","booktitle":"Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction","pages":"463--466","year":"2021","doi":"10.1145/3434074.3447214","authors":["Irvin Steve Cardenas","Kaleb Powlison","Jong-Hoon Kim"],"dblp":"conf/hri/CardenasPK21","venue":"HRI","abstract":"Navigating through an unknown, and perhaps resource-constrained environment, such as Lunar terrain or through a disaster region can be detrimental - e.g. both physically and cognitively exhausting. Difficulties during navigation can cost time, operational resources, or even a life. To this end, the interaction with a robotic exploration system in lunar or Martian environments could be key to successful exploration extravehicular activities (X-EVA). Through the use of augmented reality (AR) we can afford an astronaut with various capabilities. In particular, we focus on two: (1) The ability to obtain and display information on their current position, on important locations, and on essential objects in an augmented space. (2) The ability to control an exploratory robot system, or smart robotic tools using AR interfaces. We present our ongoing development of such AR robot control interfaces and the feedback system being implemented. This work extends the augmented reality robot navigation and audio spatial feedback components presented at the 2020 National Aeronautics and Space Administration (NASA) SUITS Challenge.","paperId":"a02f2e88408bdace6596fbe6edb7bc3d99d49aeb","paperTitle":"Reducing Cognitive Workload in Telepresence Lunar - Martian Environments Through Audiovisual Feedback in Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/a02f2e88408bdace6596fbe6edb7bc3d99d49aeb","images":["https://d3i71xaburhd42.cloudfront.net/a02f2e88408bdace6596fbe6edb7bc3d99d49aeb/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/a02f2e88408bdace6596fbe6edb7bc3d99d49aeb/4-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/a02f2e88408bdace6596fbe6edb7bc3d99d49aeb/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/a02f2e88408bdace6596fbe6edb7bc3d99d49aeb/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/a02f2e88408bdace6596fbe6edb7bc3d99d49aeb/3-Figure4-1.png"],"pdf":"https://doi.org/10.1145/3434074.3447214"},{"type":"article","key":"leitner2010physical","title":"Physical Interfaces for Tabletop Games","author":"Leitner, Jakob and Haller, Michael and Yun, Kyungdahm and Woo, Woontack and Sugimoto, Maki and Inami, Masahiko and Cheok, Adrian David and Been-Lirn, HD","journal":"Computers in Entertainment (CIE)","volume":"7","number":"4","pages":"1--21","year":"2010","publisher":"ACM New York, NY, USA","doi":"10.1145/1658866.1658880","authors":["Jakob Leitner","Michael Haller","Kyungdahm Yun","Woontack Woo","Maki Sugimoto","Masahiko Inami","Adrian David Cheok","HD Been-Lirn"],"dblp":"journals/cie/LeitnerHYWSICD09","venue":"CIE","abstract":"IncreTable is a mixed-reality tabletop game inspired by The Incredible Machine. Users can combine real and virtual game pieces in order to solve puzzles in the game. Game actions include placing virtual domino blocks with digital pens, controlling a virtual car by modifying the virtual terrain through a depth camera interface, and controlling real robots to topple over real and virtual dominoes.","paperId":"bc4dae22030fb1f9c5d629b02e5c19e9425de270","paperTitle":"Physical interfaces for tabletop games","paperUrl":"https://www.semanticscholar.org/paper/bc4dae22030fb1f9c5d629b02e5c19e9425de270","images":["https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/10-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/10-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/12-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/12-Figure14-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/13-Figure15-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/14-Figure16-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/15-Figure17-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/15-Figure18-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/16-Figure19-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/17-Figure20-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/17-Figure21-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/18-Figure22-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/18-Figure23-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/20-Figure24-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/7-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/7-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/8-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/9-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/bc4dae22030fb1f9c5d629b02e5c19e9425de270/11-TableI-1.png"],"pdf":"http://ecite.utas.edu.au/90298/1/HBL_2009_a61-leitner.pdf"},{"type":"incollection","key":"hiraki2016phygital","title":"Phygital Field: An Integrated Field with a Swarm of Physical Robots and Digital Images","author":"Hiraki, Takefumi and Fukushima, Shogo and Naemura, Takeshi","booktitle":"SIGGRAPH ASIA 2016 Emerging Technologies","pages":"1--2","year":"2016","doi":"10.1145/2988240.2988242","authors":["Takefumi Hiraki","Shogo Fukushima","Takeshi Naemura"],"dblp":"conf/siggraph/HirakiFN16","venue":"SIGGRAPH ASIA Emerging Technologies","abstract":"Collaboration between computer graphics and multiple robots has attracted increasing attention in several areas. To preserve the seamless connection between them, the system needs to be able to accurately determine the position and state of the robots and be able to control them easily and instantly. However, realizing a responsive control system for a large number of mobile robots without complex settings and at the same time avoiding the system load problem is not trivial. Our system, called \\"Phygital Field\\", can project two types of information: visible images for humans and data patterns for mobile robots in the same location by utilizing pixel-level visual light communication (PVLC) technology. Phygital Field offers two technical innovations: an initialization-free and marker-free localization and control method, and a system noted for its simplicity and scalability. Phygital Field enables the robots to always recognize their own positions and states immediately, the measurement devices are not required because localization and control of the robots are realized through projection. These features are very important to improve the reconfigurability of the system. The idea of controlling robots by using information embedded in projected images allows users to easily design an integrated environment for the physical robots and digital images to preserve the seamless connection between them.","paperId":"a6c2f487b38326da0e06f31ae6c0af89574389ca","paperTitle":"Phygital field: an integrated field with a swarm of physical robots and digital images","paperUrl":"https://www.semanticscholar.org/paper/a6c2f487b38326da0e06f31ae6c0af89574389ca","images":["https://d3i71xaburhd42.cloudfront.net/a6c2f487b38326da0e06f31ae6c0af89574389ca/2-Figure2-1.png"],"pdf":"http://www.takefumihiraki.com/wp-content/uploads/2016/01/hiraki_SIGGRAPHAsia2016_compressed.pdf"},{"type":"inproceedings","key":"malayjerdi2017mobile","title":"Mobile Robot Navigation Based on Fuzzy Cognitive Map Optimized with Grey Wolf Optimization Algorithm Used in Augmented Reality","author":"Malayjerdi, Ehsan and Yaghoobi, Mahdi and Kardan, Mohammad","booktitle":"2017 5th RSI International Conference on Robotics and Mechatronics (ICRoM)","pages":"211--218","year":"2017","organization":"IEEE","doi":"10.1109/ICROM.2017.8466169","authors":["Ehsan Malayjerdi","Mahdi Yaghoobi","Mohammad Kardan"],"venue":"2017 5th RSI International Conference on Robotics and Mechatronics (ICRoM)","abstract":"this work presents a control technique for Mobile Robot Navigation using augmented reality (AR). This navigation technique is based on optimized Fuzzy Cognitive Map (FCM) and AR\'s Glyphs. AR\'s symbols are provided by the overhead camera. The patterns are made up of glyphs and a clear path. Six practical test are manipulated to examine the strength of optimizing FCM by a mobile robot for navigation with AR\'s symbols. The experiment examined the effectiveness of a Grey Wolf Optimization Algorithm (GWOA) in optimizing the FCM. Two practical experiments confirm that AR\'s Glyphs are an effective symbol for a robot to navigation in an unknown environment. A practical experiment reveals that a robot can use AR to manage its intended movement. Augmented reality, such as the Glyphs and a simplified map, are an effective tool for mobile robots to use in navigation in unknown environments. A prototype system is made to navigate the mobile robot by using AR and FCM.","paperId":"8b8f70c7fb6500b5828c2bb6e8e45d9421cfc1fb","paperTitle":"Mobile robot navigation based on Fuzzy Cognitive Map optimized with Grey Wolf Optimization Algorithm used in Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/8b8f70c7fb6500b5828c2bb6e8e45d9421cfc1fb","images":["https://d3i71xaburhd42.cloudfront.net/8b8f70c7fb6500b5828c2bb6e8e45d9421cfc1fb/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/8b8f70c7fb6500b5828c2bb6e8e45d9421cfc1fb/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/8b8f70c7fb6500b5828c2bb6e8e45d9421cfc1fb/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/8b8f70c7fb6500b5828c2bb6e8e45d9421cfc1fb/7-Table3-1.png","https://d3i71xaburhd42.cloudfront.net/8b8f70c7fb6500b5828c2bb6e8e45d9421cfc1fb/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/8b8f70c7fb6500b5828c2bb6e8e45d9421cfc1fb/7-Table4-1.png","https://d3i71xaburhd42.cloudfront.net/8b8f70c7fb6500b5828c2bb6e8e45d9421cfc1fb/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/8b8f70c7fb6500b5828c2bb6e8e45d9421cfc1fb/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/8b8f70c7fb6500b5828c2bb6e8e45d9421cfc1fb/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/8b8f70c7fb6500b5828c2bb6e8e45d9421cfc1fb/4-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/8b8f70c7fb6500b5828c2bb6e8e45d9421cfc1fb/5-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/8b8f70c7fb6500b5828c2bb6e8e45d9421cfc1fb/5-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/8b8f70c7fb6500b5828c2bb6e8e45d9421cfc1fb/6-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/8b8f70c7fb6500b5828c2bb6e8e45d9421cfc1fb/6-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/8b8f70c7fb6500b5828c2bb6e8e45d9421cfc1fb/7-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/8b8f70c7fb6500b5828c2bb6e8e45d9421cfc1fb/7-Figure14-1.png","https://d3i71xaburhd42.cloudfront.net/8b8f70c7fb6500b5828c2bb6e8e45d9421cfc1fb/7-Figure15-1.png","https://d3i71xaburhd42.cloudfront.net/8b8f70c7fb6500b5828c2bb6e8e45d9421cfc1fb/7-Figure16-1.png"],"pdf":"https://doi.org/10.1109/ICROM.2017.8466169"},{"type":"inproceedings","key":"dragone2006mixing","title":"Mixing Robotic Realities","author":"Dragone, Mauro and Holz, Thomas and O\'Hare, Gregory MP","booktitle":"Proceedings of the 11th international conference on Intelligent user interfaces","pages":"261--263","year":"2006","doi":"10.1145/1111449.1111504","authors":["Mauro Dragone","Thomas Holz","Gregory MP O\'Hare"],"dblp":"conf/iui/DragoneHO06","venue":"IUI \'06","abstract":"This paper contests that Mixed Reality (MR) offers a potential solution in achieving transferability between Human Computer Interaction (HCI) and Human Robot Interaction (HRI). Virtual characters (possibly of a robotic genre) can offer highly expressive interfaces that are as convincing as a human, are comparably cheap and can be easily adapted and personalized. We introduce the notion of a mixed reality agent, i.e. an agent consisting of a physical robotic body and a virtual avatar displayed upon it. We realized an augmented reality interface with a Head-Mounted Display (HMD) in order to interact with such systems and conducted a pilot study to demonstrate the usefulness of mixed reality agents in human-robot collaborative tasks.","paperId":"036c5f87c5b71cf039c20da008c7b2684fda249c","paperTitle":"Mixing robotic realities","paperUrl":"https://www.semanticscholar.org/paper/036c5f87c5b71cf039c20da008c7b2684fda249c","images":["https://d3i71xaburhd42.cloudfront.net/036c5f87c5b71cf039c20da008c7b2684fda249c/2-Figure1-1.png"],"pdf":"https://doi.org/10.1145/1111449.1111504"},{"type":"inproceedings","key":"chacko2020augmented","title":"An Augmented Reality Spatial Referencing System for Mobile Robots","author":"Chacko, Sonia Mary and Granado, Armando and RajKumar, Ashwin and Kapila, Vikram","booktitle":"2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","pages":"4446--4452","year":"2020","organization":"IEEE","doi":"10.1109/IROS45743.2020.9340742","authors":["Sonia Mary Chacko","Armando Granado","Ashwin RajKumar","Vikram Kapila"],"dblp":"conf/iros/ChackoGKK20","venue":"2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","abstract":"The deployment of a mobile service robot in domestic settings is a challenging task due to the dynamic and unstructured nature of such environments. Successful operation of the robot requires continuous human supervision to update its spatial knowledge about the dynamic environment. Thus, it is essential to develop a human-robot interaction (HRI) strategy that is suitable for novice end users to effortlessly provide task-specific spatial information to the robot. Although several approaches have been developed for this purpose, most of them are not feasible or convenient for use in domestic environments. In response, we have developed an augmented reality (AR) spatial referencing system (SRS), which allows a non-expert user to tag any specific locations on a physical surface to allocate tasks to be performed by the robot at those locations. Specifically, in the AR-SRS, the user provides a spatial reference by creating an AR virtual object with a semantic label. The real-world location of the user-created virtual object is estimated and stored as spatial data along with the user-specified semantic label. We present three different approaches to establish the correspondence between the user-created virtual object locations and the real-world coordinates on an a priori static map of the service area available to the robot. The performance of each approach is evaluated and reported. We also present use-case scenarios to demonstrate potential applications of the AR-SRS for mobile service robots.","paperId":"20877cfb085a9543ef5b3d650a8d4ec5fb9eefec","paperTitle":"An Augmented Reality Spatial Referencing System for Mobile Robots","paperUrl":"https://www.semanticscholar.org/paper/20877cfb085a9543ef5b3d650a8d4ec5fb9eefec","images":["https://d3i71xaburhd42.cloudfront.net/20877cfb085a9543ef5b3d650a8d4ec5fb9eefec/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/20877cfb085a9543ef5b3d650a8d4ec5fb9eefec/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/20877cfb085a9543ef5b3d650a8d4ec5fb9eefec/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/20877cfb085a9543ef5b3d650a8d4ec5fb9eefec/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/20877cfb085a9543ef5b3d650a8d4ec5fb9eefec/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/20877cfb085a9543ef5b3d650a8d4ec5fb9eefec/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/20877cfb085a9543ef5b3d650a8d4ec5fb9eefec/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/20877cfb085a9543ef5b3d650a8d4ec5fb9eefec/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/20877cfb085a9543ef5b3d650a8d4ec5fb9eefec/7-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/20877cfb085a9543ef5b3d650a8d4ec5fb9eefec/6-TableI-1.png"],"pdf":"https://doi.org/10.1109/IROS45743.2020.9340742"},{"type":"inproceedings","key":"chacko2019augmented","title":"An Augmented Reality Interface for Human-Robot Interaction in Unconstrained Environments","author":"Chacko, Sonia Mary and Kapila, Vikram","booktitle":"2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","pages":"3222--3228","year":"2019","organization":"IEEE","doi":"10.1109/IROS40897.2019.8967973","authors":["Sonia Mary Chacko","Vikram Kapila"],"dblp":"conf/iros/ChackoK19","venue":"2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","abstract":"As robots start to become ubiquitous in the personal workspace, it is necessary to have simple and intuitive interfaces to interact with them. In this paper, we propose an augmented reality (AR) interface for human-robot interaction (HRI) in a shared working environment. By fusing marker-based and markerless AR technologies, a mobile AR interface is created that enables a smartphone to detect planar surfaces and localize a manipulator robot in its working environment while obviating the need for a controlled or constrained environment. The AR interface and robot manipulator are integrated to render a system that enables users to perform pick-and-place task effortlessly. Specifically, a smartphone-based AR application is developed that allows a user to select any location within the robot\'s workspace by merely touching on the smartphone screen. Virtual objects, rendered at user-selected locations, are used to determine the pick and place locations of objects in the real world. The virtual object\'s start and end points, originally specified in the smartphone camera coordinate frame, are transformed into the robot coordinate frame for the robot manipulator to autonomously perform the assigned task. A user study is conducted with participants to evaluate the system performance and user experience. The results show that the proposed AR interface is user-friendly and intuitive to operate the robot, and it allows users to communicate their intentions through the virtual object easily.","paperId":"c6b379d95955675c3ed82a91351408f86a583d7e","paperTitle":"An Augmented Reality Interface for Human-Robot Interaction in Unconstrained Environments","paperUrl":"https://www.semanticscholar.org/paper/c6b379d95955675c3ed82a91351408f86a583d7e","images":["https://d3i71xaburhd42.cloudfront.net/c6b379d95955675c3ed82a91351408f86a583d7e/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/c6b379d95955675c3ed82a91351408f86a583d7e/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/c6b379d95955675c3ed82a91351408f86a583d7e/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/c6b379d95955675c3ed82a91351408f86a583d7e/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/c6b379d95955675c3ed82a91351408f86a583d7e/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/c6b379d95955675c3ed82a91351408f86a583d7e/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/c6b379d95955675c3ed82a91351408f86a583d7e/6-Figure7-1.png"],"pdf":"https://doi.org/10.1109/IROS40897.2019.8967973"},{"type":"inproceedings","key":"leutert2013spatial","title":"A Spatial Augmented Reality System for Intuitive Display of Robotic Data","author":"Leutert, Florian and Herrmann, Christian and Schilling, Klaus","booktitle":"2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","pages":"179--180","year":"2013","organization":"IEEE","doi":"10.1109/HRI.2013.6483560","authors":["Florian Leutert","Christian Herrmann","Klaus Schilling"],"dblp":"conf/hri/LeutertHS13","venue":"2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","abstract":"In the emerging field of close human-robot-collaboration the human worker needs to be able to quickly and easily understand data of the robotic system. To achieve this even for untrained personnel, we propose the use of a Spatial Augmented Reality system to project the necessary information directly into the users\' workspace. The projection system consists of a fixed as well as a mobile projector mounted directly on the manipulator, allowing for visualizing data anywhere in the surroundings of the robot. By enabling the user to simply see the necessary complex information he can better understand the data and behavior of the robotic assistant and has the opportunity to analyze and potentially optimize the working process. Together with an input device, arbitrary interfaces can be realized with the projection system.","paperId":"d0bea6721f47e573e91fb90c4c546906a417999e","paperTitle":"A Spatial Augmented Reality system for intuitive display of robotic data","paperUrl":"https://www.semanticscholar.org/paper/d0bea6721f47e573e91fb90c4c546906a417999e","images":["https://d3i71xaburhd42.cloudfront.net/d0bea6721f47e573e91fb90c4c546906a417999e/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/d0bea6721f47e573e91fb90c4c546906a417999e/2-Figure3-1.png"],"pdf":"https://doi.org/10.1109/HRI.2013.6483560"},{"type":"inproceedings","key":"alrashidi2016pedagogical","title":"A Pedagogical Virtual Machine for Assembling Mobile Robot Using Augmented Reality","author":"Alrashidi, Malek and Alzahrani, Ahmed and Gardner, Michael and Callaghan, Vic","booktitle":"Proceedings of the 7th Augmented Human International Conference 2016","pages":"1--2","year":"2016","doi":"10.1145/2875194.2875229","authors":["Malek Alrashidi","Ahmed Alzahrani","Michael Gardner","Vic Callaghan"],"dblp":"conf/aughuman/AlrashidiAGC16","venue":"AH","abstract":"In this paper, we propose a pedagogical virtual machine (PVM) model that aims to link physical-object activities with learning objects to reveal the related educational value of the physical objects in question. To examine the proposed method, we present an experiment based on assembling a modularised mobile-robot task called \\"Buzz-Boards.\\" A between-group design method was chosen for both the experimental and control groups in this study. Participants in the experimental group used an augmented reality application to help them assemble the robot, while the control group took a paper-based approach. 10 students from University of Essex were randomly assigned to each group, for a total of 20 students\' participants. The evaluation factors for this study are time completion, a post-test, cognitive overload, and the learning effectiveness of each method. In overlay, assemblers who used the augmented reality application outperformed the assemblers who use the paper-based approach.","paperId":"e143097feded843e2d1fca0e238be7e4cc835d36","paperTitle":"A Pedagogical Virtual Machine for Assembling Mobile Robot using Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/e143097feded843e2d1fca0e238be7e4cc835d36","images":["https://d3i71xaburhd42.cloudfront.net/e143097feded843e2d1fca0e238be7e4cc835d36/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/e143097feded843e2d1fca0e238be7e4cc835d36/2-Figure2-1.png"],"pdf":"https://doi.org/10.1145/2875194.2875229"},{"type":"inproceedings","key":"stein2019mixed","title":"Mixed Reality Robotics for Stem Education","author":"Stein, Gordon and L\'edeczi, \'Akos","booktitle":"2019 IEEE Blocks and Beyond Workshop (B&B)","pages":"49--53","year":"2019","organization":"IEEE","doi":"10.1109/bb48857.2019.8941229","authors":["Gordon Stein","\'Akos L\'edeczi"],"venue":"2019 IEEE Blocks and Beyond Workshop (B&B)","abstract":"This work presents an overview of a new system to provide motion tracking and mixed reality integration of robots for use in K12 STEM education. The robots, commanded by students through a block-based programming environment, are placed in a virtual environment visible as an overlay on the physical space. The system\u2019s control loop allows the robots to interact with virtual objects as if they are physically present within easily reconfigurable environments. This virtual space allows for lower cost robots to be used by incorporating virtual sensors and actuators, and creates the potential for a wider range of scenarios for students to work with.","paperId":"882e1efcd8b1742c0eff77566c80fa78bc0b7599","paperTitle":"Mixed Reality Robotics for STEM Education","paperUrl":"https://www.semanticscholar.org/paper/882e1efcd8b1742c0eff77566c80fa78bc0b7599","images":["https://d3i71xaburhd42.cloudfront.net/882e1efcd8b1742c0eff77566c80fa78bc0b7599/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/882e1efcd8b1742c0eff77566c80fa78bc0b7599/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/882e1efcd8b1742c0eff77566c80fa78bc0b7599/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/882e1efcd8b1742c0eff77566c80fa78bc0b7599/4-Figure4-1.png"],"pdf":"https://doi.org/10.1109/bb48857.2019.8941229"},{"type":"article","key":"wu2020mixed","title":"Mixed Reality Enhanced User Interactive Path Planning for Omnidirectional Mobile Robot","author":"Wu, Mulun and Dai, Shi-Lu and Yang, Chenguang","journal":"Applied Sciences","volume":"10","number":"3","pages":"1135","year":"2020","publisher":"Multidisciplinary Digital Publishing Institute","doi":"10.3390/app10031135","authors":["Mulun Wu","Shi-Lu Dai","Chenguang Yang"],"venue":"","abstract":"This paper proposes a novel control system for the path planning of an omnidirectional mobile robot based on mixed reality. Most research on mobile robots is carried out in a completely real environment or a completely virtual environment. However, a real environment containing virtual objects has important actual applications. The proposed system can control the movement of the mobile robot in the real environment, as well as the interaction between the mobile robot\u2019s motion and virtual objects which can be added to a real environment. First, an interactive interface is presented in the mixed reality device HoloLens. The interface can display the map, path, control command, and other information related to the mobile robot, and it can add virtual objects to the real map to realize a real-time interaction between the mobile robot and the virtual objects. Then, the original path planning algorithm, vector field histogram* (VFH*), is modified in the aspects of the threshold, candidate direction selection, and cost function, to make it more suitable for the scene with virtual objects, reduce the number of calculations required, and improve the security. Experimental results demonstrated that this proposed method can generate the motion path of the mobile robot according to the specific requirements of the operator, and achieve a good obstacle avoidance performance.","paperId":"04bb8a402735cab7b453ec9a11168252bfe85b87","paperTitle":"Mixed Reality Enhanced User Interactive Path Planning for Omnidirectional Mobile Robot","paperUrl":"https://www.semanticscholar.org/paper/04bb8a402735cab7b453ec9a11168252bfe85b87","images":["https://d3i71xaburhd42.cloudfront.net/04bb8a402735cab7b453ec9a11168252bfe85b87/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/04bb8a402735cab7b453ec9a11168252bfe85b87/11-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/04bb8a402735cab7b453ec9a11168252bfe85b87/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/04bb8a402735cab7b453ec9a11168252bfe85b87/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/04bb8a402735cab7b453ec9a11168252bfe85b87/7-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/04bb8a402735cab7b453ec9a11168252bfe85b87/8-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/04bb8a402735cab7b453ec9a11168252bfe85b87/10-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/04bb8a402735cab7b453ec9a11168252bfe85b87/10-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/04bb8a402735cab7b453ec9a11168252bfe85b87/11-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/04bb8a402735cab7b453ec9a11168252bfe85b87/11-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/04bb8a402735cab7b453ec9a11168252bfe85b87/12-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/04bb8a402735cab7b453ec9a11168252bfe85b87/13-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/04bb8a402735cab7b453ec9a11168252bfe85b87/13-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/04bb8a402735cab7b453ec9a11168252bfe85b87/14-Figure13-1.png"],"pdf":null},{"type":"inproceedings","key":"seiger2017mixed","title":"Mixed Reality Cyber-Physical Systems Control and Workflow Composition","author":"Seiger, Ronny and Korzetz, Mandy and Gohlke, Maria and Assmann, Uwe","booktitle":"Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia","pages":"495--500","year":"2017","doi":"10.1145/3152832.3157808","authors":["Ronny Seiger","Mandy Korzetz","Maria Gohlke","Uwe Assmann"],"dblp":"conf/mum/SeigerKGA17","venue":"MUM","abstract":"With the emergence of Cyber-physical Systems (CPS) more and more smart devices become parts of our daily lives-letting us sense and control our physical surroundings via software. However, with the number of smart devices raises the complexity of understanding and managing all devices and processes within one\'s vicinity. In this demo we present the HoloFlows mixed reality app, which allows users to directly explore, monitor and control physical devices within their surroundings. Simple workflows can be created between sensors and actuators to automate repetitive tasks in an end-user friendly way. We use gestural embodied interactions combined with real world metaphors to provide an intuitive and easy to learn interactive application for controlling complex CPS.","paperId":"c7940dda6e31ebb1ef6081157db8992ede8fdf80","paperTitle":"Mixed reality cyber-physical systems control and workflow composition","paperUrl":"https://www.semanticscholar.org/paper/c7940dda6e31ebb1ef6081157db8992ede8fdf80","images":[],"pdf":"https://doi.org/10.1145/3152832.3157808"},{"type":"inproceedings","key":"lee2011note","title":"A Note on Hybrid Control of Robotic Spatial Augmented Reality","author":"Lee, Joo-Haeng and Kim, Junho and Kim, Hyun","booktitle":"2011 8th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)","pages":"621--626","year":"2011","organization":"IEEE","doi":"10.1109/URAI.2011.6145895","authors":["Joo-Haeng Lee","Junho Kim","Hyun Kim"],"dblp":"conf/urai/LeeKK11","venue":"2011 8th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)","abstract":"A robotic spatial augmented reality (RSAR) system combines robotics with spatial augmented reality (SAR) where cameras are used to recognize real objects, and projectors augment information and user interface directly on the surface of the real objects, rather than relying on mobile or wearable display devices. Hence, the control of a RSAR system requires handling of different types of control schemes at once such as classical inverse kinematics of simply linked bodies, inverse projections to find appropriate internal/external parameters of a projector, and geometric manipulation of a projection source image to increase the flexibility in control. In this paper, we outline a hybrid approach to control relevant control components in a coordinated manner, specially focused on application in a prototype RSAR system developed in ETRI.","paperId":"185fdcf4f44609e61db0a598e6e1f8a3a77df10b","paperTitle":"A note on hybrid control of robotic spatial augmented reality","paperUrl":"https://www.semanticscholar.org/paper/185fdcf4f44609e61db0a598e6e1f8a3a77df10b","images":["https://d3i71xaburhd42.cloudfront.net/185fdcf4f44609e61db0a598e6e1f8a3a77df10b/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/185fdcf4f44609e61db0a598e6e1f8a3a77df10b/2-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/185fdcf4f44609e61db0a598e6e1f8a3a77df10b/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/185fdcf4f44609e61db0a598e6e1f8a3a77df10b/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/185fdcf4f44609e61db0a598e6e1f8a3a77df10b/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/185fdcf4f44609e61db0a598e6e1f8a3a77df10b/3-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/185fdcf4f44609e61db0a598e6e1f8a3a77df10b/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/185fdcf4f44609e61db0a598e6e1f8a3a77df10b/5-Figure8-1.png"],"pdf":"https://doi.org/10.1109/URAI.2011.6145895"},{"type":"incollection","key":"yamaoka2016mirageprinter","title":"MiragePrinter: Interactive Fabrication on a 3D Printer with a mid-Air Display","author":"Yamaoka, Junichi and Kakehi, Yasuaki","booktitle":"ACM SIGGRAPH 2016 Studio","pages":"1--2","year":"2016","doi":"10.1145/2929484.2929489","authors":["Junichi Yamaoka","Yasuaki Kakehi"],"dblp":"conf/siggraph/YamaokaK16","venue":"SIGGRAPH Studio","abstract":"The rapid proliferation of digital fabrication machines has resulted in creating an environment that enables more people to make various creations. From a viewpoint of Human Computer Interaction, it is often pointed out that interfaces bridging between works in the digital environment and the physical environment are necessary to support design for personal fabrication [WILLIS 2011] [WEICHEL 2014].","paperId":"552337b512621be7bc19f168667f4e4342f1719a","paperTitle":"MiragePrinter: interactive fabrication on a 3D printer with a mid-air display","paperUrl":"https://www.semanticscholar.org/paper/552337b512621be7bc19f168667f4e4342f1719a","images":["https://d3i71xaburhd42.cloudfront.net/552337b512621be7bc19f168667f4e4342f1719a/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/552337b512621be7bc19f168667f4e4342f1719a/1-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/552337b512621be7bc19f168667f4e4342f1719a/1-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/552337b512621be7bc19f168667f4e4342f1719a/2-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/552337b512621be7bc19f168667f4e4342f1719a/2-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/552337b512621be7bc19f168667f4e4342f1719a/2-Figure9-1.png"],"pdf":"https://doi.org/10.1145/2929484.2929489"},{"type":"article","key":"sugimoto2011mobile","title":"A Mobile Mixed-Reality Environment for Children\'s Storytelling Using a Handheld Projector and a Robot","author":"Sugimoto, Masanori","journal":"IEEE Transactions on Learning Technologies","volume":"4","number":"3","pages":"249--260","year":"2011","publisher":"IEEE","doi":"10.1109/TLT.2011.13","authors":["Masanori Sugimoto"],"dblp":"journals/tlt/Sugimoto11","venue":"IEEE Transactions on Learning Technologies","abstract":"This paper describes a system called GENTORO that uses a robot and a handheld projector for supporting children\'s storytelling activities. GENTORO differs from many existing systems in that children can make a robot play their own story in a physical space augmented by mixed-reality technologies. Pilot studies have been conducted to clarify the design requirements of GENTORO from both technological and practical viewpoints. A user study indicates that GENTORO\'s ability to enable manipulation of a robot using a handheld projector in a physical space can enhance children\'s embodied participation in, and their level of engagement with, their storytelling activities, and can support children in designing and expressing creative and original stories.","paperId":"bf8dd23a4a36412f4a99bd2f351c5f870dafee15","paperTitle":"A Mobile Mixed-Reality Environment for Children\'s Storytelling Using a Handheld Projector and a Robot","paperUrl":"https://www.semanticscholar.org/paper/bf8dd23a4a36412f4a99bd2f351c5f870dafee15","images":["https://d3i71xaburhd42.cloudfront.net/bf8dd23a4a36412f4a99bd2f351c5f870dafee15/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/bf8dd23a4a36412f4a99bd2f351c5f870dafee15/8-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/bf8dd23a4a36412f4a99bd2f351c5f870dafee15/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/bf8dd23a4a36412f4a99bd2f351c5f870dafee15/10-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/bf8dd23a4a36412f4a99bd2f351c5f870dafee15/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/bf8dd23a4a36412f4a99bd2f351c5f870dafee15/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/bf8dd23a4a36412f4a99bd2f351c5f870dafee15/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/bf8dd23a4a36412f4a99bd2f351c5f870dafee15/7-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/bf8dd23a4a36412f4a99bd2f351c5f870dafee15/7-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/bf8dd23a4a36412f4a99bd2f351c5f870dafee15/7-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/bf8dd23a4a36412f4a99bd2f351c5f870dafee15/7-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/bf8dd23a4a36412f4a99bd2f351c5f870dafee15/10-Figure10-1.png"],"pdf":"https://doi.org/10.1109/tlt.2011.13"},{"type":"inproceedings","key":"park2018meet","title":"Meet AR-Bot: Meeting Anywhere, Anytime with Movable Spatial AR Robot","author":"Park, Yoon Jung and Yang, Yoonsik and Ro, Hyocheol and Byun, JungHyun and Chae, Seougho and Han, Tack Don","booktitle":"Proceedings of the 26th ACM international conference on Multimedia","pages":"1242--1243","year":"2018","doi":"10.1145/3240508.3241390","authors":["Yoon Jung Park","Yoonsik Yang","Hyocheol Ro","JungHyun Byun","Seougho Chae","Tack Don Han"],"dblp":"conf/mm/ParkYRBCH18","venue":"ACM Multimedia","abstract":"Many kinds of preparations are needed when meeting. For example, projector, laptop, cables and ETC. As such, this video have constructed Meet AR-bot, which helps users to keep meeting going smoothly, based on the projection of Augmented Reality(AR). Our system can easily provide meeting room environment through the movable setting via wheel-based stand. Users do not need to carry a personal laptop and connect them to the projector. Robot reconstructs the 3D geometry information through pan-tilt system and compute projection areas to project information in the space. Users can also control through mobile devices. We offer presentation, table interaction, file sharing and virtual object registration by mobile device.","paperId":"6fc760c73fb5966dd51085f16d390a71d751835b","paperTitle":"Meet AR-bot: Meeting Anywhere, Anytime with Movable Spatial AR Robot","paperUrl":"https://www.semanticscholar.org/paper/6fc760c73fb5966dd51085f16d390a71d751835b","images":[],"pdf":"https://doi.org/10.1145/3240508.3241390"},{"type":"inproceedings","key":"nunez2006human","title":"A Human-Robot Interaction System for Navigation Supervision Based on Augmented Reality","author":"Nunez, R and Bandera, JR and Perez-Lorenzo, JM and Sandoval, Francisco","booktitle":"MELECON 2006-2006 IEEE Mediterranean Electrotechnical Conference","pages":"441--444","year":"2006","organization":"IEEE","doi":"10.1109/MELCON.2006.1653133","authors":["R Nunez","JR Bandera","JM Perez-Lorenzo","Francisco Sandoval"],"venue":"MELECON 2006 - 2006 IEEE Mediterranean Electrotechnical Conference","abstract":"This paper proposes an innovative human-robot interaction mechanism that permits users to interact intuitively with an autonomous mobile robot which localisation problem is solved using a new and fast feature extraction method. To allow that human-robot interaction, we use an augmented reality display. This mechanism makes it possible to overlay planning, world model and sensory data provided by the robot over the same field of view. The determination of the camera pose in the AR system is solved using this novelty feature-based localisation method. Thus, the human user can intuitively help to build a topological map in an unknown environment by setting and manipulating map nodes and visualize and correct the robot\'s path planning","paperId":"89f27b6c3a07eb5bdcc2e753e929b966af6993c5","paperTitle":"A human-robot interaction system for navigation supervision based on augmented reality","paperUrl":"https://www.semanticscholar.org/paper/89f27b6c3a07eb5bdcc2e753e929b966af6993c5","images":["https://d3i71xaburhd42.cloudfront.net/89f27b6c3a07eb5bdcc2e753e929b966af6993c5/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/89f27b6c3a07eb5bdcc2e753e929b966af6993c5/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/89f27b6c3a07eb5bdcc2e753e929b966af6993c5/4-Figure3-1.png"],"pdf":"https://robolab.unex.es/wp-content/papercite-data/pdf/human-robot-interaction.pdf"},{"type":"inproceedings","key":"jeong2018mechanism","title":"Mechanism Perfboard: An Augmented Reality Environment for Linkage Mechanism Design and Fabrication","author":"Jeong, Yunwoo and Kim, Han-Jong and Nam, Tek-Jin","booktitle":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","pages":"1--11","year":"2018","doi":"10.1145/3173574.3173985","authors":["Yunwoo Jeong","Han-Jong Kim","Tek-Jin Nam"],"dblp":"conf/chi/JeongKN18","venue":"CHI","abstract":"Prototyping devices with kinetic mechanisms, such as automata and robots, has become common in physical computing projects. However, mechanism design in the early-concept exploration phase is challenging, due to the dynamic and unpredictable characteristics of mechanisms. We present Mechanism Perfboard, an augmented reality environment that supports linkage mechanism design and fabrication. It supports the concretization of ideas by generating the initial desired linkage mechanism from a real world movement. The projection of simulated movement within the environment enables iterative tests and modifications in real scale. Augmented information and accompanying tangible parts help users to fabricate mechanisms. Through a user study with 10 participants, we found that Mechanism Perfboard helped the participant to achieve their desired movement. The augmented environment enabled intuitive modification and fabrication with an understanding of mechanical movement. Based on the tool development and the user study, we discuss implications for mechanism prototyping with augmented reality and computational support.","paperId":"276e75a4d76614c257f709bc9dcf2d462f4d7f6b","paperTitle":"Mechanism Perfboard: An Augmented Reality Environment for Linkage Mechanism Design and Fabrication","paperUrl":"https://www.semanticscholar.org/paper/276e75a4d76614c257f709bc9dcf2d462f4d7f6b","images":["https://d3i71xaburhd42.cloudfront.net/276e75a4d76614c257f709bc9dcf2d462f4d7f6b/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/276e75a4d76614c257f709bc9dcf2d462f4d7f6b/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/276e75a4d76614c257f709bc9dcf2d462f4d7f6b/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/276e75a4d76614c257f709bc9dcf2d462f4d7f6b/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/276e75a4d76614c257f709bc9dcf2d462f4d7f6b/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/276e75a4d76614c257f709bc9dcf2d462f4d7f6b/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/276e75a4d76614c257f709bc9dcf2d462f4d7f6b/6-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/276e75a4d76614c257f709bc9dcf2d462f4d7f6b/6-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/276e75a4d76614c257f709bc9dcf2d462f4d7f6b/8-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/276e75a4d76614c257f709bc9dcf2d462f4d7f6b/9-Figure11-1.png"],"pdf":"https://doi.org/10.1145/3173574.3173985"},{"type":"inproceedings","key":"puljiz2020hololens","title":"What the HoloLens Maps Is Your Workspace: Fast Mapping and Set-up of Robot Cells via Head Mounted Displays and Augmented Reality","author":"Puljiz, David and Krebs, Franziska and Bosing, Fabian and Hein, Bjorn","booktitle":"2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","pages":"11445--11451","year":"2020","organization":"IEEE","doi":"10.1109/IROS45743.2020.9340879","authors":["David Puljiz","Franziska Krebs","Fabian Bosing","Bjorn Hein"],"dblp":"journals/corr/abs-2005-12651","venue":"2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","abstract":"Classical methods of modelling and mapping robot work cells are time consuming, expensive and involve expert knowledge. We present a novel approach to mapping and cell setup using modern Head Mounted Displays (HMDs) that possess self-localisation and mapping capabilities. We leveraged these capabilities to create a point cloud of the environment and build an OctoMap - a voxel occupancy grid representation of the robot\u2019s workspace for path planning. Through the use of Augmented Reality (AR) interactions, the user can edit the created Octomap and add safety zones. We perform comprehensive tests of the HoloLens\u2019 depth sensing capabilities and the quality of the resultant point cloud. A high-end laser scanner is used to provide the ground truth for the evaluation of the point cloud quality. The amount of false-positive and false-negative voxels in the OctoMap are also tested.","paperId":"7ea614d39a15e5d9d5c88f59db9e47477f43cfeb","paperTitle":"What the HoloLens Maps Is Your Workspace: Fast Mapping and Set-up of Robot Cells via Head Mounted Displays and Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/7ea614d39a15e5d9d5c88f59db9e47477f43cfeb","images":["https://d3i71xaburhd42.cloudfront.net/7ea614d39a15e5d9d5c88f59db9e47477f43cfeb/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/7ea614d39a15e5d9d5c88f59db9e47477f43cfeb/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/7ea614d39a15e5d9d5c88f59db9e47477f43cfeb/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/7ea614d39a15e5d9d5c88f59db9e47477f43cfeb/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/7ea614d39a15e5d9d5c88f59db9e47477f43cfeb/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/7ea614d39a15e5d9d5c88f59db9e47477f43cfeb/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/7ea614d39a15e5d9d5c88f59db9e47477f43cfeb/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/7ea614d39a15e5d9d5c88f59db9e47477f43cfeb/7-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/7ea614d39a15e5d9d5c88f59db9e47477f43cfeb/4-TableI-1.png","https://d3i71xaburhd42.cloudfront.net/7ea614d39a15e5d9d5c88f59db9e47477f43cfeb/5-TableIV-1.png","https://d3i71xaburhd42.cloudfront.net/7ea614d39a15e5d9d5c88f59db9e47477f43cfeb/7-TableV-1.png"],"pdf":"http://arxiv.org/pdf/2005.12651"},{"type":"inproceedings","key":"nawab2007joystick","title":"Joystick Mapped Augmented Reality Cues for End-Effector Controlled Tele-Operated Robots","author":"Nawab, Aditya and Chintamani, Keshav and Ellis, Darin and Auner, Gregory and Pandya, Abhilash","booktitle":"2007 IEEE Virtual Reality Conference","pages":"263--266","year":"2007","organization":"IEEE","doi":"10.1109/VR.2007.352496","authors":["Aditya Nawab","Keshav Chintamani","Darin Ellis","Gregory Auner","Abhilash Pandya"],"dblp":"conf/vr/NawabCEAP07","venue":"2007 IEEE Virtual Reality Conference","abstract":"End-effector control of robots using just remote camera views is difficult due to lack of perceived correspondence between the joysticks and the end-effector coordinate frame. This paper reports the positive effects of augmented reality visual cues on operator performance during end-effector controlled tele-operation using only camera views. Our solution is to overlay a color-coded coordinate system on the end-effector of the robot using AR techniques. This mapped and color-coded coordinate system is then directly mapped to similarly color-coded joysticks used for control of both position and orientation. The AR view along with mapped markings on the joystick give the user a clear notion of the effect of their joystick movements on the end-effector of the robot. All camera views display this registered dynamic overlay information on-demand. An insertion task was used to compare performance with and without the coordinate mapping using fifteen subjects. Preliminary results indicate a significant reduction in distance and reversal errors","paperId":"9fdd73058862dd3c1f0f0a5eb6ed6c075c1e22dd","paperTitle":"Joystick mapped Augmented Reality Cues for End-Effector controlled Tele-operated Robots","paperUrl":"https://www.semanticscholar.org/paper/9fdd73058862dd3c1f0f0a5eb6ed6c075c1e22dd","images":["https://d3i71xaburhd42.cloudfront.net/9fdd73058862dd3c1f0f0a5eb6ed6c075c1e22dd/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/9fdd73058862dd3c1f0f0a5eb6ed6c075c1e22dd/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/9fdd73058862dd3c1f0f0a5eb6ed6c075c1e22dd/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/9fdd73058862dd3c1f0f0a5eb6ed6c075c1e22dd/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/9fdd73058862dd3c1f0f0a5eb6ed6c075c1e22dd/4-Figure5-1.png"],"pdf":"http://ece.eng.wayne.edu/~apandya/Publications/AR_CUES_V3.pdf"},{"type":"inproceedings","key":"cao2019v","title":"V. Ra: An in-Situ Visual Authoring System for Robot-IoT Task Planning with Augmented Reality","author":"Cao, Yuanzhi and Xu, Zhuangying and Li, Fan and Zhong, Wentao and Huo, Ke and Ramani, Karthik","booktitle":"Proceedings of the 2019 on Designing Interactive Systems Conference","pages":"1059--1070","year":"2019","doi":"10.1145/3290607.3312797","authors":["Yuanzhi Cao","Zhuangying Xu","Fan Li","Wentao Zhong","Ke Huo","Karthik Ramani"],"dblp":"conf/chi/CaoXLZHR19","venue":"CHI Extended Abstracts","abstract":"We present V.Ra, a visual and spatial programming system for robot-IoT task authoring. In V.Ra, programmable mobile robots serve as binding agents to link the stationary IoTs and perform collaborative tasks. We establish an ecosystem that coherently connects the three key elements of robot task planning (human-robot-IoT) with one single AR-SLAM device. Users can perform task authoring in an analogous manner with the Augmented Reality (AR) interface. Then placing the device onto the mobile robot directly transfers the task plan in a what-you-do-is-what-robot-does (WYDWRD) manner. The mobile device mediates the interactions between the user, robot and IoT oriented tasks, and guides the path planning execution with the SLAM capability.","paperId":"3522e6a53d21e32da9ae8376755842fd3010e82f","paperTitle":"V.Ra: An In-Situ Visual Authoring System for Robot-IoT Task Planning with Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/3522e6a53d21e32da9ae8376755842fd3010e82f","images":["https://d3i71xaburhd42.cloudfront.net/3522e6a53d21e32da9ae8376755842fd3010e82f/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/3522e6a53d21e32da9ae8376755842fd3010e82f/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/3522e6a53d21e32da9ae8376755842fd3010e82f/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/3522e6a53d21e32da9ae8376755842fd3010e82f/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/3522e6a53d21e32da9ae8376755842fd3010e82f/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/3522e6a53d21e32da9ae8376755842fd3010e82f/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/3522e6a53d21e32da9ae8376755842fd3010e82f/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/3522e6a53d21e32da9ae8376755842fd3010e82f/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/3522e6a53d21e32da9ae8376755842fd3010e82f/6-Figure9-1.png"],"pdf":"https://dl.acm.org/doi/pdf/10.1145/3290607.3312797"},{"type":"inproceedings","key":"ruiz2015immersive","title":"Immersive Displays for Building Spatial Knowledge in Multi-UAV Operations","author":"Ruiz, JJ and Viguria, A and Martinez-de-Dios, JR and Ollero, A","booktitle":"2015 International Conference on Unmanned Aircraft Systems (ICUAS)","pages":"1043--1048","year":"2015","organization":"IEEE","doi":"10.1109/ICUAS.2015.7152395","authors":["JJ Ruiz","A Viguria","JR Martinez-de-Dios","A Ollero"],"venue":"2015 International Conference on Unmanned Aircraft Systems (ICUAS)","abstract":"This paper presents an experiment to evaluate the effects of immersive displays in a multi-UAV context in which the operator may need to understand not only the 3D spatial relationships between the UAVs but also where the UAV is with respect to mountains or other obstacles. To this end, a 3D simulator was developed and displays were selected based on their Field-of-Regard (FoR). Participants supervised a multi-UAV operation and the acquired spatial knowledge was evaluated using a Situational Awareness Global Assessment Technique (SAGAT). Moreover, a NASA-TLX study was done to measure the workload of each display. Finally, results showed that a gradual increase of immersion improved the spatial understanding of the UAV operator.","paperId":"8acd7aa06d102669ff018ba15a3e85176d4fbd35","paperTitle":"Immersive displays for building spatial knowledge in multi-UAV operations","paperUrl":"https://www.semanticscholar.org/paper/8acd7aa06d102669ff018ba15a3e85176d4fbd35","images":["https://d3i71xaburhd42.cloudfront.net/8acd7aa06d102669ff018ba15a3e85176d4fbd35/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/8acd7aa06d102669ff018ba15a3e85176d4fbd35/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/8acd7aa06d102669ff018ba15a3e85176d4fbd35/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/8acd7aa06d102669ff018ba15a3e85176d4fbd35/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/8acd7aa06d102669ff018ba15a3e85176d4fbd35/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/8acd7aa06d102669ff018ba15a3e85176d4fbd35/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/8acd7aa06d102669ff018ba15a3e85176d4fbd35/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/8acd7aa06d102669ff018ba15a3e85176d4fbd35/4-TableI-1.png"],"pdf":"https://doi.org/10.1109/ICUAS.2015.7152395"},{"type":"inproceedings","key":"groechel2019using","title":"Using Socially Expressive Mixed Reality Arms for Enhancing Low-Expressivity Robots","author":"Groechel, Thomas and Shi, Zhonghao and Pakkar, Roxanna and Matari\'c, Maja J","booktitle":"2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","pages":"1--8","year":"2019","organization":"IEEE","doi":"10.1109/RO-MAN46459.2019.8956458","authors":["Thomas Groechel","Zhonghao Shi","Roxanna Pakkar","Maja J Matari\'c"],"dblp":"conf/ro-man/GroechelSPM19","venue":"2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","abstract":"Expressivity\u2013the use of multiple modalities to convey internal state and intent of a robot\u2013is critical for interaction. Yet, due to cost, safety, and other constraints, many robots lack high degrees of physical expressivity. This paper explores using mixed reality to enhance a robot with limited expressivity by adding virtual arms that extend the robot\u2019s expressiveness. The arms, capable of a range of non-physically-constrained gestures, were evaluated in a between-subject study (n =34) where participants engaged in a mixed reality mathematics task with a socially assistive robot. The study results indicate that the virtual arms added a higher degree of perceived emotion, helpfulness, and physical presence to the robot. Users who reported a higher perceived physical presence also found the robot to have a higher degree of social presence, ease of use, usefulness, and had a positive attitude toward using the robot with mixed reality. The results also demonstrate the users\u2019 ability to distinguish the virtual gestures\u2019 valence and intent.","paperId":"f36a14def46a7092376d7b6eec919e1e69b8b73e","paperTitle":"Using Socially Expressive Mixed Reality Arms for Enhancing Low-Expressivity Robots","paperUrl":"https://www.semanticscholar.org/paper/f36a14def46a7092376d7b6eec919e1e69b8b73e","images":["https://d3i71xaburhd42.cloudfront.net/f36a14def46a7092376d7b6eec919e1e69b8b73e/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/f36a14def46a7092376d7b6eec919e1e69b8b73e/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/f36a14def46a7092376d7b6eec919e1e69b8b73e/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/f36a14def46a7092376d7b6eec919e1e69b8b73e/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/f36a14def46a7092376d7b6eec919e1e69b8b73e/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/f36a14def46a7092376d7b6eec919e1e69b8b73e/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/f36a14def46a7092376d7b6eec919e1e69b8b73e/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/f36a14def46a7092376d7b6eec919e1e69b8b73e/6-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/f36a14def46a7092376d7b6eec919e1e69b8b73e/7-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/f36a14def46a7092376d7b6eec919e1e69b8b73e/6-TableI-1.png","https://d3i71xaburhd42.cloudfront.net/f36a14def46a7092376d7b6eec919e1e69b8b73e/7-TableII-1.png","https://d3i71xaburhd42.cloudfront.net/f36a14def46a7092376d7b6eec919e1e69b8b73e/7-TableIII-1.png"],"pdf":"http://arxiv.org/pdf/1911.09713"},{"type":"inproceedings","key":"lakshantha2014human","title":"Human Robot Interaction and Control: Translating Diagrams into an Intuitive Augmented Reality Approach","author":"Lakshantha, Eranda and Egerton, Simon","booktitle":"2014 International Conference on Intelligent Environments","pages":"111--116","year":"2014","organization":"IEEE","doi":"10.1109/IE.2014.24","authors":["Eranda Lakshantha","Simon Egerton"],"dblp":"conf/intenv/LakshanthaE14","venue":"2014 International Conference on Intelligent Environments","abstract":"Robots will play a vital role in our future personal spaces and we need to provide ways for robots and humans to interact with each other in a way that is natural, intuitive, descriptive and unambiguous. In this paper we introduce a framework that enables a human and robot to naturally interact and communicate with each other using the idea of diagrams. The diagrams are formed by a connected set of object markers placed in the environment. These markers can either be physically present in the environment or virtually present using marker-less technology. This paper presents a marker-less method for globally persistent markers. Diagrams are formed by connecting the objects together enabling the user to easily interact and control the robot in complex ways. We report on a proof-of-concept implementation of our framework and show how the framework can be used to program a robot to carry out navigation and action tasks within an environment.","paperId":"43d00f149dc7e3a5751f6560d798b85bc2b8d470","paperTitle":"Human Robot Interaction and Control: Translating Diagrams into an Intuitive Augmented Reality Approach","paperUrl":"https://www.semanticscholar.org/paper/43d00f149dc7e3a5751f6560d798b85bc2b8d470","images":["https://d3i71xaburhd42.cloudfront.net/43d00f149dc7e3a5751f6560d798b85bc2b8d470/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/43d00f149dc7e3a5751f6560d798b85bc2b8d470/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/43d00f149dc7e3a5751f6560d798b85bc2b8d470/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/43d00f149dc7e3a5751f6560d798b85bc2b8d470/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/43d00f149dc7e3a5751f6560d798b85bc2b8d470/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/43d00f149dc7e3a5751f6560d798b85bc2b8d470/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/43d00f149dc7e3a5751f6560d798b85bc2b8d470/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/43d00f149dc7e3a5751f6560d798b85bc2b8d470/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/43d00f149dc7e3a5751f6560d798b85bc2b8d470/5-Figure9-1.png"],"pdf":"http://www.researchgate.net/profile/Eranda_Lakshantha/publication/266563189_Human_Robot_Interaction_and_Control_Translating_Diagrams_into_an_Intuitive_Augmented_Reality_Approach/links/5434e3b70cf2bf1f1f27d9c1.pdf"},{"type":"inproceedings","key":"st2015robot","title":"How Robot Verbal Feedback Can Improve Team Performance in Human-Robot Task Collaborations","author":"St. Clair, Aaron and Mataric, Maja","booktitle":"Proceedings of the tenth annual acm/ieee international conference on human-robot interaction","pages":"213--220","year":"2015","doi":"10.1145/2696454.2696491","authors":["Aaron St. Clair","Maja Mataric"],"dblp":"conf/hri/ClairM15","venue":"2015 10th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","abstract":"We detail an approach to planning effective verbal feedback during pairwise human-robot task collaboration. The approach is motivated by social science literature as well as existing work in robotics and is applicable to a variety of task scenarios. It consists of a dynamic, synthetic task implemented in an augmented reality environment. The result is combined robot task control and speech production, allowing the robot to actively participate and communicate with its teammate. A user study was conducted to experimentally validate the efficacy of the approach on a task in which a single user collaborates with an autonomous robot. The results demonstrate that the approach is capable of improving both objective measures of team performance and the user\u2019s subjective evaluation of both the task and the robot as a teammate. Categories and Subject Descriptors H.1.2 [Models and Principles]: User/Machine Systems\u2014human factors, software psychology; H.5.2 [Information Interfaces and Presentation]: User Interfaces\u2014evaluation/methodology, natural language; I.2.9 [Artificial Intelligence]: Robotics\u2014operator interfaces","paperId":"3de908e877ce148cbf9b2603b438f53f5d68ca9f","paperTitle":"How Robot Verbal Feedback Can Improve Team Performance in Human-Robot Task Collaborations","paperUrl":"https://www.semanticscholar.org/paper/3de908e877ce148cbf9b2603b438f53f5d68ca9f","images":["https://d3i71xaburhd42.cloudfront.net/3de908e877ce148cbf9b2603b438f53f5d68ca9f/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/3de908e877ce148cbf9b2603b438f53f5d68ca9f/7-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/3de908e877ce148cbf9b2603b438f53f5d68ca9f/5-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/3de908e877ce148cbf9b2603b438f53f5d68ca9f/6-Figure3-1.png"],"pdf":"http://robotics.usc.edu/publications/media/uploads/pubs/hrifp2561-st-clair.pdf"},{"type":"inproceedings","key":"materna2017using","title":"Using Persona, Scenario, and Use Case to Develop a Human-Robot Augmented Reality Collaborative Workspace","author":"Materna, Zden\u0117k and Kapinus, Michal and Beran, V\'it\u0117zslav and Smr\u0116, Pavel and Giuliani, Manuel and Mirnig, Nicole and Stadler, Susanne and Stollnberger, Gerald and Tscheligi, Manfred","booktitle":"Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction","pages":"201--202","year":"2017","doi":"10.1145/3029798.3038366","authors":["Zden\u0117k Materna","Michal Kapinus","V\'it\u0117zslav Beran","Pavel Smr\u0116","Manuel Giuliani","Nicole Mirnig","Susanne Stadler","Gerald Stollnberger","Manfred Tscheligi"],"dblp":"conf/hri/MaternaKBSGMSST17","venue":"HRI","abstract":"Up to date, methods from Human-Computer Interaction (HCI) have not been widely adopted in the development of Human-Robot Interaction systems (HRI). In this paper, we describe a system prototype and a use case. The prototype is an augmented reality-based collaborative workspace. The envisioned solution is focused on small and medium enterprises (SMEs) where it should enable ordinary-skilled workers to program a robot on a high level of abstraction and perform collaborative tasks effectively and safely. The use case consists of a scenario and a persona, two methods from the field of HCI. We outline how we are going to use these methods in the near future to refine the task of the collaborating robot and human and the interface elements of the collaborative workspace.","paperId":"ba182b72b9c82dfecc11c7af9d2a1c3c4484b782","paperTitle":"Using Persona, Scenario, and Use Case to Develop a Human-Robot Augmented Reality Collaborative Workspace","paperUrl":"https://www.semanticscholar.org/paper/ba182b72b9c82dfecc11c7af9d2a1c3c4484b782","images":["https://d3i71xaburhd42.cloudfront.net/ba182b72b9c82dfecc11c7af9d2a1c3c4484b782/1-Figure1-1.png"],"pdf":"https://doi.org/10.1145/3029798.3038366"},{"type":"inproceedings","key":"tran2021get","title":"Get This!? Mixed Reality Improves Robot Communication Regardless of Mental Workload","author":"Tran, Nhan and Grant, Trevor and Phung, Thao and Hirshfield, Leanne and Wickens, Christopher and Williams, Tom","booktitle":"Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction","pages":"412--416","year":"2021","doi":"10.1145/3434074.3447203","authors":["Nhan Tran","Trevor Grant","Thao Phung","Leanne Hirshfield","Christopher Wickens","Tom Williams"],"dblp":"conf/hri/TranGPHW021","venue":"HRI","abstract":"We present the first experiment analyzing the effectiveness of robot-generated mixed reality gestures using real robotic and mixed reality hardware. Our findings demonstrate how these gestures increase user effectiveness by decreasing user response time during visual search tasks, and show that robots can safely pair longer, more natural referring expressions with mixed reality gestures without worrying about cognitively overloading their interlocutors.","paperId":"69e3719e8fc119e3681db5b006278fb6b0e1d96c","paperTitle":"Get This!? Mixed Reality Improves Robot Communication Regardless of Mental Workload","paperUrl":"https://www.semanticscholar.org/paper/69e3719e8fc119e3681db5b006278fb6b0e1d96c","images":["https://d3i71xaburhd42.cloudfront.net/69e3719e8fc119e3681db5b006278fb6b0e1d96c/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/69e3719e8fc119e3681db5b006278fb6b0e1d96c/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/69e3719e8fc119e3681db5b006278fb6b0e1d96c/3-Figure3-1.png"],"pdf":"https://doi.org/10.1145/3434074.3447203"},{"type":"inproceedings","key":"dragone2007using","title":"Using Mixed Reality Agents as Social Interfaces for Robots","author":"Dragone, Mauro and Holz, Thomas and O\'Hare, Gregory MP","booktitle":"RO-MAN 2007-The 16th IEEE International Symposium on Robot and Human Interactive Communication","pages":"1161--1166","year":"2007","organization":"IEEE","doi":"10.1109/ROMAN.2007.4415255","authors":["Mauro Dragone","Thomas Holz","Gregory MP O\'Hare"],"dblp":"conf/ro-man/DragoneHO07","venue":"RO-MAN 2007 - The 16th IEEE International Symposium on Robot and Human Interactive Communication","abstract":"Endowing robots with a social interface is often costly and difficult. Virtual characters on the other hand are comparatively cheap and well equipped but suffer from other difficulties, most notably their inability to interact with the physical world. This paper details our wearable solution to combining physical robots and virtual characters into a mixed reality agent (MiRA) through mixed reality visualisation. It describes a pilot study demonstrating our system, and showing how such a technique can offer a viable alternative cost effective approach to enabling a rich social interface for human-robot interaction.","paperId":"1664957fd72a21ad8d485ac5c8c99212c6de997e","paperTitle":"Using Mixed Reality Agents as Social Interfaces for Robots","paperUrl":"https://www.semanticscholar.org/paper/1664957fd72a21ad8d485ac5c8c99212c6de997e","images":["https://d3i71xaburhd42.cloudfront.net/1664957fd72a21ad8d485ac5c8c99212c6de997e/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/1664957fd72a21ad8d485ac5c8c99212c6de997e/5-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/1664957fd72a21ad8d485ac5c8c99212c6de997e/6-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/1664957fd72a21ad8d485ac5c8c99212c6de997e/6-Table2-1.png"],"pdf":"https://researchrepository.ucd.ie/bitstream/10197/4526/1/P291-Dragone%2cHolz%2cO%27Hare-07.pdf"},{"type":"inproceedings","key":"prattico2019user","title":"User Perception of Robot\'s Role in Floor Projection-Based Mixed-Reality Robotic Games","author":"Prattic`o, F Gabriele and Cannav`o, Alberto and Chen, Junchao and Lamberti, Fabrizio","booktitle":"2019 IEEE 23rd International Symposium on Consumer Technologies (ISCT)","pages":"76--81","year":"2019","organization":"IEEE","doi":"10.1109/ISCE.2019.8901037","authors":["F Gabriele Prattic`o","Alberto Cannav`o","Junchao Chen","Fabrizio Lamberti"],"dblp":"conf/isce/PratticoCCL19","venue":"2019 IEEE 23rd International Symposium on Consumer Technologies (ISCT)","abstract":"Within the emerging research area represented by robotic gaming and, specifically, in application domains in which the recent literature suggests to combine commercial off-the-shelf (COTS) robots and projected mixed reality (MR) technology in order to develop engaging games, one of the crucial issues to consider in the design process is how to make the player perceive the robot as having a key role, i.e., to valorize its presence from the user experience point of view. By moving from this consideration, this paper reports efforts that are being carried out with the aim to investigate the impact of diverse game design choices in the above perspective, while at the same time extracting preliminary insights that can be exploited to orient further research in the field of MR-based robotic gaming and related scenarios.","paperId":"22a9b5e0e9af89d7e9913e5302d481a0aa4b41e7","paperTitle":"User Perception of Robot\'s Role in Floor Projection-based Mixed-Reality Robotic Games","paperUrl":"https://www.semanticscholar.org/paper/22a9b5e0e9af89d7e9913e5302d481a0aa4b41e7","images":["https://d3i71xaburhd42.cloudfront.net/22a9b5e0e9af89d7e9913e5302d481a0aa4b41e7/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/22a9b5e0e9af89d7e9913e5302d481a0aa4b41e7/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/22a9b5e0e9af89d7e9913e5302d481a0aa4b41e7/6-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/22a9b5e0e9af89d7e9913e5302d481a0aa4b41e7/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/22a9b5e0e9af89d7e9913e5302d481a0aa4b41e7/4-TableI-1.png"],"pdf":"https://iris.polito.it/retrieve/handle/11583/2734324/316044/prattico_postprint.pdf"},{"type":"inproceedings","key":"bolano2019transparent","title":"Transparent Robot Behavior Using Augmented Reality in Close Human-Robot Interaction","author":"Bolano, Gabriele and Juelg, Christian and Roennau, Arne and Dillmann, Ruediger","booktitle":"2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","pages":"1--7","year":"2019","organization":"IEEE","doi":"10.1109/RO-MAN46459.2019.8956296","authors":["Gabriele Bolano","Christian Juelg","Arne Roennau","Ruediger Dillmann"],"dblp":"conf/ro-man/BolanoJRD19","venue":"2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","abstract":"Most robots consistently repeat their motion with- out changes in a precise and consistent manner. But nowadays there are also robots able to dynamically change their motion and plan according to the people and environment that surround them. Furthermore, they are able to interact with humans and cooperate with them. With no information about the robot targets and intentions, the user feels uncomfortable even with a safe robot. In close human-robot collaboration, it is very important to make the user able to understand the robot intentions in a quick and intuitive way. In this work we have developed a system to use augmented reality to project directly into the workspace useful information. The robot intuitively shows its planned motion and task state. The AR module interacts with a vision system in order to display the changes in the workspace in a dynamic way. The representation of information about possible collisions and changes of plan allows the human to have a more comfortable and efficient interaction with the robot. The system is evaluated in different setups.","paperId":"b5021acb394e923398088b0859195a68c2872c08","paperTitle":"Transparent Robot Behavior Using Augmented Reality in Close Human-Robot Interaction","paperUrl":"https://www.semanticscholar.org/paper/b5021acb394e923398088b0859195a68c2872c08","images":["https://d3i71xaburhd42.cloudfront.net/e79a281214f59b0acda672825dd91f9e8a1e7e15/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/e79a281214f59b0acda672825dd91f9e8a1e7e15/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/e79a281214f59b0acda672825dd91f9e8a1e7e15/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/e79a281214f59b0acda672825dd91f9e8a1e7e15/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/e79a281214f59b0acda672825dd91f9e8a1e7e15/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/e79a281214f59b0acda672825dd91f9e8a1e7e15/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/e79a281214f59b0acda672825dd91f9e8a1e7e15/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/e79a281214f59b0acda672825dd91f9e8a1e7e15/4-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/e79a281214f59b0acda672825dd91f9e8a1e7e15/5-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/e79a281214f59b0acda672825dd91f9e8a1e7e15/5-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/e79a281214f59b0acda672825dd91f9e8a1e7e15/5-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/e79a281214f59b0acda672825dd91f9e8a1e7e15/5-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/e79a281214f59b0acda672825dd91f9e8a1e7e15/6-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/e79a281214f59b0acda672825dd91f9e8a1e7e15/6-Figure14-1.png"],"pdf":"https://doi.org/10.1109/RO-MAN46459.2019.8956296"},{"type":"inproceedings","key":"frank2016towards","title":"Towards Teleoperation-Based Interactive Learning of Robot Kinematics Using a Mobile Augmented Reality Interface on a Tablet","author":"Frank, Jared A and Kapila, Vikram","booktitle":"2016 Indian Control Conference (ICC)","pages":"385--392","year":"2016","organization":"IEEE","doi":"10.1109/INDIANCC.2016.7441163","authors":["Jared A Frank","Vikram Kapila"],"venue":"2016 Indian Control Conference (ICC)","abstract":"The integration of augmented reality (AR) techniques in user interface design has enhanced interactive experiences in teleoperation of robots, hands-on learning in classrooms, laboratory, and special education, and user training in an array of fields, e.g., aerospace, automotive, construction, manufacturing, medical, etc. However, AR-based user interfaces that command machines and tools have not been fully explored for their potential to enhance interactive learning of engineering concepts in the laboratory. This paper outlines the development of a mobile application executing on a tablet device, which renders an immersive AR-based graphical user interface to enable users to monitor, interact with, and control a four-link underactuated planar robot. Computer vision routines are used to extract real-time, vision-based measurements of the robot\'s joint angles and end effector location from the live video captured by the rear-facing camera on the tablet. The obtained measurements are used to render AR content to offer users with additional visual feedback. Touch gesture recognition is implemented to allow users to naturally and intuitively command the robot by tapping and dragging their fingers at desired locations on the tablet screen. Experimental results show the performance and efficacy of the proposed system as it is operated in two different modes: one in which the user has direct control over the angles of the actuated links of the robot and one in which the user has direct control over the end effector location.","paperId":"80d87412d8a17b49ac95a55cb172deef55cdf6d4","paperTitle":"Towards teleoperation-based interactive learning of robot kinematics using a mobile augmented reality interface on a tablet","paperUrl":"https://www.semanticscholar.org/paper/80d87412d8a17b49ac95a55cb172deef55cdf6d4","images":["https://d3i71xaburhd42.cloudfront.net/80d87412d8a17b49ac95a55cb172deef55cdf6d4/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/80d87412d8a17b49ac95a55cb172deef55cdf6d4/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/80d87412d8a17b49ac95a55cb172deef55cdf6d4/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/80d87412d8a17b49ac95a55cb172deef55cdf6d4/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/80d87412d8a17b49ac95a55cb172deef55cdf6d4/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/80d87412d8a17b49ac95a55cb172deef55cdf6d4/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/80d87412d8a17b49ac95a55cb172deef55cdf6d4/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/80d87412d8a17b49ac95a55cb172deef55cdf6d4/6-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/80d87412d8a17b49ac95a55cb172deef55cdf6d4/8-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/80d87412d8a17b49ac95a55cb172deef55cdf6d4/8-Figure10-1.png"],"pdf":"https://doi.org/10.1109/INDIANCC.2016.7441163"},{"type":"inproceedings","key":"kain2017tablet","title":"Tablet-Based Augmented Reality in the Factory: Influence of Knowledge in Computer Programming on Robot Teaching Tasks","author":"Kain, Kevin Sebastian and Stadler, Susanne and Giuliani, Manuel and Mirnig, Nicole and Stollnberger, Gerald and Tscheligi, Manfred","booktitle":"Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction","pages":"151--152","year":"2017","doi":"10.1145/3029798.3038347","authors":["Kevin Sebastian Kain","Susanne Stadler","Manuel Giuliani","Nicole Mirnig","Gerald Stollnberger","Manfred Tscheligi"],"dblp":"conf/hri/KainSGMST17","venue":"HRI","abstract":"We investigate whether a user\'s background in computer programming has an influence on user\'s mental workload, when using a tablet-based augmented reality robot control interface that visualizes task-based information. For this purpose we conducted a user study, in which participants solved multiple robot control tasks. In the study, the users had to control a Sphero robot ball with the help of a tablet-based augmented reality (AR) interface. The AR interface was used to show task instructions to the user. The results of the study show that users with computer programming background had a higher effort and also report a higher frustration when using the AR interface. Additionally, they rate the usability of the AR interface as \\"ok\\", in comparison to naive users without programming background, who gave it a \\"good\\" rating. These results suggest that users with a computer programming background need an AR interface that visualizes more technical information.","paperId":"62ac0e77180a1e21009a895bfce5ac891c725015","paperTitle":"Tablet-Based Augmented Reality in the Factory: Influence of Knowledge in Computer Programming on Robot Teaching Tasks","paperUrl":"https://www.semanticscholar.org/paper/62ac0e77180a1e21009a895bfce5ac891c725015","images":["https://d3i71xaburhd42.cloudfront.net/62ac0e77180a1e21009a895bfce5ac891c725015/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/62ac0e77180a1e21009a895bfce5ac891c725015/2-Figure2-1.png"],"pdf":"https://doi.org/10.1145/3029798.3038347"},{"type":"inproceedings","key":"ahn2013supporting","title":"Supporting Augmented Reality Based Children\'s Play with pro-Cam Robot: Three User Perspectives","author":"Ahn, Jong-gil and Kim, Gerard J and Yeon, Hyemin and Hyun, Eunja and Choi, Kyoung","booktitle":"Proceedings of the 12th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry","pages":"17--24","year":"2013","doi":"10.1145/2534329.2534342","authors":["Jong-gil Ahn","Gerard J Kim","Hyemin Yeon","Eunja Hyun","Kyoung Choi"],"dblp":"conf/vrcai/AhnKYHC13","venue":"VRCAI \'13","abstract":"This paper shares the experiences from the application of AR using the pro-cam robot assistant to managing children\'s play from three user perspectives, namely, the operator (teacher), the actors (children), and the audience (mainly children).\\n First a preliminary expert survey was conducted to assess the expected benefits and any particular provisions needed both educationally and technically. Based on the expert survey, the original implementation was slightly modified, particularly for the robot control interface design for the teachers (e.g. to support easier multi-tasking). Finally, a formative evaluation and analysis was conducted to assess the educational effects to the children (both actors and audiences) and their attitudes when a pro-cam robot was used to run an AR based play, as compared to when a conventional approach was used.\\n The study has found that robot-assisted AR based play showed improved learning effects, compared to the conventional play, in language and creativity and this is attributed to the operational flexibility, novelty, robotic mediation and capturing the attention of the children. The result was also made possible in part by designing an effective interface for the teachers to control the robots and manage the simultaneously occurring tasks.","paperId":"4a0d218173a77955ff474583b0c343c3d791aeaa","paperTitle":"Supporting augmented reality based children\'s play with pro-cam robot: three user perspectives","paperUrl":"https://www.semanticscholar.org/paper/4a0d218173a77955ff474583b0c343c3d791aeaa","images":["https://d3i71xaburhd42.cloudfront.net/4a0d218173a77955ff474583b0c343c3d791aeaa/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/4a0d218173a77955ff474583b0c343c3d791aeaa/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/4a0d218173a77955ff474583b0c343c3d791aeaa/4-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/4a0d218173a77955ff474583b0c343c3d791aeaa/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/4a0d218173a77955ff474583b0c343c3d791aeaa/6-Table3-1.png","https://d3i71xaburhd42.cloudfront.net/4a0d218173a77955ff474583b0c343c3d791aeaa/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/4a0d218173a77955ff474583b0c343c3d791aeaa/6-Table4-1.png","https://d3i71xaburhd42.cloudfront.net/4a0d218173a77955ff474583b0c343c3d791aeaa/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/4a0d218173a77955ff474583b0c343c3d791aeaa/6-Table5-1.png"],"pdf":"https://doi.org/10.1145/2534329.2534342"},{"type":"inproceedings","key":"leutert2012support","title":"Support of Power Plant Telemaintenance with Robots by Augmented Reality Methods","author":"Leutert, Florian and Schilling, Klaus","booktitle":"2012 2nd International Conference on Applied Robotics for the Power Industry (CARPI)","pages":"45--49","year":"2012","organization":"IEEE","doi":"10.1109/CARPI.2012.6473362","authors":["Florian Leutert","Klaus Schilling"],"venue":"2012 2nd International Conference on Applied Robotics for the Power Industry (CARPI)","abstract":"In power plants, in many cases direct human access to crucial sites for inspection or maintenance is impossible. Here, remotely operated robots offer good potential to handle these tasks. This relates to technology challenges for remote operations. In this contribution we propose the use of an Augmented Reality interface as a means to directly and intuitively visualize complex information to overcome some of the challenges and support the teleoperator in his duties. Support functions that could be employed for several tasks in the context of inspection or maintenance are introduced.","paperId":"3ae5ab3ac75f97cd6d0cbf825b0023ed17cfc929","paperTitle":"Support of power plant telemaintenance with robots by Augmented Reality methods","paperUrl":"https://www.semanticscholar.org/paper/3ae5ab3ac75f97cd6d0cbf825b0023ed17cfc929","images":["https://d3i71xaburhd42.cloudfront.net/3ae5ab3ac75f97cd6d0cbf825b0023ed17cfc929/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/3ae5ab3ac75f97cd6d0cbf825b0023ed17cfc929/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/3ae5ab3ac75f97cd6d0cbf825b0023ed17cfc929/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/3ae5ab3ac75f97cd6d0cbf825b0023ed17cfc929/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/3ae5ab3ac75f97cd6d0cbf825b0023ed17cfc929/4-Figure5-1.png"],"pdf":"https://doi.org/10.1109/CARPI.2012.6473362"},{"type":"incollection","key":"nozaki2014flying","title":"Flying Display: A Movable Display Pairing Projector and Screen in the Air","author":"Nozaki, Hiroki","booktitle":"CHI\'14 Extended Abstracts on Human Factors in Computing Systems","pages":"909--914","year":"2014","doi":"10.1145/2559206.2579410","authors":["Hiroki Nozaki"],"dblp":"conf/chi/Nozaki14","venue":"CHI Extended Abstracts","abstract":"We developed Flying Display, a novel movable public display system which can provide information to the people anywhere at anytime. This system consists of two UAVs (Unmanned Aerial Vehicles) with a projector and a screen. Flying Display achieves moving freely and keeping stable in 3-D space. Flying Display moves closer to people and gives information directly to them. To evaluate performance of Flying Display, we performed two experiments for adapting a flying control algorithm. We also showed the stability of Flying Display systems by trajectories of each UAV. This paper highlights the performance of Flying Display and discusses the Flying Display\'s potential for public displays in physical space.","paperId":"a3b3e3d44094781819dc164d425be88e8f0f633d","paperTitle":"Flying display: a movable display pairing projector and screen in the air","paperUrl":"https://www.semanticscholar.org/paper/a3b3e3d44094781819dc164d425be88e8f0f633d","images":["https://d3i71xaburhd42.cloudfront.net/a3b3e3d44094781819dc164d425be88e8f0f633d/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/a3b3e3d44094781819dc164d425be88e8f0f633d/3-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/a3b3e3d44094781819dc164d425be88e8f0f633d/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/a3b3e3d44094781819dc164d425be88e8f0f633d/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/a3b3e3d44094781819dc164d425be88e8f0f633d/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/a3b3e3d44094781819dc164d425be88e8f0f633d/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/a3b3e3d44094781819dc164d425be88e8f0f633d/5-Figure6-1.png"],"pdf":"https://doi.org/10.1145/2559206.2579410"},{"type":"inproceedings","key":"kapinus2019spatially","title":"Spatially Situated End-User Robot Programming in Augmented Reality","author":"Kapinus, Michal and Beran, V\'it\u0117zslav and Materna, Zden\u0117k and Bambu\u0161ek, Daniel","booktitle":"2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","pages":"1--8","year":"2019","organization":"IEEE","doi":"10.1109/RO-MAN46459.2019.8956336","authors":["Michal Kapinus","V\'it\u0117zslav Beran","Zden\u0117k Materna","Daniel Bambu\u0161ek"],"dblp":"conf/ro-man/KapinusBMB19","venue":"2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","abstract":"Nowadays, industrial robots are being programmed using proprietary tools developed by robot manufacturer. A skilled robot programmer is needed to create even as simple task as pick a well-known object and put it somewhere else. Contrary, in every-day life people are using end-user programming to make different electronic devices work in expected manner, without even noticing they are actually programming. We propose augmented reality-enabled end-user programming system allowing regular shop-floor workers to program industrial robotic tasks. The user interface prototype for this system was evaluated in the user study with 7 participants with respect to usability, mental workload and user experience.","paperId":"cc23613fd0f02fde5f9f9a2ea1b0598b087b2dc3","paperTitle":"Spatially Situated End-User Robot Programming in Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/cc23613fd0f02fde5f9f9a2ea1b0598b087b2dc3","images":["https://d3i71xaburhd42.cloudfront.net/a2d65b01eba3b91dfa164a783d0397bc31ea3d05/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/a2d65b01eba3b91dfa164a783d0397bc31ea3d05/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/a2d65b01eba3b91dfa164a783d0397bc31ea3d05/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/a2d65b01eba3b91dfa164a783d0397bc31ea3d05/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/a2d65b01eba3b91dfa164a783d0397bc31ea3d05/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/a2d65b01eba3b91dfa164a783d0397bc31ea3d05/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/a2d65b01eba3b91dfa164a783d0397bc31ea3d05/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/a2d65b01eba3b91dfa164a783d0397bc31ea3d05/7-TableI-1.png","https://d3i71xaburhd42.cloudfront.net/a2d65b01eba3b91dfa164a783d0397bc31ea3d05/7-TableII-1.png"],"pdf":"https://doi.org/10.1109/RO-MAN46459.2019.8956336"},{"type":"inproceedings","key":"huang2019flight","title":"Flight, Camera, Action! Using Natural Language and Mixed Reality to Control a Drone","author":"Huang, Baichuan and Bayazit, Deniz and Ullman, Daniel and Gopalan, Nakul and Tellex, Stefanie","booktitle":"2019 International Conference on Robotics and Automation (ICRA)","pages":"6949--6956","year":"2019","organization":"IEEE","doi":"10.1109/ICRA.2019.8794200","authors":["Baichuan Huang","Deniz Bayazit","Daniel Ullman","Nakul Gopalan","Stefanie Tellex"],"dblp":"conf/icra/HuangBUGT19","venue":"2019 International Conference on Robotics and Automation (ICRA)","abstract":"With increasing autonomy, robots like drones are increasingly accessible to untrained users. Most users control drones using a low-level interface, such as a radio-controlled (RC) controller. For a wider adoption of these technologies by the public, a much higher-level interface, such as natural language or mixed reality (MR), allows the automation of the control of the agent in a goal-oriented setting. We present an interface that uses natural language grounding within an MR environment to solve high-level task and navigational instructions given to an autonomous drone. To the best of our knowledge, this is the first work to perform fully autonomous language grounding in an MR setting for a robot. Given a map, our interface first grounds natural language commands to reward specifications within a Markov Decision Process (MDP) framework. Then, it passes the reward specification to an MDP solver. Finally, the drone performs the desired operations in the real world while planning and localizing itself. Our approach uses MR to provide a set of known virtual landmarks, enabling the drone to understand commands referring to objects without being equipped with object detectors for multiple novel objects or a predefined environment model. We conducted an exploratory user study to assess users\u2019 experience of our MR interface with and without natural language, as compared to a web interface. We found that users were able to command the drone more quickly via both MR interfaces as compared to the web interface, with roughly equal system usability scores across all three interfaces.","paperId":"fc9276768895889a02a6cf6e662461f442625ca1","paperTitle":"Flight, Camera, Action! Using Natural Language and Mixed Reality to Control a Drone","paperUrl":"https://www.semanticscholar.org/paper/fc9276768895889a02a6cf6e662461f442625ca1","images":["https://d3i71xaburhd42.cloudfront.net/fc9276768895889a02a6cf6e662461f442625ca1/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/fc9276768895889a02a6cf6e662461f442625ca1/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/fc9276768895889a02a6cf6e662461f442625ca1/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/fc9276768895889a02a6cf6e662461f442625ca1/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/fc9276768895889a02a6cf6e662461f442625ca1/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/fc9276768895889a02a6cf6e662461f442625ca1/4-TableI-1.png"],"pdf":"http://cs.brown.edu/research/pubs/theses/masters/2019/huang.baichuan.pdf"},{"type":"inproceedings","key":"qian2020flexivision","title":"FlexiVision: Teleporting the Surgeon\'s Eyes via Robotic Flexible Endoscope and Head-Mounted Display","author":"Qian, Long and Song, Chengzhi and Jiang, Yiwei and Luo, Qi and Ma, Xin and Chiu, Philip Waiyan and Li, Zheng and Kazanzides, Peter","booktitle":"2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","pages":"3281--3287","year":"2020","organization":"IEEE","doi":"10.1109/IROS45743.2020.9340716","authors":["Long Qian","Chengzhi Song","Yiwei Jiang","Qi Luo","Xin Ma","Philip Waiyan Chiu","Zheng Li","Peter Kazanzides"],"dblp":"conf/iros/QianSJLMC0K20","venue":"2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","abstract":"A flexible endoscope introduces more dexterity to the image capturing in endoscopic surgery. However, manual control or automatic control based on instrument tracking does not handle the misorientation between the endoscopic video and the surgeon. We propose an automatic flexible endoscope control method that tracks the surgeon\u2019s head with respect to the object in the surgical scene. The robotic flexible endoscope is actuated so that it captures the surgical scene from the same perspective as the surgeon. The surgeon wears a head-mounted display to observe the endoscopic video. The frustum of the flexible endoscope is rendered as an augmented reality overlay to provide surgical guidance. We developed the prototype, FlexiVision, integrating a 6-DOF robotic flexible endoscope based on the da Vinci Research Kit and Microsoft HoloLens. We evaluated the proposed automatic control method via a lesion observation task, and evaluated the AR surgical guidance in a lesion targeting task. The multi-user study results demonstrated that, for both tasks, FlexiVision significantly reduced the completion time (by 59% and 58%), number of errors (by 75% and 95%) and subjective task load level. With FlexiVision, the flexible endoscope could act as the surgeon\u2019s eyes teleported into the abdominal cavity of the patient.","paperId":"12242478956e8126bd098b1e5a5de31984bd818e","paperTitle":"FlexiVision: Teleporting the Surgeon\u2019s Eyes via Robotic Flexible Endoscope and Head-Mounted Display","paperUrl":"https://www.semanticscholar.org/paper/12242478956e8126bd098b1e5a5de31984bd818e","images":["https://d3i71xaburhd42.cloudfront.net/12242478956e8126bd098b1e5a5de31984bd818e/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/12242478956e8126bd098b1e5a5de31984bd818e/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/12242478956e8126bd098b1e5a5de31984bd818e/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/12242478956e8126bd098b1e5a5de31984bd818e/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/12242478956e8126bd098b1e5a5de31984bd818e/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/12242478956e8126bd098b1e5a5de31984bd818e/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/12242478956e8126bd098b1e5a5de31984bd818e/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/12242478956e8126bd098b1e5a5de31984bd818e/6-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/12242478956e8126bd098b1e5a5de31984bd818e/6-Figure9-1.png"],"pdf":"http://ras.papercept.net/images/temp/IROS/files/1565.pdf"},{"type":"inproceedings","key":"gao2019fast","title":"Fast Adaptation with Meta-Reinforcement Learning for Trust Modelling in Human-Robot Interaction","author":"Gao, Yuan and Sibirtseva, Elena and Castellano, Ginevra and Kragic, Danica","booktitle":"2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","pages":"305--312","year":"2019","organization":"IEEE","doi":"10.1109/IROS40897.2019.8967924","authors":["Yuan Gao","Elena Sibirtseva","Ginevra Castellano","Danica Kragic"],"dblp":"conf/iros/GaoSCK19","venue":"2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","abstract":"In socially assistive robotics, an important research area is the development of adaptation techniques and their effect on human-robot interaction. We present a meta-learning based policy gradient method for addressing the problem of adaptation in human-robot interaction and also investigate its role as a mechanism for trust modelling. By building an escape room scenario in mixed reality with a robot, we test our hypothesis that bi-directional trust can be influenced by different adaptation algorithms. We found that our proposed model increased the perceived trustworthiness of the robot and influenced the dynamics of gaining human\u2019s trust. Additionally, participants evaluated that the robot perceived them as more trustworthy during the interactions with the meta-learning based adaptation compared to the previously studied statistical adaptation model.","paperId":"0afd6efb31396eed87e2cc92cc3e0b8c07222e07","paperTitle":"Fast Adaptation with Meta-Reinforcement Learning for Trust Modelling in Human-Robot Interaction","paperUrl":"https://www.semanticscholar.org/paper/0afd6efb31396eed87e2cc92cc3e0b8c07222e07","images":["https://d3i71xaburhd42.cloudfront.net/0afd6efb31396eed87e2cc92cc3e0b8c07222e07/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/0afd6efb31396eed87e2cc92cc3e0b8c07222e07/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/0afd6efb31396eed87e2cc92cc3e0b8c07222e07/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/0afd6efb31396eed87e2cc92cc3e0b8c07222e07/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/0afd6efb31396eed87e2cc92cc3e0b8c07222e07/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/0afd6efb31396eed87e2cc92cc3e0b8c07222e07/8-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/0afd6efb31396eed87e2cc92cc3e0b8c07222e07/4-TableI-1.png","https://d3i71xaburhd42.cloudfront.net/0afd6efb31396eed87e2cc92cc3e0b8c07222e07/5-TableII-1.png"],"pdf":"http://arxiv.org/pdf/1908.04087"},{"type":"inproceedings","key":"renner2018facilitating","title":"Facilitating HRI by Mixed Reality Techniques","author":"Renner, Patrick and Lier, Florian and Friese, Felix and Pfeiffer, Thies and Wachsmuth, Sven","booktitle":"Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction","pages":"215--216","year":"2018","doi":"10.1145/3173386.3177032","authors":["Patrick Renner","Florian Lier","Felix Friese","Thies Pfeiffer","Sven Wachsmuth"],"dblp":"conf/hri/RennerLFPW18","venue":"HRI","abstract":"Mobile robots start to appear in our everyday life, e.g., in shopping malls, airports, nursing homes or warehouses. Often, these robots are operated by non-technical staff with no prior experience/education in robotics. Additionally, as with all new technology, there is certain reservedness when it comes to accepting robots in our personal space. In this work, we propose making use of state-of-the-art Mixed Reality (MR) technology to facilitate acceptance and interaction with mobile robots. By integrating a Microsoft HoloLens into the robot\xbbs operating space, the MR device can be used to a) visualize the robot\xbbs behavior-state and sensor data, b) visually notify the user about planned/future behavior and possible problems/obstacles of the robot, and c) to actively use the device as an additional external sensor source. Moreover, by using the HoloLens, users can operate and interact with the robot without being close to it, as the robot is able to sense with the users\xbb eyes.","paperId":"e2db7ba1e744242b5bac413bd666094d1573c64c","paperTitle":"Facilitating HRI by Mixed Reality Techniques","paperUrl":"https://www.semanticscholar.org/paper/e2db7ba1e744242b5bac413bd666094d1573c64c","images":["https://d3i71xaburhd42.cloudfront.net/e2db7ba1e744242b5bac413bd666094d1573c64c/2-Figure1-1.png"],"pdf":"https://pub.uni-bielefeld.de/download/2916801/2916802/wysiwicd-hri_preprint.pdf"},{"type":"inproceedings","key":"chakraborti2018projection","title":"Projection-Aware Task Planning and Execution for Human-in-the-Loop Operation of Robots in a Mixed-Reality Workspace","author":"Chakraborti, Tathagata and Sreedharan, Sarath and Kulkarni, Anagha and Kambhampati, Subbarao","booktitle":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","pages":"4476--4482","year":"2018","organization":"IEEE","doi":"10.1109/IROS.2018.8593830","authors":["Tathagata Chakraborti","Sarath Sreedharan","Anagha Kulkarni","Subbarao Kambhampati"],"dblp":"conf/iros/ChakrabortiSKK18","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","abstract":"Recent advances in mixed-reality technologies have renewed interest in alternative modes of communication for human-robot interaction. However, most of the work in this direction has been confined to tasks such as teleoperation, simulation or explication of individual actions of a robot. In this paper, we will discuss how the capability to project intentions affect the task planning capabilities of a robot. Specifically, we will start with a discussion on how projection actions can be used to reveal information regarding the future intentions of the robot at the time of task execution. We will then pose a new planning paradigm - projection-aware planning - whereby a robot can trade off its plan cost with its ability to reveal its intentions using its projection actions. We will demonstrate each of these scenarios with the help of a joint human-robot activity using the HoloLens.","paperId":"ba46eb9fb489f0b3a765d5c08ef17a5eeafd7551","paperTitle":"Projection-Aware Task Planning and Execution for Human-in-the-Loop Operation of Robots in a Mixed-Reality Workspace","paperUrl":"https://www.semanticscholar.org/paper/ba46eb9fb489f0b3a765d5c08ef17a5eeafd7551","images":["https://d3i71xaburhd42.cloudfront.net/ba46eb9fb489f0b3a765d5c08ef17a5eeafd7551/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/ba46eb9fb489f0b3a765d5c08ef17a5eeafd7551/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/ba46eb9fb489f0b3a765d5c08ef17a5eeafd7551/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/ba46eb9fb489f0b3a765d5c08ef17a5eeafd7551/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/ba46eb9fb489f0b3a765d5c08ef17a5eeafd7551/6-Figure5-1.png"],"pdf":"https://yochan-lab.github.io/papers/files/papers/projection-aware.pdf"},{"type":"inproceedings","key":"alrashidi2017evaluating","title":"Evaluating the Use of Pedagogical Virtual Machine with Augmented Reality to Support Learning Embedded Computing Activity","author":"Alrashidi, Malek and Gardner, Michael and Callaghan, Vic","booktitle":"Proceedings of the 9th International Conference on Computer and Automation Engineering","pages":"44--50","year":"2017","doi":"10.1145/3057039.3057088","authors":["Malek Alrashidi","Michael Gardner","Vic Callaghan"],"dblp":"conf/iccae/AlrashidiGC17","venue":"ICCAE \'17","abstract":"Embedded computing is often considered as a hidden technology where learners can require more assistance to inspect processes and activities hidden within the technologies, making use of debugging, monitoring, and visual tools. To the student, this kind of technology often has abstract behaviours where the only information/things people can see is the final action, and they do not know how the internal processes work and communicate inside the embedded computing device to achieve the desired result. Augmented reality (AR) can overcome this issue and produce a magic-lens view for revealing hidden embedded computing activities. This can result in learners achieving a better level of knowledge and awareness of the technology, as well as higher learning outcomes. AR on its own will not improve the learning processes without first considering how to manage and represent the hidden information. Therefore, a pedagogical virtual machine (PVM) model was employed, and to evaluate the learning effectiveness of the proposed model. We conducted an experiment based on a problem-solving educational mobile robot task. Twenty students participated in the experimental (AR approach) and control (conventional approach) group. The result showed that the augmented reality approach was more effective in increasing students\' computational thinking and learning outcomes. In addition, the augmented reality approach reduced both time completion and debugging times.","paperId":"7b6cdf0fa26d39891c16839ad8c031598dd13db5","paperTitle":"Evaluating the Use of Pedagogical Virtual Machine with Augmented Reality to Support Learning Embedded Computing Activity","paperUrl":"https://www.semanticscholar.org/paper/7b6cdf0fa26d39891c16839ad8c031598dd13db5","images":["https://d3i71xaburhd42.cloudfront.net/7b6cdf0fa26d39891c16839ad8c031598dd13db5/4-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/7b6cdf0fa26d39891c16839ad8c031598dd13db5/5-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/7b6cdf0fa26d39891c16839ad8c031598dd13db5/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/7b6cdf0fa26d39891c16839ad8c031598dd13db5/6-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/7b6cdf0fa26d39891c16839ad8c031598dd13db5/4-Figure3-1.png"],"pdf":"https://doi.org/10.1145/3057039.3057088"},{"type":"inproceedings","key":"evlampev2019obstacle","title":"Obstacle Avoidance for Robotic Manipulator Using Mixed Reality Glasses","author":"Evlampev, A and Ostanin, M","booktitle":"2019 3rd School on Dynamics of Complex Networks and their Application in Intellectual Robotics (DCNAIR)","pages":"46--48","year":"2019","organization":"IEEE","doi":"10.1109/DCNAIR.2019.8875555","authors":["A Evlampev","M Ostanin"],"venue":"2019 3rd School on Dynamics of Complex Networks and their Application in Intellectual Robotics (DCNAIR)","abstract":"The problem of finding the shortest path for manipulators with obstacle avoidance has many solutions. The paper presents the usage of well-known methods for solving that problem using mixed reality glasses. Glasses contain the Spatial mapping module, presented in the form of an continuously updating mesh. Analyzing space through mixed reality glasses, we have developed algorithms for obtaining the shortest path in the Cartesian and Joint spaces. In the beginning, we implement A* simple algorithm and then realized the improved RRT algorithm. The approaches were verified using Unity, Microsoft HoloLens and 6 DOF robot UR10e.","paperId":"a89e2f4873905a7be25be0400f1a2ea52154b930","paperTitle":"Obstacle avoidance for robotic manipulator using Mixed reality glasses","paperUrl":"https://www.semanticscholar.org/paper/a89e2f4873905a7be25be0400f1a2ea52154b930","images":["https://d3i71xaburhd42.cloudfront.net/a89e2f4873905a7be25be0400f1a2ea52154b930/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/a89e2f4873905a7be25be0400f1a2ea52154b930/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/a89e2f4873905a7be25be0400f1a2ea52154b930/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/a89e2f4873905a7be25be0400f1a2ea52154b930/3-Figure4-1.png"],"pdf":"https://doi.org/10.1109/DCNAIR.2019.8875555"},{"type":"inproceedings","key":"aschenbrenner2020mirrorlabs","title":"Mirrorlabs-Creating Accessible Digital Twins of Robotic Production Environment with Mixed Reality","author":"Aschenbrenner, Doris and Rieder, Jonas SI and Van Tol, Dani\\"elle and Van Dam, Joris and Rusak, Zoltan and Blech, Jan Olaf and Azangoo, Mohammad and Panu, Salo and Kruusam\\"ae, Karl and Masnavi, Houman and others","booktitle":"2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)","pages":"43--48","year":"2020","organization":"IEEE","doi":"10.1109/AIVR50618.2020.00017","authors":["Doris Aschenbrenner","Jonas SI Rieder","Dani\\"elle Van Tol","Joris Van Dam","Zoltan Rusak","Jan Olaf Blech","Mohammad Azangoo","Salo Panu","Karl Kruusam\\"ae","Houman Masnavi","undefined others"],"dblp":"conf/aivr/AschenbrennerRT20","venue":"2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)","abstract":"How to visualize recorded production data in Virtual Reality? How to use state of the art Augmented Reality displays that can show robot data? This paper introduces an opensource ICT framework approach for combining Unity-based Mixed Reality applications with robotic production equipment using ROS Industrial. This publication gives details on the implementation and demonstrates the use as a data analysis tool in the context of scientific exchange within the area of Mixed Reality enabled human-robot co-production.","paperId":"654cd9284c503738d36e0140f52ed0e82fd487a7","paperTitle":"Mirrorlabs - creating accessible Digital Twins of robotic production environment with Mixed Reality","paperUrl":"https://www.semanticscholar.org/paper/654cd9284c503738d36e0140f52ed0e82fd487a7","images":["https://d3i71xaburhd42.cloudfront.net/654cd9284c503738d36e0140f52ed0e82fd487a7/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/654cd9284c503738d36e0140f52ed0e82fd487a7/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/654cd9284c503738d36e0140f52ed0e82fd487a7/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/654cd9284c503738d36e0140f52ed0e82fd487a7/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/654cd9284c503738d36e0140f52ed0e82fd487a7/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/654cd9284c503738d36e0140f52ed0e82fd487a7/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/654cd9284c503738d36e0140f52ed0e82fd487a7/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/654cd9284c503738d36e0140f52ed0e82fd487a7/6-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/654cd9284c503738d36e0140f52ed0e82fd487a7/6-Figure9-1.png"],"pdf":"https://research.aalto.fi/files/55593832/ELEC_Aschenbrenner_etal_Mirrorlabs_Creating_Accessible_AIVR2020_acceptedauthormanuscript.pdf"},{"type":"inproceedings","key":"gruenefeld2020mind","title":"Mind the ARm: Realtime Visualization of Robot Motion Intent in Head-Mounted Augmented Reality","author":"Gruenefeld, Uwe and Pr\\"adel, Lars and Illing, Jannike and Stratmann, Tim and Drolshagen, Sandra and Pfingsthorn, Max","booktitle":"Proceedings of the Conference on Mensch und Computer","pages":"259--266","year":"2020","doi":"10.1145/3404983.3405509","authors":["Uwe Gruenefeld","Lars Pr\\"adel","Jannike Illing","Tim Stratmann","Sandra Drolshagen","Max Pfingsthorn"],"dblp":"conf/mc/GruenefeldPISDP20","venue":"Mensch & Computer","abstract":"Established safety sensor technology shuts down industrial robots when a collision is detected, causing preventable loss of productivity. To minimize downtime, we implemented three Augmented Reality (AR) visualizations (Path, Preview, and Volume) which allow users to understand robot motion intent and give way to the robot. We compare the different visualizations in a user study in which a small cognitive task is performed in a shared workspace. We found that Preview and Path required significantly longer head rotations to perceive robot motion intent. Volume, however, required the shortest head rotation and was perceived as most safe, enabling closer proximity of the robot arm before one left the shared workspace without causing shutdowns.","paperId":"3a238a546be797a9d9a7261fc0e52dddfa727e16","paperTitle":"Mind the ARm: realtime visualization of robot motion intent in head-mounted augmented reality","paperUrl":"https://www.semanticscholar.org/paper/3a238a546be797a9d9a7261fc0e52dddfa727e16","images":["https://d3i71xaburhd42.cloudfront.net/3a238a546be797a9d9a7261fc0e52dddfa727e16/4-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/3a238a546be797a9d9a7261fc0e52dddfa727e16/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/3a238a546be797a9d9a7261fc0e52dddfa727e16/5-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/3a238a546be797a9d9a7261fc0e52dddfa727e16/5-Table3-1.png","https://d3i71xaburhd42.cloudfront.net/3a238a546be797a9d9a7261fc0e52dddfa727e16/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/3a238a546be797a9d9a7261fc0e52dddfa727e16/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/3a238a546be797a9d9a7261fc0e52dddfa727e16/7-Table4-1.png","https://d3i71xaburhd42.cloudfront.net/3a238a546be797a9d9a7261fc0e52dddfa727e16/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/3a238a546be797a9d9a7261fc0e52dddfa727e16/6-Figure6-1.png"],"pdf":"https://doi.org/10.1145/3404983.3405509"},{"type":"inproceedings","key":"topliss2018establishing","title":"Establishing the Role of a Virtual Lead Vehicle as a Novel Augmented Reality Navigational Aid","author":"Topliss, Bethan Hannah and Pampel, Sanna M and Burnett, Gary and Skrypchuk, Lee and Hare, Chrisminder","booktitle":"Proceedings of the 10th International Conference on Automotive User Interfaces and Interactive Vehicular Applications","pages":"137--145","year":"2018","doi":"10.1145/3239060.3239069","authors":["Bethan Hannah Topliss","Sanna M Pampel","Gary Burnett","Lee Skrypchuk","Chrisminder Hare"],"dblp":"conf/automotiveUI/ToplissPBSH18","venue":"AutomotiveUI","abstract":"This paper reports on two studies investigating how following a lead vehicle could act as a metaphor for an Augmented Reality (AR) system to support navigation tasks. For the first formative study, 34 participants completed a video-based evaluation of the role of a real lead vehicle when navigating a coherent journey. Verbal protocols indicated that a lead vehicle may be a valuable navigation aid at a range of different junction types, but not where drivers may desire a preview of upcoming steps or their overall orientation. A subsequent driving simulator study with 22 participants examined whether an AR lead vehicle may support drivers when navigating at complex junctions, specifically large multi-exit roundabouts. The virtual car led to good navigation and driving performance, which was comparable to a more traditional screen-fixed interface. Overall, this work demonstrates that a virtual lead vehicle may be beneficial within AR navigation devices.","paperId":"6457578bfb34d631c9fa8f556158e962a988eef4","paperTitle":"Establishing the Role of a Virtual Lead Vehicle as a Novel Augmented Reality Navigational Aid","paperUrl":"https://www.semanticscholar.org/paper/6457578bfb34d631c9fa8f556158e962a988eef4","images":["https://d3i71xaburhd42.cloudfront.net/6457578bfb34d631c9fa8f556158e962a988eef4/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/6457578bfb34d631c9fa8f556158e962a988eef4/2-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/6457578bfb34d631c9fa8f556158e962a988eef4/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/6457578bfb34d631c9fa8f556158e962a988eef4/3-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/6457578bfb34d631c9fa8f556158e962a988eef4/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/6457578bfb34d631c9fa8f556158e962a988eef4/7-Table3-1.png","https://d3i71xaburhd42.cloudfront.net/6457578bfb34d631c9fa8f556158e962a988eef4/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/6457578bfb34d631c9fa8f556158e962a988eef4/7-Table4-1.png","https://d3i71xaburhd42.cloudfront.net/6457578bfb34d631c9fa8f556158e962a988eef4/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/6457578bfb34d631c9fa8f556158e962a988eef4/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/6457578bfb34d631c9fa8f556158e962a988eef4/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/6457578bfb34d631c9fa8f556158e962a988eef4/7-Figure8-1.png"],"pdf":"https://doi.org/10.1145/3239060.3239069"},{"type":"inproceedings","key":"materna2018interactive","title":"Interactive Spatial Augmented Reality in Collaborative Robot Programming: User Experience Evaluation","author":"Materna, Zden\u0117k and Kapinus, Michal and Beran, V\'it\u0117zslav and Smr\u017c, Pavel and Zem\u010b\'ik, Pavel","booktitle":"2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)","pages":"80--87","year":"2018","organization":"IEEE","doi":"10.1109/ROMAN.2018.8525662","authors":["Zden\u0117k Materna","Michal Kapinus","V\'it\u0117zslav Beran","Pavel Smr\u017c","Pavel Zem\u010b\'ik"],"dblp":"conf/ro-man/MaternaKBSZ18","venue":"2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)","abstract":"This paper presents a novel approach to interaction between human workers and industrial collaborative robots. The proposed approach addresses problems introduced by existing solutions for robot programming. It aims to reduce the mental demands and attention switches by centering all interaction in a shared workspace, combining various modalities and enabling interaction with the system without any external devices. The concept allows simple programming in the form of setting program parameters using spatial augmented reality for visualization and a touch-enabled table and robotic arms as input devices. We evaluated the concept utilizing a user experience study with six participants (shop-floor workers). All participants were able to program the robot and to collaborate with it using the program they parametrized. The final goal is to create a distraction-free, usable and low-effort interface for effective human-robot collaboration, enabling any ordinary skilled worker to customize the robot\'s program to changes in production or to personal (e.g. ergonomic) needs.","paperId":"ae06abcc72eea584ce2f7dca1607ce0d4d037c90","paperTitle":"Interactive Spatial Augmented Reality in Collaborative Robot Programming: User Experience Evaluation","paperUrl":"https://www.semanticscholar.org/paper/ae06abcc72eea584ce2f7dca1607ce0d4d037c90","images":["https://d3i71xaburhd42.cloudfront.net/ae06abcc72eea584ce2f7dca1607ce0d4d037c90/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/ae06abcc72eea584ce2f7dca1607ce0d4d037c90/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/ae06abcc72eea584ce2f7dca1607ce0d4d037c90/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/ae06abcc72eea584ce2f7dca1607ce0d4d037c90/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/ae06abcc72eea584ce2f7dca1607ce0d4d037c90/7-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/ae06abcc72eea584ce2f7dca1607ce0d4d037c90/6-TableI-1.png","https://d3i71xaburhd42.cloudfront.net/ae06abcc72eea584ce2f7dca1607ce0d4d037c90/8-TableII-1.png","https://d3i71xaburhd42.cloudfront.net/ae06abcc72eea584ce2f7dca1607ce0d4d037c90/8-TableIII-1.png"],"pdf":"https://doi.org/10.1109/ROMAN.2018.8525662"},{"type":"inproceedings","key":"ghiringhelli2014interactive","title":"Interactive Augmented Reality for Understanding and Analyzing Multi-Robot Systems","author":"Ghiringhelli, Fabrizio and Guzzi, J\'er^ome and Di Caro, Gianni A and Caglioti, Vincenzo and Gambardella, Luca M and Giusti, Alessandro","booktitle":"2014 IEEE/RSJ International Conference on Intelligent Robots and Systems","pages":"1195--1201","year":"2014","organization":"IEEE","doi":"10.1109/IROS.2014.6942709","authors":["Fabrizio Ghiringhelli","J\'er^ome Guzzi","Gianni A Di Caro","Vincenzo Caglioti","Luca M Gambardella","Alessandro Giusti"],"dblp":"conf/iros/GhiringhelliGCCGG14","venue":"2014 IEEE/RSJ International Conference on Intelligent Robots and Systems","abstract":"Once a multi-robot system is implemented on real hardware and tested in the real world, analyzing its evolution and debugging unexpected behaviors is often a very difficult task. We present a tool for aiding this activity, by visualizing an Augmented Reality overlay on a live video feed acquired by a fixed camera overlooking the robot environment. Such overlay displays live information exposed by each robot, which may be textual (state messages), symbolic (graphs, charts), or, most importantly, spatially-situated; spatially-situated information is related to the environment surrounding the robot itself, such as for example the perceived position of neighboring robots, the perceived extent of obstacles, the path the robot plans to follow. We show that, by directly representing such information on the environment it refers to, our proposal removes a layer of indirection and significantly eases the process of understanding complex multi-robot systems. We describe how the system is implemented, discuss application examples in different scenarios, and provide supplementary material including demonstration videos and a functional implementation.","paperId":"fe83cb2e293addb69165e0854f572aac46fb0808","paperTitle":"Interactive Augmented Reality for understanding and analyzing multi-robot systems","paperUrl":"https://www.semanticscholar.org/paper/fe83cb2e293addb69165e0854f572aac46fb0808","images":["https://d3i71xaburhd42.cloudfront.net/fe83cb2e293addb69165e0854f572aac46fb0808/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/fe83cb2e293addb69165e0854f572aac46fb0808/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/fe83cb2e293addb69165e0854f572aac46fb0808/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/fe83cb2e293addb69165e0854f572aac46fb0808/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/fe83cb2e293addb69165e0854f572aac46fb0808/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/fe83cb2e293addb69165e0854f572aac46fb0808/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/fe83cb2e293addb69165e0854f572aac46fb0808/7-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/fe83cb2e293addb69165e0854f572aac46fb0808/7-Figure8-1.png"],"pdf":"http://www.leet.it/home/giusti/website/lib/exe/fetch.php?media=wiki:iros2014.pdf"},{"type":"inproceedings","key":"gadre2019end","title":"End-User Robot Programming Using Mixed Reality","author":"Gadre, Samir Yitzhak and Rosen, Eric and Chien, Gary and Phillips, Elizabeth and Tellex, Stefanie and Konidaris, George","booktitle":"2019 International conference on robotics and automation (ICRA)","pages":"2707--2713","year":"2019","organization":"IEEE","doi":"10.1109/ICRA.2019.8793988","authors":["Samir Yitzhak Gadre","Eric Rosen","Gary Chien","Elizabeth Phillips","Stefanie Tellex","George Konidaris"],"dblp":"conf/icra/GadreRCPTK19","venue":"2019 International Conference on Robotics and Automation (ICRA)","abstract":"Mixed Reality (MR) is a promising interface for robot programming because it can project an immersive 3D visualization of a robot\u2019s intended movement onto the real world. MR can also support hand gestures, which provide an intuitive way for users to construct and modify robot motions. We present a Mixed Reality Head-Mounted Display (MRHMD) interface that enables end-users to easily create and edit robot motions using waypoints. We describe a user study where 20 participants were asked to program a robot arm using 2D and MR interfaces to perform two pick-and-place tasks. In the primitive task, participants created typical pickand-place programs. In the adapted task, participants adapted their primitive programs to address a more complex pickand-place scenario, which included obstacles and conditional reasoning. Compared to the 2D interface, a higher number of users were able to complete both tasks in significantly less time, and reported experiencing lower cognitive workload, higher usability, and higher naturalness with the MR-HMD interface.","paperId":"f60ca31c432009f571a32f10d6bad9f86a0fee6e","paperTitle":"End-User Robot Programming Using Mixed Reality","paperUrl":"https://www.semanticscholar.org/paper/f60ca31c432009f571a32f10d6bad9f86a0fee6e","images":["https://d3i71xaburhd42.cloudfront.net/f60ca31c432009f571a32f10d6bad9f86a0fee6e/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/f60ca31c432009f571a32f10d6bad9f86a0fee6e/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/f60ca31c432009f571a32f10d6bad9f86a0fee6e/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/f60ca31c432009f571a32f10d6bad9f86a0fee6e/6-Figure4-1.png"],"pdf":"http://cs.brown.edu/people/er35/publications/enduser.pdf"},{"type":"article","key":"clemente2016humans","title":"Humans Can Integrate Augmented Reality Feedback in Their Sensorimotor Control of a Robotic Hand","author":"Clemente, Francesco and Dosen, Strahinja and Lonini, Luca and Markovic, Marko and Farina, Dario and Cipriani, Christian","journal":"IEEE Transactions on Human-Machine Systems","volume":"47","number":"4","pages":"583--589","year":"2016","publisher":"IEEE","doi":"10.1109/THMS.2016.2611998","authors":["Francesco Clemente","Strahinja Dosen","Luca Lonini","Marko Markovic","Dario Farina","Christian Cipriani"],"dblp":"journals/thms/ClementeDLMFC17","venue":"IEEE Transactions on Human-Machine Systems","abstract":"Tactile feedback is pivotal for grasping and manipulation in humans. Providing functionally effective sensory feedback to prostheses users is an open challenge. Past paradigms were mostly based on vibro- or electrotactile stimulations. However, the tactile sensitivity on the targeted body parts (usually the forearm) is greatly less than that of the hand/fingertips, restricting the amount of information that can be provided through this channel. Visual feedback is the most investigated technique in motor learning studies, where it showed positive effects in learning both simple and complex tasks; however, it was not exploited in prosthetics due to technological limitations. Here, we investigated if visual information provided in the form of augmented reality (AR) feedback can be integrated by able-bodied participants in their sensorimotor control of a pick-and-lift task while controlling a robotic hand. For this purpose, we provided visual continuous feedback related to grip force and hand closure to the participants. Each variable was mapped to the length of one of the two ellipse axes visualized on the screen of wearable single-eye display AR glasses. We observed changes in behavior when subtle (i.e., not announced to the participants) manipulation of the AR feedback was introduced, which indicated that the participants integrated the artificial feedback within the sensorimotor control of the task. These results demonstrate that it is possible to deliver effective information through AR feedback in a compact and wearable fashion. This feedback modality may be exploited for delivering sensory feedback to amputees in a clinical scenario.","paperId":"918290a32cc7cde320f46d52aa2f15df1096eacb","paperTitle":"Humans Can Integrate Augmented Reality Feedback in Their Sensorimotor Control of a Robotic Hand","paperUrl":"https://www.semanticscholar.org/paper/918290a32cc7cde320f46d52aa2f15df1096eacb","images":["https://d3i71xaburhd42.cloudfront.net/918290a32cc7cde320f46d52aa2f15df1096eacb/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/918290a32cc7cde320f46d52aa2f15df1096eacb/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/918290a32cc7cde320f46d52aa2f15df1096eacb/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/918290a32cc7cde320f46d52aa2f15df1096eacb/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/918290a32cc7cde320f46d52aa2f15df1096eacb/6-Figure5-1.png"],"pdf":"http://spiral.imperial.ac.uk/bitstream/10044/1/49548/2/Clemente%202017.pdf"},{"type":"inproceedings","key":"qiu2020human","title":"Human-Robot Interaction in a Shared Augmented Reality Workspace","author":"Qiu, Shuwen and Liu, Hangxin and Zhang, Zeyu and Zhu, Yixin and Zhu, Song-Chun","booktitle":"2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","pages":"11413--11418","year":"2020","organization":"IEEE","doi":"10.1109/IROS45743.2020.9340781","authors":["Shuwen Qiu","Hangxin Liu","Zeyu Zhang","Yixin Zhu","Song-Chun Zhu"],"dblp":"journals/corr/abs-2007-12656","venue":"2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","abstract":"We design and develop a new shared Augmented Reality (AR) workspace for Human-Robot Interaction (HRI), which establishes a bi-directional communication between human agents and robots. In a prototype system, the shared AR workspace enables a shared perception, so that a physical robot not only perceives the virtual elements in its own view but also infers the utility of the human agent\u2014the cost needed to perceive and interact in AR\u2014by sensing the human agent\u2019s gaze and pose. Such a new HRI design also affords a shared manipulation, wherein the physical robot can control and alter virtual objects in AR as an active agent; crucially, a robot can proactively interact with human agents, instead of purely passively executing received commands. In experiments, we design a resource collection game that qualitatively demonstrates how a robot perceives, processes, and manipulates in AR and quantitatively evaluates the efficacy of HRI using the shared AR workspace. We further discuss how the system can potentially benefit future HRI studies that are otherwise challenging.","paperId":"6203c7daaeb10e9b9c66e67b42320625710a338a","paperTitle":"Human-Robot Interaction in a Shared Augmented Reality Workspace","paperUrl":"https://www.semanticscholar.org/paper/6203c7daaeb10e9b9c66e67b42320625710a338a","images":["https://d3i71xaburhd42.cloudfront.net/6203c7daaeb10e9b9c66e67b42320625710a338a/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/6203c7daaeb10e9b9c66e67b42320625710a338a/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/6203c7daaeb10e9b9c66e67b42320625710a338a/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/6203c7daaeb10e9b9c66e67b42320625710a338a/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/6203c7daaeb10e9b9c66e67b42320625710a338a/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/6203c7daaeb10e9b9c66e67b42320625710a338a/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/6203c7daaeb10e9b9c66e67b42320625710a338a/5-Figure7-1.png"],"pdf":"http://arxiv.org/pdf/2007.12656"},{"type":"inproceedings","key":"magnenat2015enhancing","title":"Enhancing Robot Programming with Visual Feedback and Augmented Reality","author":"Magnenat, St\'ephane and Ben-Ari, Morderchai and Klinger, Severin and Sumner, Robert W","booktitle":"Proceedings of the 2015 ACM conference on innovation and technology in computer science education","pages":"153--158","year":"2015","doi":"10.1145/2729094.2742585","authors":["St\'ephane Magnenat","Morderchai Ben-Ari","Severin Klinger","Robert W Sumner"],"dblp":"conf/iticse/MagnenatBKS15","venue":"ITiCSE","abstract":"In our previous research, we showed that students using the educational robot Thymio and its visual programming environment were able to learn the important computer-science concept of event-handling. This paper extends that work by integrating augmented reality (AR) into the activities. Students used a tablet that displays in real time the event executed on the robot. The event is overlaid on the tablet over the image from a camera, which shows the location of the robot when the event was executed. In addition, visual feedback (FB) was implemented in the software. We developed a novel video questionnaire to investigate the performance of the students on robotics tasks. Data were collected comparing four groups: AR+FB, AR+non-FB, non-AR+FB, non-AR+non-FB. The results showed that students receiving feedback made significantly fewer errors on the tasks. Those using AR made fewer errors, but this improvement was not significant, although their performance improved. Technical problems with the AR hardware and software showed where improvements are needed.","paperId":"786f8feac0f99046792e2fcb038db738305fba3c","paperTitle":"Enhancing Robot Programming with Visual Feedback and Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/786f8feac0f99046792e2fcb038db738305fba3c","images":["https://d3i71xaburhd42.cloudfront.net/786f8feac0f99046792e2fcb038db738305fba3c/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/786f8feac0f99046792e2fcb038db738305fba3c/5-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/786f8feac0f99046792e2fcb038db738305fba3c/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/786f8feac0f99046792e2fcb038db738305fba3c/5-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/786f8feac0f99046792e2fcb038db738305fba3c/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/786f8feac0f99046792e2fcb038db738305fba3c/5-Table3-1.png","https://d3i71xaburhd42.cloudfront.net/786f8feac0f99046792e2fcb038db738305fba3c/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/786f8feac0f99046792e2fcb038db738305fba3c/5-Table4-1.png","https://d3i71xaburhd42.cloudfront.net/786f8feac0f99046792e2fcb038db738305fba3c/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/786f8feac0f99046792e2fcb038db738305fba3c/6-Table5-1.png"],"pdf":"http://stephane.magnenat.net/publications/Enhancing%20Robot%20Programming%20With%20Visual%20Feedback%20and%20Augmented%20Reality%20-%20Magnenat%20et%20al.%20-%20ITiCSE%20-%202015.pdf"},{"type":"inproceedings","key":"reardon2019communicating","title":"Communicating via Augmented Reality for Human-Robot Teaming in Field Environments","author":"Reardon, Christopher and Lee, Kevin and Rogers, John G and Fink, Jonathan","booktitle":"2019 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)","pages":"94--101","year":"2019","organization":"IEEE","doi":"10.1109/SSRR.2019.8848971","authors":["Christopher Reardon","Kevin Lee","John G Rogers","Jonathan Fink"],"dblp":"conf/ssrr/ReardonLRF19","venue":"2019 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)","abstract":"This work focuses on the critical problem of treating humans and autonomous robots as peer teammates for tasks performed in unstructured, uninstrumented environments. To accomplish this peer relationship, we propose to use the communication of information to the human and the autonomous robot teammate as a method of influencing their behavior. We use augmented reality technology to achieve a bidirectional communication of information between teammates. We outline multiple strategies for information communication from the autonomous robot to the human teammate. We examine these alternatives in the context of human-robot cooperative exploration of an unknown and uninstrumented environment. We present results from three experiments which show potential influence on human task performance in cooperative exploration.","paperId":"c8ce166295621695eac2f6362670249fc77aa11a","paperTitle":"Communicating via Augmented Reality for Human-Robot Teaming in Field Environments","paperUrl":"https://www.semanticscholar.org/paper/c8ce166295621695eac2f6362670249fc77aa11a","images":["https://d3i71xaburhd42.cloudfront.net/c8ce166295621695eac2f6362670249fc77aa11a/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/c8ce166295621695eac2f6362670249fc77aa11a/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/c8ce166295621695eac2f6362670249fc77aa11a/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/c8ce166295621695eac2f6362670249fc77aa11a/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/c8ce166295621695eac2f6362670249fc77aa11a/7-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/c8ce166295621695eac2f6362670249fc77aa11a/8-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/c8ce166295621695eac2f6362670249fc77aa11a/6-TableI-1.png"],"pdf":"https://doi.org/10.1109/SSRR.2019.8848971"},{"type":"inproceedings","key":"kastner2019augmented","title":"Augmented-Reality-Based Visualization of Navigation Data of Mobile Robots on the Microsoft Hololens-Possibilities and Limitations","author":"K\\"astner, Linh and Lambrecht, Jens","booktitle":"2019 IEEE International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE Conference on Robotics, Automation and Mechatronics (RAM)","pages":"344--349","year":"2019","organization":"IEEE","doi":"10.1109/CIS-RAM47153.2019.9095836","authors":["Linh K\\"astner","Jens Lambrecht"],"dblp":"conf/ram/KastnerL19","venue":"2019 IEEE International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE Conference on Robotics, Automation and Mechatronics (RAM)","abstract":"The demand for mobile robots has rapidly increased in recent years due to the flexibility and high variety of application fields comparing to static robots. To deal with complex tasks such as navigation, they work with high amounts of different sensor data making it difficult to operate with for non-experts. To enhance user understanding and human robot interaction, we propose an approach to visualize the navigation stack within a cutting edge 3D Augmented Reality device -the Microsoft Hololens. Therefore, relevant navigation stack data including laser scan, environment map and path planing data are visualized in 3D within the head mounted device. Based on that prototype, we evaluate the Hololens in terms of computational capabilities and limitations for dealing with huge amount of real-time data. Results show that the Hololens is capable of a proper visualization of huge amounts of sensor data. We demonstrate a proper visualization of navigation stack data in 3D within the Hololens. However, there are limitations when transferring and displaying different kinds of data simultaneously.","paperId":"59399e26b03a0753c659e7c88a27e8e29fede07c","paperTitle":"Augmented-Reality-Based Visualization of Navigation Data of Mobile Robots on the Microsoft Hololens - Possibilities and Limitations","paperUrl":"https://www.semanticscholar.org/paper/59399e26b03a0753c659e7c88a27e8e29fede07c","images":["https://d3i71xaburhd42.cloudfront.net/59399e26b03a0753c659e7c88a27e8e29fede07c/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/59399e26b03a0753c659e7c88a27e8e29fede07c/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/59399e26b03a0753c659e7c88a27e8e29fede07c/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/59399e26b03a0753c659e7c88a27e8e29fede07c/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/59399e26b03a0753c659e7c88a27e8e29fede07c/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/59399e26b03a0753c659e7c88a27e8e29fede07c/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/59399e26b03a0753c659e7c88a27e8e29fede07c/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/59399e26b03a0753c659e7c88a27e8e29fede07c/5-Figure8-1.png"],"pdf":"http://arxiv.org/pdf/1912.12109"},{"type":"inproceedings","key":"peppoloni2015augmented","title":"Augmented Reality-Aided Tele-Presence System for Robot Manipulation in Industrial Manufacturing","author":"Peppoloni, Lorenzo and Brizzi, Filippo and Ruffaldi, Emanuele and Avizzano, Carlo Alberto","booktitle":"Proceedings of the 21st ACM Symposium on Virtual Reality Software and Technology","pages":"237--240","year":"2015","doi":"10.1145/2821592.2821620","authors":["Lorenzo Peppoloni","Filippo Brizzi","Emanuele Ruffaldi","Carlo Alberto Avizzano"],"dblp":"conf/vrst/PeppoloniBRA15","venue":"VRST","abstract":"This work investigates the use of a highly immersive telepresence system for industrial robotics. A Robot Operating System integrated framework is presented where a remote robot is controlled through operator\'s movements and muscle contractions captured with a wearable device. An augmented 3D visual feedback is sent to the user providing the remote environment scenario from the robot\'s point of view and additional information pertaining to the task execution. The system proposed, using robot mounted RGB-D camera, identifies known objects and relates their pose to robot arm pose and to targets relevant to the task execution. The system is preliminary validated during a pick-and-place task using a Baxter robot. The experiment shows the practicability and the effectiveness of the proposed approach.","paperId":"bec9233f8dd8faf14437ff3d858d8ea20d2971ec","paperTitle":"Augmented reality-aided tele-presence system for robot manipulation in industrial manufacturing","paperUrl":"https://www.semanticscholar.org/paper/bec9233f8dd8faf14437ff3d858d8ea20d2971ec","images":["https://d3i71xaburhd42.cloudfront.net/bec9233f8dd8faf14437ff3d858d8ea20d2971ec/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/bec9233f8dd8faf14437ff3d858d8ea20d2971ec/4-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/bec9233f8dd8faf14437ff3d858d8ea20d2971ec/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/bec9233f8dd8faf14437ff3d858d8ea20d2971ec/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/bec9233f8dd8faf14437ff3d858d8ea20d2971ec/4-Figure4-1.png"],"pdf":"http://www.eruffaldi.com/papers/2015_C_VRST.pdf"},{"type":"inproceedings","key":"garcia2011educational","title":"Educational Autonomous Robotics Setup Using Mixed Reality","author":"Garc\'ia, Abraham Prieto and Fern\'andez, Gervasio Varela and Torres, Blanca Mar\'ia Priego and L\'opez-Pe~na, Fernando","booktitle":"2011 7th International Conference on Next Generation Web Services Practices","pages":"452--457","year":"2011","organization":"IEEE","doi":"10.1109/NWESP.2011.6088222","authors":["Abraham Prieto Garc\'ia","Gervasio Varela Fern\'andez","Blanca Mar\'ia Priego Torres","Fernando L\'opez-Pe~na"],"venue":"2011 7th International Conference on Next Generation Web Services Practices","abstract":"This paper is concerned with the presentation of a mixed-reality operational environment for carrying out experiments in robotics. This set up was designed as an intermediate system between a real and a simulated scenario in order to work with real robots and to ease and simplify the configuration and modification of the rest of the elements of the experiments. The environment uses video projection over the arena of the experiment, image capture to obtain the current position of the robots and a computer which monitors and controls the scenario. The graphic representation of the virtual elements of the scenario is projected onto the arena and their state is updated based on world rules of the experiment and on the state and actions of the robots. The scheme followed to control and structure this environment is based on the one proposed in a previous work for the creation of a simulation environment named Waspbed, which was designed for the study of simulated coevolution processes in multiagent systems. This setup is used for getting students from engineering degrees used to deal with robotics and all its associated fields, such as artificial vision, evolutionary robotics, communication protocols, etc. in a very simple, quick and cheap way, which otherwise wouldn\'t be feasible taking into account the academic constraints of time and resources.","paperId":"cb8eebdde5541ac1ef73d01efa27bd93f40d93c7","paperTitle":"Educational autonomous robotics setup using mixed reality","paperUrl":"https://www.semanticscholar.org/paper/cb8eebdde5541ac1ef73d01efa27bd93f40d93c7","images":["https://d3i71xaburhd42.cloudfront.net/cb8eebdde5541ac1ef73d01efa27bd93f40d93c7/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/cb8eebdde5541ac1ef73d01efa27bd93f40d93c7/5-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/cb8eebdde5541ac1ef73d01efa27bd93f40d93c7/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/cb8eebdde5541ac1ef73d01efa27bd93f40d93c7/6-Figure4-1.png"],"pdf":"https://doi.org/10.1109/NWESP.2011.6088222"},{"type":"inproceedings","key":"huang2020augmented","title":"Augmented Reality with Multi-View Merging for Robot Teleoperation","author":"Huang, Bidan and Timmons, Nicholas Gerard and Li, Qiang","booktitle":"Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction","pages":"260--262","year":"2020","doi":"10.1145/3371382.3378336","authors":["Bidan Huang","Nicholas Gerard Timmons","Qiang Li"],"dblp":"conf/hri/HuangTL20","venue":"HRI","abstract":"This paper proposes a user-friendly teleoperation interface for manipulation. We provide the user with a view of the scene augmented with depth information from local cameras to provide visibility in occluded areas during manipulation tasks. This gives an improved sense of the 3D environment which results in better task performance. Further, we monitor the pose of the robot\'s end effector in real-time so that we are able to superimpose a virtual representation into the scene when parts are occluded. The integration of these features enables the user to perform difficult manipulation tasks when the action environment is normally occluded in the main camera view. We performed preliminary studies with this new setup and users provided positive feedback regarding the proposed augmented reality (AR) system.","paperId":"9856a4fa1efc8f43d1f74ef2b901b44824e6850f","paperTitle":"Augmented Reality with Multi-view Merging for Robot Teleoperation","paperUrl":"https://www.semanticscholar.org/paper/9856a4fa1efc8f43d1f74ef2b901b44824e6850f","images":["https://d3i71xaburhd42.cloudfront.net/28ecec07d360d92a5ca075c595194bef9e2b8341/1-Figure1-1.png"],"pdf":"https://doi.org/10.1145/3371382.3378336"},{"type":"inproceedings","key":"livatino2010augmented","title":"Augmented Reality Stereoscopic Visualization for Intuitive Robot Teleguide","author":"Livatino, Salvatore and Muscato, Giovanni and De Tommaso, Davide and Macaluso, Marco","booktitle":"2010 IEEE International Symposium on Industrial Electronics","pages":"2828--2833","year":"2010","organization":"IEEE","doi":"10.1109/ISIE.2010.5636955","authors":["Salvatore Livatino","Giovanni Muscato","Davide De Tommaso","Marco Macaluso"],"venue":"2010 IEEE International Symposium on Industrial Electronics","abstract":"This paper proposes a method to simultaneously and coherently present visual and laser sensors information through an augmented reality visualization interface further enhanced by stereoscopic viewing. The use of graphical objects is proposed to represent proximity measurements which are superimposed and suitably aligned to video information through image processing. This new methodology enables an operator to quickly comprehend scene layout and dynamics, and to respond in an accurate and timely manner. Therefore the human-robot interaction is expected to be intuitive, accurate and fast. The use of graphical elements to assist teleoperation, sometime discussed in the literature, is here proposed following a systematic approach and developed based on authors\' previous works on stereoscopic teleoperation. The approach is experimented on a real telerobotic system where a user operates a robot located approximately 3,000 kilometers apart. The results of a pilot test were very encouraging. They showed simplicity and effectiveness of the approach proposed and represent a base for further investigations.","paperId":"0488af25639d07ab6e21b11037157133fc05c478","paperTitle":"Augmented reality stereoscopic visualization for intuitive robot teleguide","paperUrl":"https://www.semanticscholar.org/paper/0488af25639d07ab6e21b11037157133fc05c478","images":["https://d3i71xaburhd42.cloudfront.net/0488af25639d07ab6e21b11037157133fc05c478/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/0488af25639d07ab6e21b11037157133fc05c478/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/0488af25639d07ab6e21b11037157133fc05c478/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/0488af25639d07ab6e21b11037157133fc05c478/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/0488af25639d07ab6e21b11037157133fc05c478/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/0488af25639d07ab6e21b11037157133fc05c478/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/0488af25639d07ab6e21b11037157133fc05c478/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/0488af25639d07ab6e21b11037157133fc05c478/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/0488af25639d07ab6e21b11037157133fc05c478/5-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/0488af25639d07ab6e21b11037157133fc05c478/5-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/0488af25639d07ab6e21b11037157133fc05c478/6-Figure11-1.png"],"pdf":"http://uhra.herts.ac.uk/bitstream/2299/5227/1/Augmented%20reality%20stereoscopic%20visualization%20for%20intuitive%20robot%20teleguide.pdf"},{"type":"inproceedings","key":"diehl2020augmented","title":"Augmented Reality Interface to Verify Robot Learning","author":"Diehl, Maximilian and Plopski, Alexander and Kato, Hirokazu and Ramirez-Amaro, Karinne","booktitle":"2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","pages":"378--383","year":"2020","organization":"IEEE","doi":"10.1109/RO-MAN47096.2020.9223502","authors":["Maximilian Diehl","Alexander Plopski","Hirokazu Kato","Karinne Ramirez-Amaro"],"dblp":"conf/ro-man/DiehlPKR20","venue":"2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","abstract":"Teaching robots new skills is considered as an important aspect of Human-Robot Collaboration (HRC). One challenge is that robots cannot communicate feedback in the same ways as humans do. This decreases the trust towards robots since it is difficult to judge, before the actual execution, if the robot has learned the task correctly. In this paper, we introduce an Augmented Reality (AR) based visualization tool that allows humans to verify the taught behavior before its execution. Our verification interface displays a virtual simulation embedded into the real environment, timely coupled with a semantic description of the current action. We developed three designs based on different interface/visualization-technology combinations to explore the potential benefits of enhanced simulations using AR over traditional simulation environments like RViz. We conducted a user study with 18 participants to assess the effectiveness of the proposed visualization tools regarding error detection capabilities. One of the advantages of the AR interfaces is that they provide more realistic feedback than traditional simulations with a lower cost of not having to model the entire environment.","paperId":"71efe68ff75996821d0dffb683d172b582f8a889","paperTitle":"Augmented Reality interface to verify Robot Learning","paperUrl":"https://www.semanticscholar.org/paper/71efe68ff75996821d0dffb683d172b582f8a889","images":["https://d3i71xaburhd42.cloudfront.net/71efe68ff75996821d0dffb683d172b582f8a889/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/71efe68ff75996821d0dffb683d172b582f8a889/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/71efe68ff75996821d0dffb683d172b582f8a889/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/71efe68ff75996821d0dffb683d172b582f8a889/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/71efe68ff75996821d0dffb683d172b582f8a889/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/71efe68ff75996821d0dffb683d172b582f8a889/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/71efe68ff75996821d0dffb683d172b582f8a889/4-TableI-1.png","https://d3i71xaburhd42.cloudfront.net/71efe68ff75996821d0dffb683d172b582f8a889/4-TableII-1.png"],"pdf":"https://doi.org/10.1109/RO-MAN47096.2020.9223502"},{"type":"inproceedings","key":"reina2015augmented","title":"Augmented Reality for Robots: Virtual Sensing Technology Applied to a Swarm of E-Pucks","author":"Reina, Andreagiovanni and Salvaro, Mattia and Francesca, Gianpiero and Garattoni, Lorenzo and Pinciroli, Carlo and Dorigo, Marco and Birattari, Mauro","booktitle":"2015 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)","pages":"1--6","year":"2015","organization":"IEEE","doi":"10.1109/AHS.2015.7231154","authors":["Andreagiovanni Reina","Mattia Salvaro","Gianpiero Francesca","Lorenzo Garattoni","Carlo Pinciroli","Marco Dorigo","Mauro Birattari"],"dblp":"conf/ahs/ReinaSFGPDB15","venue":"2015 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)","abstract":"We present a novel technology that allows real robots to perceive an augmented reality environment through virtual sensors. Virtual sensors are a useful and desirable technology for research activities because they allow researchers to quickly and efficiently perform experiments that would otherwise be more expensive, or even impossible. In particular, augmented reality is useful (i) for prototyping and assessing the impact of new sensors before they are physically produced; and (ii) for developing and studying the behaviour of robots that should deal with phenomena that cannot be easily reproduced in a laboratory environment because, for example, they are dangerous (e.g., fire, radiations). We realised an augmented reality system for robots in which a simulator retrieves real-time data on the real environment through a multi-camera tracking system and delivers post-processed information to the robot swarm according to each robot\'s sensing range. We illustrate the proposed virtual sensing technology through an experiment involving 15 e-pucks.","paperId":"0c777e102e946aa81dfe5e535e3f1f9e97fe6137","paperTitle":"Augmented reality for robots: Virtual sensing technology applied to a swarm of e-pucks","paperUrl":"https://www.semanticscholar.org/paper/0c777e102e946aa81dfe5e535e3f1f9e97fe6137","images":["https://d3i71xaburhd42.cloudfront.net/0c777e102e946aa81dfe5e535e3f1f9e97fe6137/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/0c777e102e946aa81dfe5e535e3f1f9e97fe6137/6-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/0c777e102e946aa81dfe5e535e3f1f9e97fe6137/2-TableI-1.png","https://d3i71xaburhd42.cloudfront.net/0c777e102e946aa81dfe5e535e3f1f9e97fe6137/5-TableII-1.png"],"pdf":"http://iridia.ulb.ac.be/%7Eareina/pdf/reina_AHS2015.pdf"},{"type":"inproceedings","key":"lupetti2016designing","title":"Designing Playful HRI: Acceptability of Robots in Everyday Life through Play","author":"Lupetti, Maria Luce","booktitle":"2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","pages":"631--632","year":"2016","organization":"IEEE","doi":"10.1109/HRI.2016.7451891","authors":["Maria Luce Lupetti"],"dblp":"conf/hri/Lupetti16","venue":"2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","abstract":"The spread of edutainment robotics in everyday life raises new opportunities that can lead to a redefinition of the traditional game scenarios. Robots, indeed, represents a challenge for designer since allows a physical embodiment of a game character/element. These new opportunities have been analyzed in parallel with the world of childhood, its main characteristics, current topics and emerging issues. This analysis is at the basis of the Phygital Play project, a mixed-reality playground in which children can play with or against a robot. The project aims to encourage children to play physically in order to reduce sedentary behaviors, which are recently increasing accordingly to the spread of screen-based activities.","paperId":"58e6d163660c408a19291e0e8208d5375b17e55b","paperTitle":"Designing playful HRI: Acceptability of robots in everyday life through play","paperUrl":"https://www.semanticscholar.org/paper/58e6d163660c408a19291e0e8208d5375b17e55b","images":["https://d3i71xaburhd42.cloudfront.net/58e6d163660c408a19291e0e8208d5375b17e55b/2-Figure1-1.png"],"pdf":"http://dl.acm.org/citation.cfm?id=2907016"},{"type":"inproceedings","key":"xia2012augmented","title":"Augmented Reality Environment with Virtual Fixtures for Robotic Telemanipulation in Space","author":"Xia, Tian and L\'eonard, Simon and Deguet, Anton and Whitcomb, Louis and Kazanzides, Peter","booktitle":"2012 IEEE/RSJ International Conference on Intelligent Robots and Systems","pages":"5059--5064","year":"2012","organization":"IEEE","doi":"10.1109/IROS.2012.6386169","authors":["Tian Xia","Simon L\'eonard","Anton Deguet","Louis Whitcomb","Peter Kazanzides"],"dblp":"conf/iros/XiaLDWK12","venue":"2012 IEEE/RSJ International Conference on Intelligent Robots and Systems","abstract":"This paper presents an augmented reality framework, implemented on the master console of a modified da Vinci\xae surgical robot, that enables the operator to design and implement assistive virtual fixtures during teleoperation. Our specific goal is to facilitate teleoperation with large time delays, such as the delay of several seconds that occurs with ground-based control of robotic systems in earth orbit. The virtual fixtures give immediate visual feedback and motion guidance to the operator, while the remote slave performs motions consistent with those constraints. This approach is suitable for tasks in unstructured environments, such as servicing of existing on-orbit spacecraft that were not designed for servicing. We conducted a pilot study by teleoperating a remote slave robot for a thermal barrier blanket cutting task using virtual fixtures with and without time delay. The results show that virtual fixtures reduce the time required to complete the task while also eliminating significant manipulation errors, such as tearing the blanket. The improvement in performance is especially dramatic when a simulated time delay (4 seconds) is introduced.","paperId":"dd21a9a3166557a4bf5aae14d636d02d996c427d","paperTitle":"Augmented reality environment with virtual fixtures for robotic telemanipulation in space","paperUrl":"https://www.semanticscholar.org/paper/dd21a9a3166557a4bf5aae14d636d02d996c427d","images":["https://d3i71xaburhd42.cloudfront.net/dd21a9a3166557a4bf5aae14d636d02d996c427d/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/dd21a9a3166557a4bf5aae14d636d02d996c427d/5-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/dd21a9a3166557a4bf5aae14d636d02d996c427d/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/dd21a9a3166557a4bf5aae14d636d02d996c427d/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/dd21a9a3166557a4bf5aae14d636d02d996c427d/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/dd21a9a3166557a4bf5aae14d636d02d996c427d/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/dd21a9a3166557a4bf5aae14d636d02d996c427d/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/dd21a9a3166557a4bf5aae14d636d02d996c427d/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/dd21a9a3166557a4bf5aae14d636d02d996c427d/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/dd21a9a3166557a4bf5aae14d636d02d996c427d/5-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/dd21a9a3166557a4bf5aae14d636d02d996c427d/5-TableI-1.png"],"pdf":"https://zenodo.org/record/1273764/files/article.pdf"},{"type":"inproceedings","key":"rozenberszki2021towards","title":"Towards Universal User Interfaces for Mobile Robots","author":"Rozenberszki, D\'avid and S\\"or\\"os, G\'abor","booktitle":"Augmented Humans Conference 2021","pages":"274--276","year":"2021","doi":"10.1145/3458709.3458996","authors":["D\'avid Rozenberszki","G\'abor S\\"or\\"os"],"dblp":"conf/aughuman/RozenberszkiS21","venue":"AHs","abstract":"We demonstrate the concept and a prototype of automatically generated virtual user interfaces for mobile robots. Human(s) and robot(s) are co-localized in the space via their own on-board navigation and shared spatial anchors. The robot sends the description of context-aware user interface elements to a head-mounted display, which renders the virtual widgets around and seemingly attached to the physical robot.","paperId":"3ef7e8fa094d49364d8827b50ba5b9a80b2bf3b3","paperTitle":"Demo: Towards Universal User Interfaces for Mobile Robots","paperUrl":"https://www.semanticscholar.org/paper/3ef7e8fa094d49364d8827b50ba5b9a80b2bf3b3","images":["https://d3i71xaburhd42.cloudfront.net/3ef7e8fa094d49364d8827b50ba5b9a80b2bf3b3/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/3ef7e8fa094d49364d8827b50ba5b9a80b2bf3b3/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/3ef7e8fa094d49364d8827b50ba5b9a80b2bf3b3/2-Figure4-1.png"],"pdf":"https://doi.org/10.1145/3458709.3458996"},{"type":"incollection","key":"park2019deep","title":"Deep-ChildAR Bot: Educational Activities and Safety Care Augmented Reality System with Deep-Learning for Preschool","author":"Park, Yoon Jung and Ro, Hyocheol and Han, Tack-Don","booktitle":"ACM SIGGRAPH 2019 Posters","pages":"1--2","year":"2019","doi":"10.1145/3306214.3338589","authors":["Yoon Jung Park","Hyocheol Ro","Tack-Don Han"],"dblp":"conf/siggraph/ParkRH19","venue":"SIGGRAPH Posters","abstract":"We propose a projection-based augmented reality (AR) robot system that provides pervasive support for the education and safety of preschoolers via a deep learning framework. This system can utilize real-world objects as metaphors for educational tools by performing object detection based on deep learning in real-time, and it can help recognize the dangers of real-world objects that may pose risks to children. We designed the system in a simple and intuitive way to provide user-friendly interfaces and interactions for children. Children\'s experiences through the proposed system can improve their physical, cognitive, emotional, and thinking abilities.","paperId":"a736c58dd49e7bd472b0d6e1502cf237a7ba0fb3","paperTitle":"Deep-ChildAR bot: educational activities and safety care augmented reality system with deep-learning for preschool","paperUrl":"https://www.semanticscholar.org/paper/a736c58dd49e7bd472b0d6e1502cf237a7ba0fb3","images":["https://d3i71xaburhd42.cloudfront.net/a736c58dd49e7bd472b0d6e1502cf237a7ba0fb3/2-Figure2-1.png"],"pdf":"https://doi.org/10.1145/3306214.3338589"},{"type":"inproceedings","key":"cao2018ani","title":"Ani-Bot: A Modular Robotics System Supporting Creation, Tweaking, and Usage with Mixed-Reality Interactions","author":"Cao, Yuanzhi and Xu, Zhuangying and Glenn, Terrell and Huo, Ke and Ramani, Karthik","booktitle":"Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction","pages":"419--428","year":"2018","doi":"10.1145/3173225.3173226","authors":["Yuanzhi Cao","Zhuangying Xu","Terrell Glenn","Ke Huo","Karthik Ramani"],"dblp":"conf/tei/CaoXGHR18","venue":"Tangible and Embedded Interaction","abstract":"Ani-Bot is a modular robotics system that allows users to control their DIY robots using Mixed-Reality Interaction (MRI). This system takes advantage of MRI to enable users to visually program the robot through the augmented view of a Head-Mounted Display (HMD). In this paper, we first explain the design of the Mixed-Reality (MR) ready modular robotics system, which allows users to instantly perform MRI once they finish assembling the robot. Then, we elaborate the augmentations provided by the MR system in the three primary phases of a construction kit\'s lifecycle: Creation, Tweaking, and Usage. Finally, we demonstrate Ani-Bot with four application examples and evaluate the system with a two-session user study. The results of our evaluation indicate that Ani-Bot does successfully embed MRI into the lifecycle (Creation, Tweaking, Usage) of DIY robotics and that it does show strong potential for delivering an enhanced user experience.","paperId":"ed5250ac64e89015c9df0f28a9a315854bf794b6","paperTitle":"Ani-Bot: A Modular Robotics System Supporting Creation, Tweaking, and Usage with Mixed-Reality Interactions","paperUrl":"https://www.semanticscholar.org/paper/ed5250ac64e89015c9df0f28a9a315854bf794b6","images":["https://d3i71xaburhd42.cloudfront.net/ed5250ac64e89015c9df0f28a9a315854bf794b6/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/ed5250ac64e89015c9df0f28a9a315854bf794b6/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/ed5250ac64e89015c9df0f28a9a315854bf794b6/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/ed5250ac64e89015c9df0f28a9a315854bf794b6/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/ed5250ac64e89015c9df0f28a9a315854bf794b6/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/ed5250ac64e89015c9df0f28a9a315854bf794b6/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/ed5250ac64e89015c9df0f28a9a315854bf794b6/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/ed5250ac64e89015c9df0f28a9a315854bf794b6/6-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/ed5250ac64e89015c9df0f28a9a315854bf794b6/6-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/ed5250ac64e89015c9df0f28a9a315854bf794b6/6-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/ed5250ac64e89015c9df0f28a9a315854bf794b6/7-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/ed5250ac64e89015c9df0f28a9a315854bf794b6/7-Figure14-1.png","https://d3i71xaburhd42.cloudfront.net/ed5250ac64e89015c9df0f28a9a315854bf794b6/8-Figure15-1.png"],"pdf":"https://engineering.purdue.edu/cdesign/wp/wp-content/uploads/2018/04/Ani-Bot_TEI2018.pdf"},{"type":"inproceedings","key":"guhl2017concept","title":"Concept and Architecture for Programming Industrial Robots Using Augmented Reality with Mobile Devices like Microsoft HoloLens","author":"Guhl, Jan and Tung, Son and Kruger, J\\"org","booktitle":"2017 22nd IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)","pages":"1--4","year":"2017","organization":"IEEE","doi":"10.1109/ETFA.2017.8247749","authors":["Jan Guhl","Son Tung","J\\"org Kruger"],"dblp":"conf/etfa/GuhlTK17","venue":"2017 22nd IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)","abstract":"This paper proposes a concept for human-robot interaction using techniques of virtual and augmented reality on mobile devices like cell phones and tablets or mixed reality devices like the HoloLens. By combining data received from real robots together with the perception and abilities of a human operator innovative applications are imaginable. Visualizing not only the current robot state but also the robots environment captured with different sensors and processed with both machine and human vision can lead to a rising percentage of robot assisted workplaces or robot installations. Since the visualization of and the interaction with the robotic application is not locally restricted new or improved use cases like remote maintenance or faster startup of industrial robots can be realized. Therefore, an architecture split into three functional units and an overview of our implementation is presented.","paperId":"8fe41a140bb36a96a42d98cc1552e48530a9f4bc","paperTitle":"Concept and architecture for programming industrial robots using augmented reality with mobile devices like microsoft HoloLens","paperUrl":"https://www.semanticscholar.org/paper/8fe41a140bb36a96a42d98cc1552e48530a9f4bc","images":["https://d3i71xaburhd42.cloudfront.net/8fe41a140bb36a96a42d98cc1552e48530a9f4bc/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/8fe41a140bb36a96a42d98cc1552e48530a9f4bc/3-Figure2-1.png"],"pdf":"https://doi.org/10.1109/ETFA.2017.8247749"},{"type":"incollection","key":"subin2017android","title":"Android Based Augmented Reality as a Social Interface for Low Cost Social Robots","author":"Subin, EK and Hameed, Ashik and Sudheer, AP","booktitle":"Proceedings of the Advances in Robotics","pages":"1--4","year":"2017","doi":"10.1145/3132446.3134907","authors":["EK Subin","Ashik Hameed","AP Sudheer"],"dblp":"conf/air/SubinHS17","venue":"AIR \'17","abstract":"Social robots are gradually populating the human space. The utility of such robots is enormous. They can have socially important functions like training for kids with autism and molding the character and behavior of kids. The human-like features of social robots tend to elicit and maintain and enhance positive emotions in a child. The conclusive aim of social robotics is to develop robots that can seamlessly interact with humans. Making them more anthropomorphic is one of the main tasks in designing them. A humanoid robot requires an enormous amount of compactness of all actuators and sensors for expressing anthropomorphic characters. The cost and laboring required to meet these are huge. Also, some of their facial expressions and body movements do not need any physical interaction with the real world. Here comes the need of virtual robots which have the capability of showing a higher level of anthropomorphism. This paper presents a novel method for designing a low-cost android based social robot by replacing the actuators in humanoid robots and implementing virtual avatars instead. The paper contributes a novel integration methodology which combines a mobile robotic base and a virtual character using augmented reality.","paperId":"4e548b3bbe26e9f717b4a7c4902f0f758f9265b1","paperTitle":"Android based augmented reality as a social interface for low cost social robots","paperUrl":"https://www.semanticscholar.org/paper/4e548b3bbe26e9f717b4a7c4902f0f758f9265b1","images":["https://d3i71xaburhd42.cloudfront.net/4e548b3bbe26e9f717b4a7c4902f0f758f9265b1/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/4e548b3bbe26e9f717b4a7c4902f0f758f9265b1/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/4e548b3bbe26e9f717b4a7c4902f0f758f9265b1/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/4e548b3bbe26e9f717b4a7c4902f0f758f9265b1/2-Figure4.1-1.png","https://d3i71xaburhd42.cloudfront.net/4e548b3bbe26e9f717b4a7c4902f0f758f9265b1/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/4e548b3bbe26e9f717b4a7c4902f0f758f9265b1/3-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/4e548b3bbe26e9f717b4a7c4902f0f758f9265b1/3-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/4e548b3bbe26e9f717b4a7c4902f0f758f9265b1/3-Figure8.1-1.png","https://d3i71xaburhd42.cloudfront.net/4e548b3bbe26e9f717b4a7c4902f0f758f9265b1/4-Figure9-1.png"],"pdf":"https://doi.org/10.1145/3132446.3134907"},{"type":"article","key":"chadalavada2020bi","title":"Bi-Directional Navigation Intent Communication Using Spatial Augmented Reality and Eye-Tracking Glasses for Improved Safety in Human--Robot Interaction","author":"Chadalavada, Ravi Teja and Andreasson, Henrik and Schindler, Maike and Palm, Rainer and Lilienthal, Achim J","journal":"Robotics and Computer-Integrated Manufacturing","volume":"61","pages":"101830","year":"2020","publisher":"Elsevier","doi":"10.1016/J.RCIM.2019.101830","authors":["Ravi Teja Chadalavada","Henrik Andreasson","Maike Schindler","Rainer Palm","Achim J Lilienthal"],"dblp":"journals/rcim/ChadalavadaASPL20","venue":"Robotics Comput. Integr. Manuf.","abstract":null,"paperId":"3e2049f92ac52ca1fbbfaa47fd007a9503b1986b","paperTitle":"Bi-directional navigation intent communication using spatial augmented reality and eye-tracking glasses for improved safety in human-robot interaction","paperUrl":"https://www.semanticscholar.org/paper/3e2049f92ac52ca1fbbfaa47fd007a9503b1986b","images":[],"pdf":"https://doi.org/10.1016/j.rcim.2019.101830"},{"type":"inproceedings","key":"fung2011augmented","title":"An Augmented Reality System for Teaching Sequential Tasks to a Household Robot","author":"Fung, Richard and Hashimoto, Sunao and Inami, Masahiko and Igarashi, Takeo","booktitle":"2011 RO-MAN","pages":"282--287","year":"2011","organization":"IEEE","doi":"10.1109/ROMAN.2011.6005235","authors":["Richard Fung","Sunao Hashimoto","Masahiko Inami","Takeo Igarashi"],"dblp":"conf/ro-man/FungHII11","venue":"2011 RO-MAN","abstract":"We present a method of instructing a sequential task to a household robot using a hand-held augmented reality device. The user decomposes a high-level goal such as \u201cprepare a drink\u201d into steps such as delivering a mug under a kettle and pouring hot water into the mug. The user takes a photograph of each step using the device and annotates it with necessary information via touch operation. The resulting sequence of annotated photographs serves as a reference for review and reuse at a later time. We created a working prototype system with various types of robots and appliances.","paperId":"f9282968c804b775269fb9fc05f86da49d4e9609","paperTitle":"An augmented reality system for teaching sequential tasks to a household robot","paperUrl":"https://www.semanticscholar.org/paper/f9282968c804b775269fb9fc05f86da49d4e9609","images":["https://d3i71xaburhd42.cloudfront.net/f9282968c804b775269fb9fc05f86da49d4e9609/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/f9282968c804b775269fb9fc05f86da49d4e9609/5-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/f9282968c804b775269fb9fc05f86da49d4e9609/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/f9282968c804b775269fb9fc05f86da49d4e9609/5-Figure4-1.png"],"pdf":"http://grouplab.cpsc.ucalgary.ca/grouplab/uploads/Publications/Publications/2011-TasksToRobots.Roman.pdf"},{"type":"article","key":"ganesan2018better","title":"Better Teaming through Visual Cues: How Projecting Imagery in a Workspace Can Improve Human-Robot Collaboration","author":"Ganesan, Ramsundar Kalpagam and Rathore, Yash K and Ross, Heather M and Amor, Heni Ben","journal":"IEEE Robotics & Automation Magazine","volume":"25","number":"2","pages":"59--71","year":"2018","publisher":"IEEE","doi":"10.1109/MRA.2018.2815655","authors":["Ramsundar Kalpagam Ganesan","Yash K Rathore","Heather M Ross","Heni Ben Amor"],"dblp":"journals/ram/GanesanRRA18","venue":"IEEE Robotics & Automation Magazine","abstract":"In this article, we present a communication paradigm using a context-aware mixed-reality approach for instructing human workers when collaborating with robots. The main objective is to utilize the physical work environment as a canvas to communicate task-related instructions and robot intentions in the form of visual cues. A vision-based object-tracking algorithm is used to precisely determine the pose and state of physical objects in and around the workspace. A projection-mapping technique is employed to overlay visual cues on the tracked objects and the workspace. Simultaneous tracking and projection onto objects enable the system to provide just-in-time instructions for carrying out a procedural task.","paperId":"a8a395ba8dbf2e5cb0fcd17ef39d890cc4b29ec3","paperTitle":"Better Teaming Through Visual Cues: How Projecting Imagery in a Workspace Can Improve Human-Robot Collaboration","paperUrl":"https://www.semanticscholar.org/paper/a8a395ba8dbf2e5cb0fcd17ef39d890cc4b29ec3","images":["https://d3i71xaburhd42.cloudfront.net/a8a395ba8dbf2e5cb0fcd17ef39d890cc4b29ec3/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/a8a395ba8dbf2e5cb0fcd17ef39d890cc4b29ec3/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/a8a395ba8dbf2e5cb0fcd17ef39d890cc4b29ec3/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/a8a395ba8dbf2e5cb0fcd17ef39d890cc4b29ec3/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/a8a395ba8dbf2e5cb0fcd17ef39d890cc4b29ec3/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/a8a395ba8dbf2e5cb0fcd17ef39d890cc4b29ec3/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/a8a395ba8dbf2e5cb0fcd17ef39d890cc4b29ec3/8-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/a8a395ba8dbf2e5cb0fcd17ef39d890cc4b29ec3/8-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/a8a395ba8dbf2e5cb0fcd17ef39d890cc4b29ec3/9-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/a8a395ba8dbf2e5cb0fcd17ef39d890cc4b29ec3/4-TableI-1.png","https://d3i71xaburhd42.cloudfront.net/a8a395ba8dbf2e5cb0fcd17ef39d890cc4b29ec3/5-TableII-1.png","https://d3i71xaburhd42.cloudfront.net/a8a395ba8dbf2e5cb0fcd17ef39d890cc4b29ec3/8-TableIII-1.png"],"pdf":"https://interactive-robotics.engineering.asu.edu/wp-content/uploads/2019/05/intentionprojection.pdf"},{"type":"inproceedings","key":"hwang2008augmented","title":"Augmented Robot Agent: Enhancing Co-Presence of the Remote Participant","author":"Hwang, Jane and Lee, Sangyup and Ahn, Sang Chul and Kim, Hyoung-gon","booktitle":"2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality","pages":"161--162","year":"2008","organization":"IEEE","doi":"10.1109/ISMAR.2008.4637346","authors":["Jane Hwang","Sangyup Lee","Sang Chul Ahn","Hyoung-gon Kim"],"dblp":"conf/ismar/HwangLAK08","venue":"2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality","abstract":"In this paper, we present a tele-meeting system which uses an augmented robot agent as the representation of the remote participant. In this system, we augment a 3D volume video of the remote participant over the on-site robot. The robot agent in this system represents a remote user with camera, microphone, and mobility. Using this robot agent, the remote user and the local user can show their appearances and interact with tangible interfaces. This robot agent system can be applied to various tele-meeting applications such as tele-marketing and tele-tutoring. For example, we implemented a tele-marketing system to show the feasibility of the suggested system.","paperId":"2174cf05b9b27dc418f1319a8105d35814b0116f","paperTitle":"Augmented robot agent: Enhancing co-presence of the remote participant","paperUrl":"https://www.semanticscholar.org/paper/2174cf05b9b27dc418f1319a8105d35814b0116f","images":["https://d3i71xaburhd42.cloudfront.net/2174cf05b9b27dc418f1319a8105d35814b0116f/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/2174cf05b9b27dc418f1319a8105d35814b0116f/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/2174cf05b9b27dc418f1319a8105d35814b0116f/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/2174cf05b9b27dc418f1319a8105d35814b0116f/2-Figure4-1.png"],"pdf":"https://doi.org/10.1109/ISMAR.2008.4637346"},{"type":"incollection","key":"carroll2013augmented","title":"Augmented Reality Gaming with Sphero","author":"Carroll, Jon and Polo, Fabrizio","booktitle":"ACM Siggraph 2013 Mobile","pages":"1--1","year":"2013","doi":"10.1145/2503512.2503535","authors":["Jon Carroll","Fabrizio Polo"],"dblp":"conf/siggraph/CarrollP13","venue":"SIGGRAPH \'13","abstract":"Sphero is revolutionizing the augmented reality gaming genre by eliminating the need for a static paper marker and replacing it with a fully motive robot. Robotics unlocks endless possibilities for new gameplay. No longer restricted to playing near a stationary marker or the television screen, the player can run around their entire house using a mobile device as a window into an augmented world. Imagine fighting monsters in your bedroom, saving the princess in the kitchen or even taking the virtual experience outside.","paperId":"ba4c9a79f51060435e15ad8a36867361a384cb53","paperTitle":"Augmented reality gaming with sphero","paperUrl":"https://www.semanticscholar.org/paper/ba4c9a79f51060435e15ad8a36867361a384cb53","images":["https://d3i71xaburhd42.cloudfront.net/ba4c9a79f51060435e15ad8a36867361a384cb53/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/ba4c9a79f51060435e15ad8a36867361a384cb53/1-Figure2-1.png"],"pdf":"https://doi.org/10.1145/2503512.2503535"},{"type":"inproceedings","key":"tominaga2014around","title":"Around Me: A System with an Escort Robot Providing a Sports Player\'s Self-Images","author":"Tominaga, Junya and Kawauchi, Kensaku and Rekimoto, Jun","booktitle":"Proceedings of the 5th Augmented Human International Conference","pages":"1--8","year":"2014","doi":"10.1145/2582051.2582094","authors":["Junya Tominaga","Kensaku Kawauchi","Jun Rekimoto"],"dblp":"conf/aughuman/TominagaKR14","venue":"AH","abstract":"Providing self-images is an effective approach to identifying sports players\' body movements that should be corrected. Traditional means providing self-images, however, such as mirrors and videos, are not effective in terms of mobility and immediacy. In this paper we propose a system, Around Me, providing self-images through a display attached to an escort robot that runs in front of the user. This system captures the user\'s posture from the front and recognizes his/her position relative to the robot. The user\'s movements are synchronized with the robot\'s movements because the robot\'s movements are determined by the user\'s location. In this research we developed an experimental prototype specialized for assistance in jogging. In pilot studies we observed that the ability of Around Me to provide real-time images is potentially able to encourage the user to improve his/her jogging form, which is essential for performance and for injury prevention. In addition, compared with the robot running in front of the user with one following behind the user, we clarified the frontal robot\'s characteristics: the robot can control the jogging speed, and the user needs to adjust the robot\'s steering and the distance between the robot and him/her as he/she requires. Then we found indications that Around Me can, with various jogging support functions, encourage the user to practice jogging with ideal form.","paperId":"5ab5431ab8f9e622393d622f141e3e4ee3033ec8","paperTitle":"Around me: a system with an escort robot providing a sports player\'s self-images","paperUrl":"https://www.semanticscholar.org/paper/5ab5431ab8f9e622393d622f141e3e4ee3033ec8","images":["https://d3i71xaburhd42.cloudfront.net/5ab5431ab8f9e622393d622f141e3e4ee3033ec8/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/5ab5431ab8f9e622393d622f141e3e4ee3033ec8/3-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/5ab5431ab8f9e622393d622f141e3e4ee3033ec8/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/5ab5431ab8f9e622393d622f141e3e4ee3033ec8/3-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/5ab5431ab8f9e622393d622f141e3e4ee3033ec8/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/5ab5431ab8f9e622393d622f141e3e4ee3033ec8/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/5ab5431ab8f9e622393d622f141e3e4ee3033ec8/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/5ab5431ab8f9e622393d622f141e3e4ee3033ec8/6-Table6-1.png","https://d3i71xaburhd42.cloudfront.net/5ab5431ab8f9e622393d622f141e3e4ee3033ec8/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/5ab5431ab8f9e622393d622f141e3e4ee3033ec8/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/5ab5431ab8f9e622393d622f141e3e4ee3033ec8/6-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/5ab5431ab8f9e622393d622f141e3e4ee3033ec8/6-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/5ab5431ab8f9e622393d622f141e3e4ee3033ec8/8-Figure11-1.png"],"pdf":"https://doi.org/10.1145/2582051.2582094"},{"type":"inproceedings","key":"jones2021ar","title":"AR/vr Tutorial System for Human-Robot Teaming","author":"Jones, Colin and Novitzky, Michael and Korpela, Christopher","booktitle":"2021 IEEE 11th Annual Computing and Communication Workshop and Conference (CCWC)","pages":"0878--0882","year":"2021","organization":"IEEE","doi":"10.1109/CCWC51732.2021.9375845","authors":["Colin Jones","Michael Novitzky","Christopher Korpela"],"dblp":"conf/ccwc/JonesNK21","venue":"2021 IEEE 11th Annual Computing and Communication Workshop and Conference (CCWC)","abstract":"The role of human-robot teaming (HRT), or the interaction between humans and robots to complete tasks, is becoming increasingly important in the modern age. From medicine to military applications, robots have established themselves as powerful tools in the completion of human-directed objectives. Given the importance of this teaming, it is equally important that there exists a system to develop such partnerships. For this purpose we present a tutorial based on the framework of the Project Aquaticus human-robot teaming test-bed, where participants in previous experiments felt overwhelmed while working with an autonomous robot teammate. To improve and develop this participant-robot relationship, we developed a tutorial for HRT using the Unity game engine. The Project Aquaticus test-bed centers around a game of capture-the-flag played in boats on the water, where each team is equipped with two human-steered boats and two autonomous robot teammates. The tutorial is designed based on the analysis of previous literature and includes features found in other, effective training systems and tutorials. The creation of this automated tutorial system shows promise in improving the effectiveness of HRT in the context of Project Aquaticus. In the near future, the effectiveness of this tutorial system will be tested with human subjects to evaluate improvements to situational awareness and cognitive load.","paperId":"e61a35f7078c03a0e2325774fb88a01f3e08c096","paperTitle":"AR/VR Tutorial System for Human-Robot Teaming","paperUrl":"https://www.semanticscholar.org/paper/e61a35f7078c03a0e2325774fb88a01f3e08c096","images":["https://d3i71xaburhd42.cloudfront.net/e61a35f7078c03a0e2325774fb88a01f3e08c096/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/e61a35f7078c03a0e2325774fb88a01f3e08c096/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/e61a35f7078c03a0e2325774fb88a01f3e08c096/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/e61a35f7078c03a0e2325774fb88a01f3e08c096/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/e61a35f7078c03a0e2325774fb88a01f3e08c096/4-Figure5-1.png"],"pdf":"https://doi.org/10.1109/CCWC51732.2021.9375845"},{"type":"inproceedings","key":"xiao2016andantino","title":"Andantino: Teaching Children Piano with Projected Animated Characters","author":"Xiao, Xiao and Puentes, Pablo and Ackermann, Edith and Ishii, Hiroshi","booktitle":"Proceedings of the the 15th international conference on interaction design and children","pages":"37--45","year":"2016","doi":"10.1145/2930674.2930689","authors":["Xiao Xiao","Pablo Puentes","Edith Ackermann","Hiroshi Ishii"],"dblp":"conf/acmidc/XiaoPAI16","venue":"IDC","abstract":"This paper explores how multi-modal body-syntonic interactive systems may be used to teach children to play the piano beyond the typical focus on reading musical scores and \\"surface correctness\\". Our work draws from Dalcroze Eurhythmics, a method of music pedagogy aimed at instilling an understanding of music rooted in the body. We present a Dalcrozian process of piano learning as a five-step iterative cycle of: listen, internalize, extend, analyze, and improvise. As a case study of how digital technologies may support this process, we present Andantino, a set of extensions of Andante, which projects musical lines as miniature light silhouettes that appear to walk on the keyboard of a player piano. We discuss features of Andantino based on each stage, or step, of the iterative framework and discuss directions for future research, based on two preliminary studies with children between the ages of 7 and 13.","paperId":"9bde89fc7a45b9d68fb92ed1b9a573990d1a5a20","paperTitle":"Andantino: Teaching Children Piano with Projected Animated Characters","paperUrl":"https://www.semanticscholar.org/paper/9bde89fc7a45b9d68fb92ed1b9a573990d1a5a20","images":["https://d3i71xaburhd42.cloudfront.net/9bde89fc7a45b9d68fb92ed1b9a573990d1a5a20/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/9bde89fc7a45b9d68fb92ed1b9a573990d1a5a20/5-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/9bde89fc7a45b9d68fb92ed1b9a573990d1a5a20/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/9bde89fc7a45b9d68fb92ed1b9a573990d1a5a20/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/9bde89fc7a45b9d68fb92ed1b9a573990d1a5a20/6-Figure4-1.png"],"pdf":"https://dspace.mit.edu/bitstream/1721.1/138018.2/4/_Andantino_final.pdf"},{"type":"inproceedings","key":"chae2018pervasive","title":"A Pervasive Assistive Robot System including Projection-Camera Technology for Older Adults","author":"Chae, Seungho and Ro, Hyocheol and Yang, Yoonsik and Han, Tack-Don","booktitle":"Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction","pages":"83--84","year":"2018","doi":"10.1145/3173386.3177007","authors":["Seungho Chae","Hyocheol Ro","Yoonsik Yang","Tack-Don Han"],"dblp":"conf/hri/ChaeRYH18","venue":"HRI","abstract":"Here we present a projection augmented reality (AR) based assistive robot, which we call the Pervasive Assistive Robot System (PARS). The PARS aims to improve the quality of life by of the elderly and less able-bodied. In particular, the proposed system will support dynamic display and monitoring systems, which will be helpful for older adults who have difficulty moving their limbs and who have a weak memory.We attempted to verify the usefulness of the PARS using various scenarios. We expected that PARSs will be used as assistive robots for people who experience physical discomfort in their daily lives.","paperId":"c1fe990c7e3bf59187d70e088f9ea494f40ae85e","paperTitle":"A Pervasive Assistive Robot System Including Projection-Camera Technology for Older Adults","paperUrl":"https://www.semanticscholar.org/paper/c1fe990c7e3bf59187d70e088f9ea494f40ae85e","images":["https://d3i71xaburhd42.cloudfront.net/c1fe990c7e3bf59187d70e088f9ea494f40ae85e/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/c1fe990c7e3bf59187d70e088f9ea494f40ae85e/2-Figure2-1.png"],"pdf":"https://doi.org/10.1145/3173386.3177007"},{"type":"article","key":"everitt20193d","title":"3D Printed Deformable Surfaces for Shape-Changing Displays","author":"Everitt, Aluna and Alexander, Jason","journal":"Frontiers in Robotics and AI","volume":"6","pages":"80","year":"2019","publisher":"Frontiers","doi":"10.3389/frobt.2019.00080","authors":["Aluna Everitt","Jason Alexander"],"dblp":"journals/firai/EverittA19","venue":"Front. Robot. AI","abstract":"We use interlinked 3D printed panels to fabricate deformable surfaces that are specifically designed for shape-changing displays. Our exploration of 3D printed deformable surfaces, as a fabrication technique for shape-changing displays, shows new and diverse forms of shape output, visualizations, and interaction capabilities. This article describes our general design and fabrication approach, the impact of varying surface design parameters, and a demonstration of possible application examples. We conclude by discussing current limitations and future directions for this work.","paperId":"10fb7aa982638c4f9fb53a863608afccd0c470a5","paperTitle":"3D Printed Deformable Surfaces for Shape-Changing Displays","paperUrl":"https://www.semanticscholar.org/paper/10fb7aa982638c4f9fb53a863608afccd0c470a5","images":["https://d3i71xaburhd42.cloudfront.net/10fb7aa982638c4f9fb53a863608afccd0c470a5/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/10fb7aa982638c4f9fb53a863608afccd0c470a5/5-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/10fb7aa982638c4f9fb53a863608afccd0c470a5/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/10fb7aa982638c4f9fb53a863608afccd0c470a5/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/10fb7aa982638c4f9fb53a863608afccd0c470a5/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/10fb7aa982638c4f9fb53a863608afccd0c470a5/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/10fb7aa982638c4f9fb53a863608afccd0c470a5/7-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/10fb7aa982638c4f9fb53a863608afccd0c470a5/7-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/10fb7aa982638c4f9fb53a863608afccd0c470a5/8-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/10fb7aa982638c4f9fb53a863608afccd0c470a5/8-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/10fb7aa982638c4f9fb53a863608afccd0c470a5/10-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/10fb7aa982638c4f9fb53a863608afccd0c470a5/10-Figure12-1.png"],"pdf":"https://www.frontiersin.org/articles/10.3389/frobt.2019.00080/pdf"},{"type":"article","key":"jones2021belonging","title":"Belonging There: VROOM-Ing into the Uncanny Valley of XR Telepresence","author":"Jones, Brennan and Zhang, Yaying and Wong, Priscilla NY and Rintel, Sean","journal":"Proceedings of the ACM on Human-Computer Interaction","volume":"5","number":"CSCW1","pages":"1--31","year":"2021","publisher":"ACM New York, NY, USA","authors":["Brennan Jones","Yaying Zhang","Priscilla NY Wong","Sean Rintel"],"venue":"","abstract":"The world is entering a new normal of hybrid organisations, in which it will be common that some members are co-located and others are remote. Hybridity is rife with asymmetries that affect our sense of belonging in an organisational space. This paper reports a study of an XR Telepresence technology probe to explore how remote workers might present themselves and be perceived as an equal and unique embodied being in a workplace. VROOM (Virtual Robot Overlay for Online Meetings) augments a standard Mobile Robotic Telepresence experience by (1) adding a virtual avatar overlay of the remote person to the local space, viewable through a HoloLens worn by the local user, through which the remote user can gesture and express themselves, and (2) giving the remote user an immersive 360\xb0 view of the local space, captured by a 360\xb0 camera on the robot, which they can view through a VR headset. We ran a study to understand how pairs of participants (one local and one remote) collaborate using VROOM in a search and word-guessing game. Our findings illustrate that there is much potential for a system like VROOM to support dynamic collaborative activities in which embodiment, gesturing, mobility, spatial awareness, and non-verbal expressions are important. However, there are also challenges to be addressed, specifically around proprioception, the mixing of a physical robot body with a virtual human avatar, uncertainties of others\u2019 views and capabilities, fidelity of expressions, and the appearance of the avatar. We conclude with further design suggestions and recommendations for future work.","paperId":"0be6fb9bf348356b35dbfb78232be433a0875bce","paperTitle":"Belonging There: VROOM-ing into the Uncanny Valley of XR Telepresence","paperUrl":"https://www.semanticscholar.org/paper/0be6fb9bf348356b35dbfb78232be433a0875bce","images":["https://d3i71xaburhd42.cloudfront.net/0be6fb9bf348356b35dbfb78232be433a0875bce/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/0be6fb9bf348356b35dbfb78232be433a0875bce/7-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/0be6fb9bf348356b35dbfb78232be433a0875bce/7-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/0be6fb9bf348356b35dbfb78232be433a0875bce/8-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/0be6fb9bf348356b35dbfb78232be433a0875bce/8-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/0be6fb9bf348356b35dbfb78232be433a0875bce/14-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/0be6fb9bf348356b35dbfb78232be433a0875bce/15-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/0be6fb9bf348356b35dbfb78232be433a0875bce/17-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/0be6fb9bf348356b35dbfb78232be433a0875bce/18-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/0be6fb9bf348356b35dbfb78232be433a0875bce/18-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/0be6fb9bf348356b35dbfb78232be433a0875bce/19-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/0be6fb9bf348356b35dbfb78232be433a0875bce/19-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/0be6fb9bf348356b35dbfb78232be433a0875bce/20-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/0be6fb9bf348356b35dbfb78232be433a0875bce/21-Figure14-1.png","https://d3i71xaburhd42.cloudfront.net/0be6fb9bf348356b35dbfb78232be433a0875bce/22-Figure15-1.png"],"pdf":"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/cscw2021-belonging-there-vroom-preprint.pdf"},{"type":"article","key":"young2006mixed","title":"A Mixed Reality Approach to Human-Robot Interaction","author":"Young, James and Sharlin, Ehud","year":"2006","publisher":"University of Calgary","doi":"10.11575/PRISM/30998","authors":["James Young","Ehud Sharlin"],"venue":"","abstract":"This paper offers a mixed reality approach to humanrobot interaction (HRI) which exploits the fact that robots are both digital and physical entities. We use mixed reality (MR) to integrate digital interaction into the physical environment, allowing users to interact with robots\' ideas and thoughts directly within the shared physical interaction space. We also present a taxonomy which we use to organise and classify the various interaction techniques that this environment offers. We demonstrate this environment and taxonomy by detailing two interaction techniques, thought crumbs and bubblegrams, and to evaluate these techniques, we offer the design of an implementation prototype.","paperId":"158c7e837dc118826bf3a390860d4ef8b9412c5e","paperTitle":"A Mixed Reality Approach to Human-Robot Interaction","paperUrl":"https://www.semanticscholar.org/paper/158c7e837dc118826bf3a390860d4ef8b9412c5e","images":["https://d3i71xaburhd42.cloudfront.net/158c7e837dc118826bf3a390860d4ef8b9412c5e/5-Figure2-1.png"],"pdf":"http://dspace.ucalgary.ca/bitstream/1880/45628/2/2006-816-09.pdf"},{"type":"inproceedings","key":"yamada2017isphere","title":"iSphere: Self-Luminous Spherical Drone Display","author":"Yamada, Wataru and Yamada, Kazuhiro and Manabe, Hiroyuki and Ikeda, Daizo","booktitle":"Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology","pages":"635--643","year":"2017","doi":"10.1145/3126594.3126631","authors":["Wataru Yamada","Kazuhiro Yamada","Hiroyuki Manabe","Daizo Ikeda"],"dblp":"conf/uist/YamadaYMI17","venue":"UIST","abstract":"We present iSphere, a flying spherical display that can display high resolution and bright images in all directions from anywhere in 3D space. Our goal is to build a new platform which can physically and directly emerge arbitrary bodies in the real world. iSphere flies by itself using a built-in drone and creates a spherical display by rotating arcuate multi light-emitting diode (LED) tapes around the drone. As a result of the persistence of human vision, we see it as a spherical display flying in the sky. The proposed method yields large display surfaces, high resolution, drone mobility, high visibility and 360\xb0 field of view. Previous approaches fail to match these characteristics, because of problems with aerodynamics and payload. We construct a prototype and validate the proposed method. The unique characteristics and benefits of flying spherical display surfaces are discussed and we describe application scenarios based on iSphere such as guidance, signage and telepresence.","paperId":"37a7ef9cc5f64274a27123ec1955ff2ac2bb5237","paperTitle":"iSphere: Self-Luminous Spherical Drone Display","paperUrl":"https://www.semanticscholar.org/paper/37a7ef9cc5f64274a27123ec1955ff2ac2bb5237","images":["https://d3i71xaburhd42.cloudfront.net/37a7ef9cc5f64274a27123ec1955ff2ac2bb5237/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/37a7ef9cc5f64274a27123ec1955ff2ac2bb5237/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/37a7ef9cc5f64274a27123ec1955ff2ac2bb5237/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/37a7ef9cc5f64274a27123ec1955ff2ac2bb5237/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/37a7ef9cc5f64274a27123ec1955ff2ac2bb5237/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/37a7ef9cc5f64274a27123ec1955ff2ac2bb5237/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/37a7ef9cc5f64274a27123ec1955ff2ac2bb5237/6-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/37a7ef9cc5f64274a27123ec1955ff2ac2bb5237/6-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/37a7ef9cc5f64274a27123ec1955ff2ac2bb5237/7-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/37a7ef9cc5f64274a27123ec1955ff2ac2bb5237/7-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/37a7ef9cc5f64274a27123ec1955ff2ac2bb5237/7-Figure12-1.png"],"pdf":"https://doi.org/10.1145/3126594.3126631"},{"type":"article","key":"elsharkawy2021uwb","title":"A UWB-Driven Self-Actuated Projector Platform for Interactive Augmented Reality Applications","author":"Elsharkawy, Ahmed and Naheem, Khawar and Koo, Dongwoo and Kim, Mun Sang","journal":"Applied Sciences","volume":"11","number":"6","pages":"2871","year":"2021","publisher":"Multidisciplinary Digital Publishing Institute","doi":"10.3390/APP11062871","authors":["Ahmed Elsharkawy","Khawar Naheem","Dongwoo Koo","Mun Sang Kim"],"venue":"","abstract":"With the rapid development of interactive technology, creating systems that allow users to define their interactive envelope freely and provide multi-interactive modalities is important to build up an intuitive interactive space. We present an indoor interactive system where a human can customize and interact through a projected screen utilizing the surrounding surfaces. An ultra-wideband (UWB) wireless sensor network was used to assist human-centered interaction design and navigate the self-actuated projector platform. We developed a UWB-based calibration algorithm to facilitate the interaction with the customized projected screens, where a hand-held input device was designed to perform mid-air interactive functions. Sixteen participants were recruited to evaluate the system performance. A prototype level implementation was tested inside a simulated museum environment, where a self-actuated projector provides interactive explanatory content for the on-display artifacts under the user\u2019s command. Our results depict the applicability to designate the interactive screen efficiently indoors and interact with the augmented content with reasonable accuracy and relatively low workload. Our findings also provide valuable user experience information regarding the design of mobile and projection-based augmented reality systems, with the ability to overcome the limitations of other conventional techniques.","paperId":"9561c89bca7102d922ff9aaec0852f4b9a71c95b","paperTitle":"A UWB-Driven Self-Actuated Projector Platform for Interactive Augmented Reality Applications","paperUrl":"https://www.semanticscholar.org/paper/9561c89bca7102d922ff9aaec0852f4b9a71c95b","images":[],"pdf":"https://www.mdpi.com/2076-3417/11/6/2871/pdf"},{"type":"inproceedings","key":"costa2015augmented","title":"Augmented Reality behind the Wheel-Human Interactive Assistance by Mobile Robots","author":"Costa, Nuno and Arsenio, Artur","booktitle":"2015 6th International Conference on Automation, Robotics and Applications (ICARA)","pages":"63--69","year":"2015","organization":"IEEE","doi":"10.1109/ICARA.2015.7081126","authors":["Nuno Costa","Artur Arsenio"],"dblp":"conf/icara/CostaA15","venue":"2015 6th International Conference on Automation, Robotics and Applications (ICARA)","abstract":"Novel approaches have taken Augmented Reality (AR) beyond traditional body-worn or hand-held displays, leading to the creation of a new branch of AR: Spatial Augmented Reality (SAR) providing additional application areas. SAR is a rapidly emerging field that uses digital projectors to render virtual objects onto 3D objects in the real space. When mounting digital projectors on robots, this collaboration paves the way for unique Human-Robot Interactions (HRI) that otherwise would not be possible. Adding to robots the capability of projecting interactive Augmented Reality content enables new forms of interactions between humans, robots, and virtual objects, enabling new applications. In this work it is investigated the use of SAR techniques on mobile robots for better enabling this to interact in the future with elderly or injured people during rehabilitation, or with children in the pediatric ward of a hospital.","paperId":"c49c380d1dfb76ca18271959a99b326df7d7507b","paperTitle":"Augmented Reality behind the wheel - Human Interactive Assistance by Mobile Robots","paperUrl":"https://www.semanticscholar.org/paper/c49c380d1dfb76ca18271959a99b326df7d7507b","images":["https://d3i71xaburhd42.cloudfront.net/c49c380d1dfb76ca18271959a99b326df7d7507b/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/c49c380d1dfb76ca18271959a99b326df7d7507b/6-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/c49c380d1dfb76ca18271959a99b326df7d7507b/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/c49c380d1dfb76ca18271959a99b326df7d7507b/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/c49c380d1dfb76ca18271959a99b326df7d7507b/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/c49c380d1dfb76ca18271959a99b326df7d7507b/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/c49c380d1dfb76ca18271959a99b326df7d7507b/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/c49c380d1dfb76ca18271959a99b326df7d7507b/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/c49c380d1dfb76ca18271959a99b326df7d7507b/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/c49c380d1dfb76ca18271959a99b326df7d7507b/6-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/c49c380d1dfb76ca18271959a99b326df7d7507b/6-TableI-1.png"],"pdf":"https://doi.org/10.1109/ICARA.2015.7081126"},{"type":"incollection","key":"aoki2005kobito","title":"Kobito: Virtual Brownies","author":"Aoki, Takafumi and Matsushita, Takashi and Iio, Yuichiro and Mitake, Hironori and Toyama, Takashi and Hasegawa, Shoichi and Ayukawa, Rikiya and Ichikawa, Hiroshi and Sato, Makoto and Kuriyama, Takatsugu and others","booktitle":"ACM SIGGRAPH 2005 emerging technologies","pages":"11--es","year":"2005","doi":"10.1145/1187297.1187309","authors":["Takafumi Aoki","Takashi Matsushita","Yuichiro Iio","Hironori Mitake","Takashi Toyama","Shoichi Hasegawa","Rikiya Ayukawa","Hiroshi Ichikawa","Makoto Sato","Takatsugu Kuriyama","undefined others"],"dblp":"conf/siggraph/AokiMIMTHAISKAK05","venue":"SIGGRAPH \'05","abstract":"One common way to create imaginary, virtual creatures is to overlay computer graphics images on real scenes. But this method is not sufficient, because it allows people to only see the imaginary creatures. In Kobito: Virtual Brownies, imaginary creatures interact with the real world. They move real objects, and people interact with them through the real objects. The real objects function as a kind of \u201dhaptic interface.\u201d This technology can be used in the fields of design, amusement, and healthcare because it conveys haptic information in addition to the visual information that is delivered in current artificial life systems.","paperId":"53676fcd4dd835cb8034d1efbc156a7ff29e0d7b","paperTitle":"Kobito: virtual brownies","paperUrl":"https://www.semanticscholar.org/paper/53676fcd4dd835cb8034d1efbc156a7ff29e0d7b","images":["https://d3i71xaburhd42.cloudfront.net/53676fcd4dd835cb8034d1efbc156a7ff29e0d7b/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/53676fcd4dd835cb8034d1efbc156a7ff29e0d7b/1-Figure2-1.png"],"pdf":"https://doi.org/10.1145/1187297.1187309"},{"type":"inproceedings","key":"taher2015exploring","title":"Exploring Interactions with Physically Dynamic Bar Charts","author":"Taher, Faisal and Hardy, John and Karnik, Abhijit and Weichel, Christian and Jansen, Yvonne and Hornbaek, Kasper and Alexander, Jason","booktitle":"Proceedings of the 33rd annual acm conference on human factors in computing systems","pages":"3237--3246","year":"2015","doi":"10.1145/2702123.2702604","authors":["Faisal Taher","John Hardy","Abhijit Karnik","Christian Weichel","Yvonne Jansen","Kasper Hornbaek","Jason Alexander"],"dblp":"conf/chi/TaherHKWJHA15","venue":"CHI","abstract":"Visualizations such as bar charts help users reason about data, but are mostly screen-based, rarely physical, and almost never physical and dynamic. This paper investigates the role of physically dynamic bar charts and evaluates new interactions for exploring and working with datasets rendered in dynamic physical form. To facilitate our exploration we constructed a 10x10 interactive bar chart and designed interactions that supported fundamental visualisation tasks, specifically; annotation, filtering, organization, and navigation. The interactions were evaluated in a user study with 17 participants. Our findings identify the preferred methods of working with the data for each task i.e. directly tapping rows to hide bars, highlight the strengths and limitations of working with physical data, and discuss the challenges of integrating the proposed interactions together into a larger data exploration system. In general, physical interactions were intuitive, informative, and enjoyable, paving the way for new explorations in physical data visualizations.","paperId":"59560c20f01d36a7f383b2220d27cb8356d7eb7b","paperTitle":"Exploring Interactions with Physically Dynamic Bar Charts","paperUrl":"https://www.semanticscholar.org/paper/59560c20f01d36a7f383b2220d27cb8356d7eb7b","images":["https://d3i71xaburhd42.cloudfront.net/59560c20f01d36a7f383b2220d27cb8356d7eb7b/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/59560c20f01d36a7f383b2220d27cb8356d7eb7b/4-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/59560c20f01d36a7f383b2220d27cb8356d7eb7b/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/59560c20f01d36a7f383b2220d27cb8356d7eb7b/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/59560c20f01d36a7f383b2220d27cb8356d7eb7b/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/59560c20f01d36a7f383b2220d27cb8356d7eb7b/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/59560c20f01d36a7f383b2220d27cb8356d7eb7b/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/59560c20f01d36a7f383b2220d27cb8356d7eb7b/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/59560c20f01d36a7f383b2220d27cb8356d7eb7b/7-Figure8-1.png"],"pdf":"http://www.kasperhornbaek.dk/papers/CHI2015_Emerge.pdf"},{"type":"inproceedings","key":"leithinger2011direct","title":"Direct and Gestural Interaction with Relief: A 2.5 D Shape Display","author":"Leithinger, Daniel and Lakatos, David and DeVincenzi, Anthony and Blackshaw, Matthew and Ishii, Hiroshi","booktitle":"Proceedings of the 24th annual ACM symposium on User interface software and technology","pages":"541--548","year":"2011","doi":"10.1145/2047196.2047268","authors":["Daniel Leithinger","David Lakatos","Anthony DeVincenzi","Matthew Blackshaw","Hiroshi Ishii"],"dblp":"conf/uist/LeithingerLDBI11","venue":"UIST","abstract":"Actuated shape output provides novel opportunities for experiencing, creating and manipulating 3D content in the physical world. While various shape displays have been proposed, a common approach utilizes an array of linear actuators to form 2.5D surfaces. Through identifying a set of common interactions for viewing and manipulating content on shape displays, we argue why input modalities beyond direct touch are required. The combination of freehand gestures and direct touch provides additional degrees of freedom and resolves input ambiguities, while keeping the locus of interaction on the shape output. To demonstrate the proposed combination of input modalities and explore applications for 2.5D shape displays, two example scenarios are implemented on a prototype system.","paperId":"832cc86b886cafb3f3a95a80bf925a430c2ecb57","paperTitle":"Direct and gestural interaction with relief: a 2.5D shape display","paperUrl":"https://www.semanticscholar.org/paper/832cc86b886cafb3f3a95a80bf925a430c2ecb57","images":["https://d3i71xaburhd42.cloudfront.net/7e212ae39c1671d00593bbcbaa454d6c447be4bf/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/7e212ae39c1671d00593bbcbaa454d6c447be4bf/2-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/7e212ae39c1671d00593bbcbaa454d6c447be4bf/1-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/7e212ae39c1671d00593bbcbaa454d6c447be4bf/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/7e212ae39c1671d00593bbcbaa454d6c447be4bf/2-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/7e212ae39c1671d00593bbcbaa454d6c447be4bf/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/7e212ae39c1671d00593bbcbaa454d6c447be4bf/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/7e212ae39c1671d00593bbcbaa454d6c447be4bf/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/7e212ae39c1671d00593bbcbaa454d6c447be4bf/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/7e212ae39c1671d00593bbcbaa454d6c447be4bf/5-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/7e212ae39c1671d00593bbcbaa454d6c447be4bf/5-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/7e212ae39c1671d00593bbcbaa454d6c447be4bf/6-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/7e212ae39c1671d00593bbcbaa454d6c447be4bf/6-Figure14-1.png"],"pdf":"https://dspace.mit.edu/bitstream/1721.1/79851/1/Ishii_Direct%20and%20gestural.pdf"},{"type":"inproceedings","key":"nakagaki2016materiable","title":"Materiable: Rendering Dynamic Material Properties in Response to Direct Physical Touch with Shape Changing Interfaces","author":"Nakagaki, Ken and Vink, Luke and Counts, Jared and Windham, Daniel and Leithinger, Daniel and Follmer, Sean and Ishii, Hiroshi","booktitle":"Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems","pages":"2764--2772","year":"2016","doi":"10.1145/2858036.2858104","authors":["Ken Nakagaki","Luke Vink","Jared Counts","Daniel Windham","Daniel Leithinger","Sean Follmer","Hiroshi Ishii"],"dblp":"conf/chi/NakagakiVCWLFI16","venue":"CHI","abstract":"Shape changing interfaces give physical shapes to digital data so that users can feel and manipulate data with their hands and bodies. However, physical objects in our daily life not only have shape but also various material properties. In this paper, we propose an interaction technique to represent material properties using shape changing interfaces. Specifically, by integrating the multi-modal sensation techniques of haptics, our approach builds a perceptive model for the properties of deformable materials in response to direct manipulation. As a proof-of-concept prototype, we developed preliminary physics algorithms running on pin-based shape displays. The system can create computationally variable properties of deformable materials that are visually and physically perceivable. In our experiments, users identify three deformable material properties (flexibility, elasticity and viscosity) through direct touch interaction with the shape display and its dynamic movements. In this paper, we describe interaction techniques, our implementation, future applications and evaluation on how users differentiate between specific properties of our system. Our research shows that shape changing interfaces can go beyond simply displaying shape allowing for rich embodied interaction and perceptions of rendered materials with the hands and body.","paperId":"662e9f54efa42e74b0671c7401d937499b22cd90","paperTitle":"Materiable: Rendering Dynamic Material Properties in Response to Direct Physical Touch with Shape Changing Interfaces","paperUrl":"https://www.semanticscholar.org/paper/662e9f54efa42e74b0671c7401d937499b22cd90","images":["https://d3i71xaburhd42.cloudfront.net/662e9f54efa42e74b0671c7401d937499b22cd90/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/662e9f54efa42e74b0671c7401d937499b22cd90/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/662e9f54efa42e74b0671c7401d937499b22cd90/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/662e9f54efa42e74b0671c7401d937499b22cd90/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/662e9f54efa42e74b0671c7401d937499b22cd90/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/662e9f54efa42e74b0671c7401d937499b22cd90/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/662e9f54efa42e74b0671c7401d937499b22cd90/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/662e9f54efa42e74b0671c7401d937499b22cd90/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/662e9f54efa42e74b0671c7401d937499b22cd90/5-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/662e9f54efa42e74b0671c7401d937499b22cd90/6-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/662e9f54efa42e74b0671c7401d937499b22cd90/6-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/662e9f54efa42e74b0671c7401d937499b22cd90/7-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/662e9f54efa42e74b0671c7401d937499b22cd90/8-Figure13-1.png"],"pdf":"https://dspace.mit.edu/bitstream/1721.1/138020.2/1/PDF%281%29"},{"type":"inproceedings","key":"everitt2017polysurface","title":"PolySurface: A Design Approach for Rapid Prototyping of Shape-Changing Displays Using Semi-Solid Surfaces","author":"Everitt, Aluna and Alexander, Jason","booktitle":"Proceedings of the 2017 Conference on Designing Interactive Systems","pages":"1283--1294","year":"2017","doi":"10.1145/3064663.3064677","authors":["Aluna Everitt","Jason Alexander"],"dblp":"conf/ACMdis/EverittA17","venue":"Conference on Designing Interactive Systems","abstract":"We present a design approach for rapid fabrication of high fidelity interactive shape-changing displays using bespoke semi-solid surfaces. This is achieved by segmenting virtual representations of the given data and mapping it to a dynamic physical polygonal surface. First, we establish the design and fabrication approach for generating semi-solid reconfigurable surfaces. Secondly, we demonstrate the generalizability of this approach by presenting design sessions using datasets provided by experts from a diverse range of domains. Thirdly, we evaluate user engagement with the prototype hardware systems that are built. We learned that all participants, all of whom had no previous interaction with shape-changing displays, were able to successfully design interactive hardware systems that physically represent data specific to their work. Finally, we reflect on the content generated to understand if our approach is effective at representing intended output based on a set of user defined functionality requirements.","paperId":"2dd834ddf8c7f1b0b6de23b381f4ecf4276d3b69","paperTitle":"PolySurface: A Design Approach for Rapid Prototyping of Shape-Changing Displays Using Semi-Solid Surfaces","paperUrl":"https://www.semanticscholar.org/paper/2dd834ddf8c7f1b0b6de23b381f4ecf4276d3b69","images":["https://d3i71xaburhd42.cloudfront.net/2dd834ddf8c7f1b0b6de23b381f4ecf4276d3b69/6-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/2dd834ddf8c7f1b0b6de23b381f4ecf4276d3b69/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/2dd834ddf8c7f1b0b6de23b381f4ecf4276d3b69/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/2dd834ddf8c7f1b0b6de23b381f4ecf4276d3b69/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/2dd834ddf8c7f1b0b6de23b381f4ecf4276d3b69/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/2dd834ddf8c7f1b0b6de23b381f4ecf4276d3b69/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/2dd834ddf8c7f1b0b6de23b381f4ecf4276d3b69/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/2dd834ddf8c7f1b0b6de23b381f4ecf4276d3b69/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/2dd834ddf8c7f1b0b6de23b381f4ecf4276d3b69/6-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/2dd834ddf8c7f1b0b6de23b381f4ecf4276d3b69/7-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/2dd834ddf8c7f1b0b6de23b381f4ecf4276d3b69/7-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/2dd834ddf8c7f1b0b6de23b381f4ecf4276d3b69/8-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/2dd834ddf8c7f1b0b6de23b381f4ecf4276d3b69/8-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/2dd834ddf8c7f1b0b6de23b381f4ecf4276d3b69/9-Figure14-1.png"],"pdf":"https://eprints.lancs.ac.uk/id/eprint/86792/1/DIS17_PolySurface.pdf"},{"type":"inproceedings","key":"roudaut2013morphees","title":"Morphees: Toward High\\" Shape Resolution\\" in Self-Actuated Flexible Mobile Devices","author":"Roudaut, Anne and Karnik, Abhijit and L\\"ochtefeld, Markus and Subramanian, Sriram","booktitle":"Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","pages":"593--602","year":"2013","doi":"10.1145/2470654.2470738","authors":["Anne Roudaut","Abhijit Karnik","Markus L\\"ochtefeld","Sriram Subramanian"],"dblp":"conf/chi/RoudautKLS13","venue":"CHI","abstract":"We introduce the term shape resolution, which adds to the existing definitions of screen and touch resolution. We propose a framework, based on a geometric model (Non-Uniform Rational B-splines), which defines a metric for shape resolution in ten features. We illustrate it by comparing the current related work of shape changing devices. We then propose the concept of Morphees that are self-actuated flexible mobile devices adapting their shapes on their own to the context of use in order to offer better affordances. For instance, when a game is launched, the mobile device morphs into a console-like shape by curling two opposite edges to be better grasped with two hands. We then create preliminary prototypes of Morphees in order to explore six different building strategies using advanced shape changing materials (dielectric electro active polymers and shape memory alloys). By comparing the shape resolution of our prototypes, we generate insights to help designers toward creating high shape resolution Morphees.","paperId":"18e2fee150b35b42dd262704a479e4546cbf901f","paperTitle":"Morphees: toward high \\"shape resolution\\" in self-actuated flexible mobile devices","paperUrl":"https://www.semanticscholar.org/paper/18e2fee150b35b42dd262704a479e4546cbf901f","images":["https://d3i71xaburhd42.cloudfront.net/18e2fee150b35b42dd262704a479e4546cbf901f/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/18e2fee150b35b42dd262704a479e4546cbf901f/3-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/18e2fee150b35b42dd262704a479e4546cbf901f/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/18e2fee150b35b42dd262704a479e4546cbf901f/8-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/18e2fee150b35b42dd262704a479e4546cbf901f/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/18e2fee150b35b42dd262704a479e4546cbf901f/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/18e2fee150b35b42dd262704a479e4546cbf901f/7-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/18e2fee150b35b42dd262704a479e4546cbf901f/7-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/18e2fee150b35b42dd262704a479e4546cbf901f/8-Figure7-1.png"],"pdf":"https://doi.org/10.1145/2470654.2470738"},{"type":"inproceedings","key":"hirai2018xslate","title":"Xslate: A Stiffness-Controlled Surface for Shape-Changing Interfaces","author":"Hirai, Takayuki and Nakamaru, Satoshi and Kawahara, Yoshihiro and Kakehi, Yasuaki","booktitle":"Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems","pages":"1--4","year":"2018","doi":"10.1145/3170427.3186496","authors":["Takayuki Hirai","Satoshi Nakamaru","Yoshihiro Kawahara","Yasuaki Kakehi"],"dblp":"conf/chi/HiraiNKK18","venue":"CHI Extended Abstracts","abstract":"In this paper, we propose a new system called xSlate, a stiffness controlled surface for shape changing interfaces. It is enabled by a deformable frame structure that consists of linear actuators, and an elastic skin surface that can configure its stiffness by pneumatic jamming. We describe the implementation methods of xSlate and how it can be used for future applications.","paperId":"5951b133373a711c8ead32e9bc48533230741a3e","paperTitle":"xSlate: A Stiffness-Controlled Surface for Shape-Changing Interfaces","paperUrl":"https://www.semanticscholar.org/paper/5951b133373a711c8ead32e9bc48533230741a3e","images":[],"pdf":"https://doi.org/10.1145/3170427.3186496"},{"type":"inproceedings","key":"xiao2013mirrorfugue","title":"MirrorFugue Iii: Conjuring the Recorded Pianist.","author":"Xiao, Xiao and Aguilera, Paula and Williams, Jonathan and Ishii, Hiroshi","booktitle":"CHI Extended Abstracts","pages":"2891--2892","year":"2013","organization":"Citeseer","doi":"10.1145/2468356.2479564","authors":["Xiao Xiao","Paula Aguilera","Jonathan Williams","Hiroshi Ishii"],"dblp":"conf/chi/XiaoAWI13","venue":"NIME","abstract":"The body channels rich layers of information when playing music, from intricate manipulations of the instrument to vivid personifications of expression. But when music is captured and replayed across distance and time, the performer\'s body is rarely present. MirrorFugue conjures the recorded performer at the piano by combining the moving keys of a player piano with life-sized projection of the pianist\'s hands and upper body. Inspired by reflections on a lacquered grand piano, MirrorFugue evokes the sense that the virtual pianist is playing the physically moving keys. This video tells two stories of interactions across space and time mediated by MirrorFugue. One presents a great concert broadcasted across the world, where a young pianist learns by playing along. The other depicts a woman who plays a duet with her childhood self.","paperId":"c22c8673a536a3c8aecefdbc3d00ee867a655858","paperTitle":"MirrorFugue iii: conjuring the recorded pianist","paperUrl":"https://www.semanticscholar.org/paper/c22c8673a536a3c8aecefdbc3d00ee867a655858","images":["https://d3i71xaburhd42.cloudfront.net/c22c8673a536a3c8aecefdbc3d00ee867a655858/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/c22c8673a536a3c8aecefdbc3d00ee867a655858/5-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/c22c8673a536a3c8aecefdbc3d00ee867a655858/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/c22c8673a536a3c8aecefdbc3d00ee867a655858/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/c22c8673a536a3c8aecefdbc3d00ee867a655858/2-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/c22c8673a536a3c8aecefdbc3d00ee867a655858/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/c22c8673a536a3c8aecefdbc3d00ee867a655858/4-Figure6-1.png"],"pdf":"http://www.nime.org/2013/program/papers/day1/paper1/28/28_Paper.pdf"},{"type":"inproceedings","key":"ozgur2017cellulo","title":"Cellulo: Versatile Handheld Robots for Education","author":"\\"Ozg\\"ur, Ayberk and Lemaignan, S\'everin and Johal, Wafa and Beltran, Maria and Briod, Manon and Pereyre, L\'ea and Mondada, Francesco and Dillenbourg, Pierre","booktitle":"2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI","pages":"119--127","year":"2017","organization":"IEEE","doi":"10.1145/2909824.3020247","authors":["Ayberk \\"Ozg\\"ur","S\'everin Lemaignan","Wafa Johal","Maria Beltran","Manon Briod","L\'ea Pereyre","Francesco Mondada","Pierre Dillenbourg"],"dblp":"conf/hri/OzgurLJBBPMD17","venue":"2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI","abstract":"In this article, we present Cellulo, a novel robotic platform that investigates the intersection of three ideas for robotics in education: designing the robots to be versatile and generic tools; blending robots into the classroom by designing them to be pervasive objects and by creating tight interactions with (already pervasive) paper; and finally considering the practical constraints of real classrooms at every stage of the design. Our platform results from these considerations and builds on a unique combination of technologies: groups of handheld haptic-enabled robots, tablets and activity sheets printed on regular paper. The robots feature holonomic motion, haptic feedback capability and high accuracy localization through a microdot pattern overlaid on top of the activity sheets, while remaining affordable (robots cost about 125 at the prototype stage) and classroom-friendly. We present the platform and report on our first interaction studies, involving about 230 children.","paperId":"3f642fb8187b0364067e51d3b2a056fb224cad99","paperTitle":"Cellulo: Versatile Handheld Robots for Education","paperUrl":"https://www.semanticscholar.org/paper/3f642fb8187b0364067e51d3b2a056fb224cad99","images":["https://d3i71xaburhd42.cloudfront.net/3f642fb8187b0364067e51d3b2a056fb224cad99/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/3f642fb8187b0364067e51d3b2a056fb224cad99/3-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/3f642fb8187b0364067e51d3b2a056fb224cad99/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/3f642fb8187b0364067e51d3b2a056fb224cad99/6-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/3f642fb8187b0364067e51d3b2a056fb224cad99/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/3f642fb8187b0364067e51d3b2a056fb224cad99/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/3f642fb8187b0364067e51d3b2a056fb224cad99/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/3f642fb8187b0364067e51d3b2a056fb224cad99/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/3f642fb8187b0364067e51d3b2a056fb224cad99/7-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/3f642fb8187b0364067e51d3b2a056fb224cad99/7-Figure8-1.png"],"pdf":"https://infoscience.epfl.ch/record/224129/files/paper.pdf"},{"type":"inproceedings","key":"scheible2013displaydrone","title":"Displaydrone: A Flying Robot Based Interactive Display","author":"Scheible, J\\"urgen and Hoth, Achim and Saal, Julian and Su, Haifeng","booktitle":"Proceedings of the 2nd ACM International Symposium on Pervasive Displays","pages":"49--54","year":"2013","doi":"10.1145/2491568.2491580","authors":["J\\"urgen Scheible","Achim Hoth","Julian Saal","Haifeng Su"],"dblp":"conf/perdis/ScheibleHSS13","venue":"PerDis","abstract":"This paper introduces the Displaydrone, a novel system that combines a multicopter (flying robot) with a video projector and a mobile phone into a flying interactive display for projecting onto walls and arbitrary objects in physical space. Being quickly and flexibly deployed in open space, the Displaydrone allows the angle and direction of the projection to be adjusted on the fly. We realized the Displaydrone system with a custom-built octocopter, an off-the-shelf pico projector, an Android phone and went on flying. By carrying out an experimental evaluation we obtained first impressions on how the Displaydrone is perceived by a viewing audience. The paper highlights the findings and discusses the Displaydrone\'s potential for enabling new kinds of social group interactions in physical space.","paperId":"6293d7622f7aeb2fbd38511a057a73bd0a8d3b31","paperTitle":"Displaydrone: a flying robot based interactive display","paperUrl":"https://www.semanticscholar.org/paper/6293d7622f7aeb2fbd38511a057a73bd0a8d3b31","images":["https://d3i71xaburhd42.cloudfront.net/6293d7622f7aeb2fbd38511a057a73bd0a8d3b31/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/6293d7622f7aeb2fbd38511a057a73bd0a8d3b31/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/6293d7622f7aeb2fbd38511a057a73bd0a8d3b31/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/6293d7622f7aeb2fbd38511a057a73bd0a8d3b31/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/6293d7622f7aeb2fbd38511a057a73bd0a8d3b31/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/6293d7622f7aeb2fbd38511a057a73bd0a8d3b31/5-Figure6-1.png"],"pdf":"http://www.mobilenin.com/displaydrone/materials/Displaydrone_scheible_hdm.pdf"},{"type":"inproceedings","key":"wistort2011tofudraw","title":"TofuDraw: A Mixed-Reality Choreography Tool for Authoring Robot Character Performance","author":"Wistort, Ryan and Breazeal, Cynthia","booktitle":"Proceedings of the 10th International Conference on Interaction Design and Children","pages":"213--216","year":"2011","doi":"10.1145/1999030.1999064","authors":["Ryan Wistort","Cynthia Breazeal"],"dblp":"conf/acmidc/WistortB11","venue":"IDC","abstract":"TofuDraw combines an expressive semi-autonomous robot character (called Tofu) with a new mixed reality DigitalPaint interface whereby children can draw a \\"program\\" on the floor that governs the robot character\'s behavior. Initial evaluations of the TofuDraw system with children ages 3--8 suggest that children can successfully use this interface to choreograph the expressive robot\'s behavior. Our ultimate goal for this tool is to enable young children to engage in STEM learning experiences in new contexts such as creating interactive robot theatre performances.","paperId":"b32dfa9b05f9bd545c1b1c96ca9aa7e6fc602333","paperTitle":"TofuDraw: a mixed-reality choreography tool for authoring robot character performance","paperUrl":"https://www.semanticscholar.org/paper/b32dfa9b05f9bd545c1b1c96ca9aa7e6fc602333","images":[],"pdf":"https://doi.org/10.1145/1999030.1999064"},{"type":"inproceedings","key":"lingamaneni2017dronecast","title":"DroneCAST: Towards a Programming Toolkit for Airborne Multimedia Display Applications","author":"Lingamaneni, Ragavendra and Kubitza, Thomas and Scheible, J\\"urgen","booktitle":"Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services","pages":"1--8","year":"2017","doi":"10.1145/3098279.3122128","authors":["Ragavendra Lingamaneni","Thomas Kubitza","J\\"urgen Scheible"],"dblp":"conf/mhci/LingamaneniKS17","venue":"MobileHCI","abstract":"In recent years, new type of public displays have captured the interest of researchers-displays with the ability to move freely in three-dimensional space. We refer to such display systems as Airborne Multimedia Display (AMD) systems. In this paper, we provide a comprehensive analysis of requirements for developing interactive AMD applications based on extensive literature survey of the related work and from our own experience of flying AMD systems. We then outline the design and implementation of DroneCAST: a programming toolkit for developing AMD applications with remote delivery and control mechanism for multimedia content with IoT middleware as the core part of the system. Finally, we build a sample AMD application with the toolkit to further illustrate the applicability of the system.","paperId":"35759e60b6d4ede9837be2dd91c454a7c2dda4ba","paperTitle":"DroneCAST: towards a programming toolkit for airborne multimedia display applications","paperUrl":"https://www.semanticscholar.org/paper/35759e60b6d4ede9837be2dd91c454a7c2dda4ba","images":["https://d3i71xaburhd42.cloudfront.net/35759e60b6d4ede9837be2dd91c454a7c2dda4ba/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/35759e60b6d4ede9837be2dd91c454a7c2dda4ba/5-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/35759e60b6d4ede9837be2dd91c454a7c2dda4ba/6-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/35759e60b6d4ede9837be2dd91c454a7c2dda4ba/7-Figure4-1.png"],"pdf":"https://doi.org/10.1145/3098279.3122128"},{"type":"inproceedings","key":"suzuki2016gushed","title":"Gushed Diffusers: Fast-Moving, Floating, and Lightweight Midair Display","author":"Suzuki, Ippei and Yoshimitsu, Shuntarou and Kawahara, Keisuke and Ito, Nobutaka and Shinoda, Atushi and Ishii, Akira and Yoshida, Takatoshi and Ochiai, Yoichi","booktitle":"Proceedings of the 29th Annual Symposium on User Interface Software and Technology","pages":"69--70","year":"2016","doi":"10.1145/2984751.2985706","authors":["Ippei Suzuki","Shuntarou Yoshimitsu","Keisuke Kawahara","Nobutaka Ito","Atushi Shinoda","Akira Ishii","Takatoshi Yoshida","Yoichi Ochiai"],"dblp":"conf/uist/SuzukiYKISIYO16","venue":"UIST","abstract":"We present a novel method for fast-moving aerial imaging using aerosol-based fog screens. Conventional systems of aerial imaging cannot move fast because they need large and heavy setup. In this study, we propose to add new tradeoffs between limited display time and payloads. This system employ aerosol distribution from off-the-shelf spray as a fog screen that can resist the wind, and have high portability. As application examples, we present wearable application and aerial imaging on objects with high speed movements such as a drone, a radio-controlled model car, and performers. We believe that our study contribute to the exploration of new application areas for fog displays and expand expressions of entertainments and interactivity.","paperId":"8beb797b1ef771dcb4f23488cc69cf5b56fd2548","paperTitle":"Gushed Diffusers: Fast-moving, Floating, and Lightweight Midair Display","paperUrl":"https://www.semanticscholar.org/paper/8beb797b1ef771dcb4f23488cc69cf5b56fd2548","images":["https://d3i71xaburhd42.cloudfront.net/8beb797b1ef771dcb4f23488cc69cf5b56fd2548/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/8beb797b1ef771dcb4f23488cc69cf5b56fd2548/2-Figure3-1.png"],"pdf":"https://akira.io/papers/glf_demo_uist2016.pdf"},{"type":"inproceedings","key":"lee2018physical","title":"The Physical-Virtual Table: Exploring the Effects of a Virtual Human\'s Physical Influence on Social Interaction","author":"Lee, Myungho and Norouzi, Nahal and Bruder, Gerd and Wisniewski, Pamela J and Welch, Gregory F","booktitle":"Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology","pages":"1--11","year":"2018","doi":"10.1145/3281505.3281533","authors":["Myungho Lee","Nahal Norouzi","Gerd Bruder","Pamela J Wisniewski","Gregory F Welch"],"dblp":"conf/vrst/LeeNBWW18","venue":"VRST","abstract":"In this paper, we investigate the effects of the physical influence of a virtual human (VH) in the context of face-to-face interaction in augmented reality (AR). In our study, participants played a tabletop game with a VH, in which each player takes a turn and moves their own token along the designated spots on the shared table. We compared two conditions as follows: the VH in the virtual condition moves a virtual token that can only be seen through AR glasses, while the VH in the physical condition moves a physical token as the participants do; therefore the VH\'s token can be seen even in the periphery of the AR glasses. For the physical condition, we designed an actuator system underneath the table. The actuator moves a magnet under the table which then moves the VH\'s physical token over the surface of the table. Our results indicate that participants felt higher co-presence with the VH in the physical condition, and participants assessed the VH as a more physical entity compared to the VH in the virtual condition. We further observed transference effects when participants attributed the VH\'s ability to move physical objects to other elements in the real world. Also, the VH\'s physical influence improved participants\' overall experience with the VH. We discuss potential explanations for the findings and implications for future shared AR tabletop setups.","paperId":"f19ade7bf540a231393982801784cbce06b9254b","paperTitle":"The physical-virtual table: exploring the effects of a virtual human\'s physical influence on social interaction","paperUrl":"https://www.semanticscholar.org/paper/f19ade7bf540a231393982801784cbce06b9254b","images":["https://d3i71xaburhd42.cloudfront.net/f19ade7bf540a231393982801784cbce06b9254b/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/f19ade7bf540a231393982801784cbce06b9254b/6-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/f19ade7bf540a231393982801784cbce06b9254b/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/f19ade7bf540a231393982801784cbce06b9254b/9-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/f19ade7bf540a231393982801784cbce06b9254b/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/f19ade7bf540a231393982801784cbce06b9254b/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/f19ade7bf540a231393982801784cbce06b9254b/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/f19ade7bf540a231393982801784cbce06b9254b/7-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/f19ade7bf540a231393982801784cbce06b9254b/7-Figure7-1.png"],"pdf":"https://sreal.ucf.edu/wp-content/uploads/2018/11/Lee2018ab.pdf"},{"type":"incollection","key":"tobita2011floating","title":"Floating Avatar: Blimp-Based Telepresence System for Communication and Entertainment","author":"Tobita, Hiroaki and Maruyama, Shigeaki and Kuji, Takuya","booktitle":"ACM SIGGRAPH 2011 Emerging Technologies","pages":"1--1","year":"2011","doi":"10.1145/2048259.2048263","authors":["Hiroaki Tobita","Shigeaki Maruyama","Takuya Kuji"],"dblp":"conf/siggraph/TobitaMK11","venue":"SIGGRAPH \'11","abstract":"We have created a unique telepresence system for communications and entertainment whereby users can share visual and sound information and express their feelings and impressions more directly than when using conventional meeting systems. We mainly focused on two features to create a unique avatar---its presence in the real world and its ability to interact with people---and created a system based on a blimp. Blimps are physical, not virtual, so they can be used as avatars in the real world. We installed a projector as the output function inside the blimp so that our system can work as a display and express the user\'s attributes. A camera and microphone mounted on the outside of the blimp provide the input function, which means the user can control the blimp from a distance through the network. The proposed system makes unique network communications between floating avatars and humans a legitimate possibility.","paperId":"0252c68dd1fb65df32eddd970d7507c4061fc832","paperTitle":"Floating avatar: blimp-based telepresence system for communication and entertainment","paperUrl":"https://www.semanticscholar.org/paper/0252c68dd1fb65df32eddd970d7507c4061fc832","images":["https://d3i71xaburhd42.cloudfront.net/0252c68dd1fb65df32eddd970d7507c4061fc832/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/0252c68dd1fb65df32eddd970d7507c4061fc832/1-Figure2-1.png"],"pdf":"https://doi.org/10.1145/2048259.2048263"},{"type":"inproceedings","key":"ishii2009designing","title":"Designing Laser Gesture Interface for Robot Control","author":"Ishii, Kentaro and Zhao, Shengdong and Inami, Masahiko and Igarashi, Takeo and Imai, Michita","booktitle":"IFIP Conference on Human-Computer Interaction","pages":"479--492","year":"2009","organization":"Springer","doi":"10.1007/978-3-642-03658-3_52","authors":["Kentaro Ishii","Shengdong Zhao","Masahiko Inami","Takeo Igarashi","Michita Imai"],"dblp":"conf/interact/IshiiZIII09","venue":"INTERACT","abstract":"A laser pointer can be a powerful tool for robot control. However, in the past, their use in the field of robotics has been limited to simple target designation, without exploring their potential as versatile input devices. This paper proposes to create a laser pointer-based user interface for giving various instructions to a robot by applying stroke gesture recognition to the laser\'s trajectory. Through this interface, the user can draw stroke gestures using a laser pointer to specify target objects and commands for the robot to execute accordingly. This system, which includes lasso and dwelling gestures for object selection, stroke gestures for robot operation, and push-button commands for movement cancellation, has been refined from its prototype form through several user-study evaluations. Our results suggest that laser pointers can be effective not only for target designation but also for specifying command and target location for a robot to perform.","paperId":"5b8d52be9afe5cb2fd7058a9b45619498591ce4e","paperTitle":"Designing Laser Gesture Interface for Robot Control","paperUrl":"https://www.semanticscholar.org/paper/5b8d52be9afe5cb2fd7058a9b45619498591ce4e","images":["https://d3i71xaburhd42.cloudfront.net/5b8d52be9afe5cb2fd7058a9b45619498591ce4e/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/5b8d52be9afe5cb2fd7058a9b45619498591ce4e/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/5b8d52be9afe5cb2fd7058a9b45619498591ce4e/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/5b8d52be9afe5cb2fd7058a9b45619498591ce4e/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/5b8d52be9afe5cb2fd7058a9b45619498591ce4e/7-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/5b8d52be9afe5cb2fd7058a9b45619498591ce4e/7-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/5b8d52be9afe5cb2fd7058a9b45619498591ce4e/8-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/5b8d52be9afe5cb2fd7058a9b45619498591ce4e/11-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/5b8d52be9afe5cb2fd7058a9b45619498591ce4e/12-Figure13-1.png"],"pdf":"https://link.springer.com/content/pdf/10.1007%2F978-3-642-03658-3_52.pdf"},{"type":"inproceedings","key":"kojima2006augmented","title":"Augmented Coliseum: An Augmented Game Environment with Small Vehicles","author":"Kojima, Minoru and Sugimoto, Maki and Nakamura, Akihiro and Tomita, Masahiro and Nii, Hideaki and Inami, Masahiko","booktitle":"First IEEE International Workshop on Horizontal Interactive Human-Computer Systems (TABLETOP\'06)","pages":"6--pp","year":"2006","organization":"IEEE","doi":"10.1109/TABLETOP.2006.3","authors":["Minoru Kojima","Maki Sugimoto","Akihiro Nakamura","Masahiro Tomita","Hideaki Nii","Masahiko Inami"],"dblp":"conf/tabletop/KojimaSNTIN06","venue":"First IEEE International Workshop on Horizontal Interactive Human-Computer Systems (TABLETOP \'06)","abstract":"We propose a novel display based game environment using augmented reality technology with small robots. In this environment, the small robots can be augmented by a display image according to their positions and postures. The augmentation activity reinforces the fun of playing with such small robots in the real world.","paperId":"cc1bcfc02fd38f3e65ddaba71dc3caa15839b176","paperTitle":"Augmented coliseum: an augmented game environment with small vehicles","paperUrl":"https://www.semanticscholar.org/paper/cc1bcfc02fd38f3e65ddaba71dc3caa15839b176","images":["https://d3i71xaburhd42.cloudfront.net/cc1bcfc02fd38f3e65ddaba71dc3caa15839b176/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/cc1bcfc02fd38f3e65ddaba71dc3caa15839b176/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/cc1bcfc02fd38f3e65ddaba71dc3caa15839b176/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/cc1bcfc02fd38f3e65ddaba71dc3caa15839b176/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/cc1bcfc02fd38f3e65ddaba71dc3caa15839b176/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/cc1bcfc02fd38f3e65ddaba71dc3caa15839b176/3-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/cc1bcfc02fd38f3e65ddaba71dc3caa15839b176/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/cc1bcfc02fd38f3e65ddaba71dc3caa15839b176/4-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/cc1bcfc02fd38f3e65ddaba71dc3caa15839b176/5-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/cc1bcfc02fd38f3e65ddaba71dc3caa15839b176/5-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/cc1bcfc02fd38f3e65ddaba71dc3caa15839b176/5-Figure11-1.png"],"pdf":"http://www-ui.is.s.u-tokyo.ac.jp/~takeo/course/2007/media/papers/augmented_coliseum.pdf"},{"type":"inproceedings","key":"arevalo2020there","title":"There\'s More than Meets the Eye: Enhancing Robot Control through Augmented Visual Cues","author":"Ar\'evalo Arboleda, Stephanie and Dierks, Tim and R\\"ucker, Franziska and Gerken, Jens","booktitle":"Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction","pages":"104--106","year":"2020","doi":"10.1145/3371382.3378240","authors":["Stephanie Ar\'evalo Arboleda","Tim Dierks","Franziska R\\"ucker","Jens Gerken"],"dblp":"conf/hri/ArboledaDRG20","venue":"HRI","abstract":"In this paper, we present the design of a visual feedback mechanism using Augmented Reality, which we call augmented visual cues, to assist pick-and-place tasks during robot control. We propose to augment the robot operator\'s visual space in order to avoid attention splitting and increase situational awareness (SA). In particular, we aim to improve on the SA concepts of perception, comprehension, and projection as well as the overall task performance. For that, we built upon the interaction design paradigm proposed by Walker et al.. On the one hand, our design augments the robot to support picking-tasks; and, on the other hand, we augment the environment to support placing-tasks. We evaluated our design in a first user study, and results point to specific design aspects that need improvement while showing promise for the overall approach, in particular regarding user satisfaction and certain SA concepts.","paperId":"6ec2d8c22f989485b661668dcbc55ca729d17a20","paperTitle":"There\'s More than Meets the Eye: Enhancing Robot Control through Augmented Visual Cues","paperUrl":"https://www.semanticscholar.org/paper/6ec2d8c22f989485b661668dcbc55ca729d17a20","images":[],"pdf":"https://doi.org/10.1145/3371382.3378240"},{"type":"article","key":"frank2017mobile","title":"Mobile Mixed-Reality Interfaces That Enhance Human--Robot Interaction in Shared Spaces","author":"Frank, Jared A and Moorhead, Matthew and Kapila, Vikram","journal":"Frontiers in Robotics and AI","volume":"4","pages":"20","year":"2017","publisher":"Frontiers","doi":"10.3389/frobt.2017.00020","authors":["Jared A Frank","Matthew Moorhead","Vikram Kapila"],"dblp":"journals/firai/FrankMK17","venue":"Front. Robot. AI","abstract":"Although user interfaces with gesture-based input and augmented graphics have promoted intuitive human-robot interactions (HRI), they are often implemented in remote applications on research-grade platforms requiring significant training and limiting operator mobility. This paper proposes a mobile mixed-reality interface approach to enhance HRI in shared spaces. As a user points a mobile device at the robot\'s workspace, a mixed-reality environment is rendered providing a common frame of reference for the user and robot to effectively communicate spatial information for performing object manipulation tasks, improving the user\'s situational awareness while interacting with augmented graphics to intuitively command the robot. An evaluation with participants is conducted to examine task performance and user experience associated with the proposed interface strategy in comparison to conventional approaches that utilize egocentric or exocentric views from cameras mounted on the robot or in the environment, respectively. Results indicate that, despite the suitability of the conventional approaches in remote applications, the proposed interface approach provides comparable task performance and user experiences in shared spaces without the need to install operator stations or vision systems on or around the robot. Moreover, the proposed interface approach provides users the flexibility to direct robots from their own visual perspective (at the expense of some physical workload) and leverages the sensing capabilities of the tablet to expand the robot\'s perceptual range.","paperId":"feb17c5032c4c9c240ca03353959549d070fc61f","paperTitle":"Mobile Mixed-Reality Interfaces That Enhance Human\u2013Robot Interaction in Shared Spaces","paperUrl":"https://www.semanticscholar.org/paper/feb17c5032c4c9c240ca03353959549d070fc61f","images":["https://d3i71xaburhd42.cloudfront.net/feb17c5032c4c9c240ca03353959549d070fc61f/8-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/feb17c5032c4c9c240ca03353959549d070fc61f/10-Figure12-1.png"],"pdf":"https://www.frontiersin.org/articles/10.3389/frobt.2017.00020/pdf"},{"type":"inproceedings","key":"dinh2017augmented","title":"Augmented Reality Interface for Taping Robot","author":"Dinh, Huy and Yuan, Quilong and Vietcheslav, Iastrebov and Seet, Gerald","booktitle":"2017 18th International Conference on Advanced Robotics (ICAR)","pages":"275--280","year":"2017","organization":"IEEE","doi":"10.1109/ICAR.2017.8023530","authors":["Huy Dinh","Quilong Yuan","Iastrebov Vietcheslav","Gerald Seet"],"dblp":"conf/icar/DinhYIS17","venue":"2017 18th International Conference on Advanced Robotics (ICAR)","abstract":"Applying masking tape to a particular area is a very important step for protecting an uninvolved surface in processes like mechanical part repairing or surface protection. In the past, the task was very time-consuming and required a lot of manual works. In recent years, with some advances in the fields of automatic robotic system and computer vision, the task now can be completed with the help of an automatic taping system containing a 3D scanner, a manipulator and a rotating platform. This implementation has been proved to provide better quality and be at least twice as fast as comparing to the work done by a human operator. However, there are still some limitations of this setup. First, it is difficult for the user to monitor the taping process since the system uses the 3D scanner to reconstruct the surface model and there is no calibrated projector to overlay the manipulator\'s trajectory over the real surface. Second, the main user is supposed to use a computer with keyboard and mouse to identify the area for masking which requires some expert knowledge and might not be appropriate in an industrial context where people wear protective equipment such as gloves or helmet. This paper introduces the use of spatial augmented reality technology and wearable device in the semi-automatic taping robotic system and the related calibration algorithms to enhance the user experience. The framework and its components are presented, with a case study and some results.","paperId":"aee519a37597dac6953ca8fb776dbc9b0f0f0b91","paperTitle":"Augmented reality interface for taping robot","paperUrl":"https://www.semanticscholar.org/paper/aee519a37597dac6953ca8fb776dbc9b0f0f0b91","images":["https://d3i71xaburhd42.cloudfront.net/aee519a37597dac6953ca8fb776dbc9b0f0f0b91/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/aee519a37597dac6953ca8fb776dbc9b0f0f0b91/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/aee519a37597dac6953ca8fb776dbc9b0f0f0b91/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/aee519a37597dac6953ca8fb776dbc9b0f0f0b91/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/aee519a37597dac6953ca8fb776dbc9b0f0f0b91/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/aee519a37597dac6953ca8fb776dbc9b0f0f0b91/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/aee519a37597dac6953ca8fb776dbc9b0f0f0b91/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/aee519a37597dac6953ca8fb776dbc9b0f0f0b91/5-Figure8-1.png"],"pdf":"https://dr.ntu.edu.sg/bitstream/10356/138181/2/%2bICAR2017Official.pdf"},{"type":"inproceedings","key":"fuste2020kinetic","title":"Kinetic AR: A Framework for Robotic Motion Systems in Spatial Computing","author":"Fuste, Anna and Reynolds, Ben and Hobin, James and Heun, Valentin","booktitle":"Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems","pages":"1--8","year":"2020","doi":"10.1145/3334480.3382814","authors":["Anna Fuste","Ben Reynolds","James Hobin","Valentin Heun"],"dblp":"conf/chi/FusteRHH20","venue":"CHI Extended Abstracts","abstract":"We present Kinetic AR, a holistic user experience framework for visual programming of robotic motion systems in Augmented Reality. The Kinetic AR framework facilitates human-robot collaboration in a co-located environment. Our goal is to present a deployable guide for the creation and visualization of manifold robotic interfaces while maintaining a low entry barrier to complex spatial hardware programming. A two phase validation process has been conducted to assess our work. As an initial phase, we have performed a set of interviews with robotics experts. Based on these interviews we have established three main areas that our framework tackles in different time domains. In a second phase, we have developed a set of prototypes using mobile Augmented Reality that apply the principles of Kinetic AR to multiple hardware actors including an AGV, a robotic arm, and a prosthetic system. Additional feedback from experts indicate the potential of the Kinetic ARframework.","paperId":"d5c1c7370e01c96f30fb165609a386b1c117a548","paperTitle":"Kinetic AR: A Framework for Robotic Motion Systems in Spatial Computing","paperUrl":"https://www.semanticscholar.org/paper/d5c1c7370e01c96f30fb165609a386b1c117a548","images":["https://d3i71xaburhd42.cloudfront.net/d5c1c7370e01c96f30fb165609a386b1c117a548/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/d5c1c7370e01c96f30fb165609a386b1c117a548/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/d5c1c7370e01c96f30fb165609a386b1c117a548/6-Figure5-1.png"],"pdf":"https://doi.org/10.1145/3334480.3382814"},{"type":"inproceedings","key":"villanueva2021robotar","title":"RobotAR: An Augmented Reality Compatible Teleconsulting Robotics Toolkit for Augmented Makerspace Experiences","author":"Villanueva, Ana M and Liu, Ziyi and Zhu, Zhengzhe and Du, Xin and Huang, Joey and Peppler, Kylie A and Ramani, Karthik","booktitle":"Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems","pages":"1--13","year":"2021","doi":"10.1145/3411764.3445726","authors":["Ana M Villanueva","Ziyi Liu","Zhengzhe Zhu","Xin Du","Joey Huang","Kylie A Peppler","Karthik Ramani"],"dblp":"conf/chi/VillanuevaLZDHP21","venue":"CHI","abstract":"Distance learning is facing a critical moment finding a balance between high quality education for remote students and engaging them in hands-on learning. This is particularly relevant for project-based classrooms and makerspaces, which typically require extensive trouble-shooting and example demonstrations from instructors. We present RobotAR, a teleconsulting robotics toolkit for creating Augmented Reality (AR) makerspaces. We present the hardware and software for an AR-compatible robot, which behaves as a student\u2019s voice assistant and can be embodied by the instructor for teleconsultation. As a desktop-based teleconsulting agent, the instructor has control of the robot\u2019s joints and position to better focus on areas of interest inside the workspace. Similarly, the instructor has access to the student\u2019s virtual environment and the capability to create AR content to aid the student with problem-solving. We also performed a user study which compares current techniques for distance hands-on learning and an implementation of our toolkit.","paperId":"c9e9210fe4e26c9cb6cbd3dd705dbfe871c64f8c","paperTitle":"RobotAR: An Augmented Reality Compatible Teleconsulting Robotics Toolkit for Augmented Makerspace Experiences","paperUrl":"https://www.semanticscholar.org/paper/c9e9210fe4e26c9cb6cbd3dd705dbfe871c64f8c","images":["https://d3i71xaburhd42.cloudfront.net/c9e9210fe4e26c9cb6cbd3dd705dbfe871c64f8c/9-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/c9e9210fe4e26c9cb6cbd3dd705dbfe871c64f8c/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/c9e9210fe4e26c9cb6cbd3dd705dbfe871c64f8c/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/c9e9210fe4e26c9cb6cbd3dd705dbfe871c64f8c/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/c9e9210fe4e26c9cb6cbd3dd705dbfe871c64f8c/7-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/c9e9210fe4e26c9cb6cbd3dd705dbfe871c64f8c/8-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/c9e9210fe4e26c9cb6cbd3dd705dbfe871c64f8c/9-Figure7-1.png"],"pdf":"https://engineering.purdue.edu/cdesign/wp/wp-content/uploads/2021/06/RobotAR.pdf"},{"type":"inproceedings","key":"stadler2016augmented","title":"Augmented Reality for Industrial Robot Programmers: Workload Analysis for Task-Based, Augmented Reality-Supported Robot Control","author":"Stadler, Susanne and Kain, Kevin and Giuliani, Manuel and Mirnig, Nicole and Stollnberger, Gerald and Tscheligi, Manfred","booktitle":"2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)","pages":"179--184","year":"2016","organization":"IEEE","doi":"10.1109/ROMAN.2016.7745108","authors":["Susanne Stadler","Kevin Kain","Manuel Giuliani","Nicole Mirnig","Gerald Stollnberger","Manfred Tscheligi"],"dblp":"conf/ro-man/StadlerKGMST16","venue":"2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)","abstract":"Augmented reality (AR) can serve as a tool to provide helpful information in a direct way to industrial robot programmers throughout the teaching process. It seems obvious that AR support eases the programming process and increases the programmer\'s productivity and programming accuracy. However, additional information can also potentially increase the programmer\'s perceived workload. To explore the impact of augmented reality on robot teaching, as a first step we have chosen a Sphero robot control scenario and conducted a within-subject user study with 19 professional industrial robot programmers, including novices and experts. We focused on the perceived workload of industrial robot programmers and their task completion time when using a tablet-based AR approach with visualization of task-based information for controlling a robot. Each participant had to execute three typical robot programming tasks: tool center point teaching, trajectory teaching, and overlap teaching. We measured the programmers\' workload in the dimensions of mental demand, physical demand, temporal demand, frustration, effort, and performance. The study results show that the presentation of task-based information in the tablet-based AR interface decreases the mental demand of the industrial robot programmers during the robot control process. At the same time, however, the programmers\' task completion time increases.","paperId":"7f58b3408428c456b1631d16e9334241cf2a5f05","paperTitle":"Augmented reality for industrial robot programmers: Workload analysis for task-based, augmented reality-supported robot control","paperUrl":"https://www.semanticscholar.org/paper/7f58b3408428c456b1631d16e9334241cf2a5f05","images":["https://d3i71xaburhd42.cloudfront.net/7f58b3408428c456b1631d16e9334241cf2a5f05/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/7f58b3408428c456b1631d16e9334241cf2a5f05/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/7f58b3408428c456b1631d16e9334241cf2a5f05/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/7f58b3408428c456b1631d16e9334241cf2a5f05/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/7f58b3408428c456b1631d16e9334241cf2a5f05/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/7f58b3408428c456b1631d16e9334241cf2a5f05/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/7f58b3408428c456b1631d16e9334241cf2a5f05/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/7f58b3408428c456b1631d16e9334241cf2a5f05/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/7f58b3408428c456b1631d16e9334241cf2a5f05/2-TableI-1.png"],"pdf":"https://uwe-repository.worktribe.com/preview/906024/31024.pdf"},{"type":"article","key":"ostanin2018interactive","title":"Interactive Robot Programing Using Mixed Reality","author":"Ostanin, Mikhail and Klimchik, Alexandr","journal":"IFAC-PapersOnLine","volume":"51","number":"22","pages":"50--55","year":"2018","publisher":"Elsevier","doi":"10.1016/J.IFACOL.2018.11.517","authors":["Mikhail Ostanin","Alexandr Klimchik"],"dblp":"conf/syroco/OstaninK18","venue":"SyRoCo","abstract":null,"paperId":"45bea9943783a77faa5e170b804b8a881063a8be","paperTitle":"Interactive Robot Programing Using Mixed Reality","paperUrl":"https://www.semanticscholar.org/paper/45bea9943783a77faa5e170b804b8a881063a8be","images":[],"pdf":"https://doi.org/10.1016/j.ifacol.2018.11.517"},{"type":"inproceedings","key":"piumatti2017spatial","title":"Spatial Augmented Reality Meets Robots: Human-Machine Interaction in Cloud-Based Projected Gaming Environments","author":"Piumatti, Giovanni and Sanna, Andrea and Gaspardone, Marco and Lamberti, Fabrizio","booktitle":"2017 IEEE International Conference on Consumer Electronics (ICCE)","pages":"176--179","year":"2017","organization":"IEEE","doi":"10.1109/ICCE.2017.7889276","authors":["Giovanni Piumatti","Andrea Sanna","Marco Gaspardone","Fabrizio Lamberti"],"dblp":"conf/iccel/PiumattiSGL17","venue":"2017 IEEE International Conference on Consumer Electronics (ICCE)","abstract":"Augmented Reality (AR) is expected to change the way we play, by transforming the world around us in an incredibly rich gaming environment. In this work, connected robots and natural interaction means are combined with projected AR to create a gaming experience more physical and engaging.","paperId":"96e4ff48662990d3c96858343e7f14c18883cf82","paperTitle":"Spatial Augmented Reality meets robots: Human-machine interaction in cloud-based projected gaming environments","paperUrl":"https://www.semanticscholar.org/paper/96e4ff48662990d3c96858343e7f14c18883cf82","images":["https://d3i71xaburhd42.cloudfront.net/96e4ff48662990d3c96858343e7f14c18883cf82/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/96e4ff48662990d3c96858343e7f14c18883cf82/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/96e4ff48662990d3c96858343e7f14c18883cf82/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/96e4ff48662990d3c96858343e7f14c18883cf82/3-TableI-1.png","https://d3i71xaburhd42.cloudfront.net/96e4ff48662990d3c96858343e7f14c18883cf82/4-TableII-1.png"],"pdf":"https://doi.org/10.1109/ICCE.2017.7889276"},{"type":"inproceedings","key":"liu2011roboshop","title":"Roboshop: Multi-Layered Sketching Interface for Robot Housework Assignment and Management","author":"Liu, Kexi and Sakamoto, Daisuke and Inami, Masahiko and Igarashi, Takeo","booktitle":"Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","pages":"647--656","year":"2011","doi":"10.1145/1978942.1979035","authors":["Kexi Liu","Daisuke Sakamoto","Masahiko Inami","Takeo Igarashi"],"dblp":"conf/chi/LiuSII11","venue":"CHI","abstract":"As various home robots come into homes, the need for efficient robot task management tools is arising. Current tools are designed for controlling individual robots independently, so they are not ideally suitable for assigning coordinated action among multiple robots. To address this problem, we developed a management tool for home robots with a graphical editing interface. The user assigns instructions by selecting a tool from a toolbox and sketching on a bird\'s-eye view of the environment. Layering supports the management of multiple tasks in the same room. Layered graphical representation gives a quick overview of and access to rich information tied to the physical environment. This paper describes the prototype system and reports on our evaluation of the system.","paperId":"4d84953a18dd329968d74da7aa14d2178a536384","paperTitle":"Roboshop: multi-layered sketching interface for robot housework assignment and management","paperUrl":"https://www.semanticscholar.org/paper/4d84953a18dd329968d74da7aa14d2178a536384","images":["https://d3i71xaburhd42.cloudfront.net/4d84953a18dd329968d74da7aa14d2178a536384/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/4d84953a18dd329968d74da7aa14d2178a536384/7-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/4d84953a18dd329968d74da7aa14d2178a536384/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/4d84953a18dd329968d74da7aa14d2178a536384/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/4d84953a18dd329968d74da7aa14d2178a536384/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/4d84953a18dd329968d74da7aa14d2178a536384/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/4d84953a18dd329968d74da7aa14d2178a536384/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/4d84953a18dd329968d74da7aa14d2178a536384/6-Figure7-1.png"],"pdf":"http://www-ui.is.s.u-tokyo.ac.jp/~takeo/papers/kexi_chi2011_roboshop.pdf"},{"type":"article","key":"yuan2019human","title":"Human Gaze-Driven Spatial Tasking of an Autonomous MAV","author":"Yuan, Liangzhe and Reardon, Christopher and Warnell, Garrett and Loianno, Giuseppe","journal":"IEEE Robotics and Automation Letters","volume":"4","number":"2","pages":"1343--1350","year":"2019","publisher":"IEEE","doi":"10.1109/LRA.2019.2895419","authors":["Liangzhe Yuan","Christopher Reardon","Garrett Warnell","Giuseppe Loianno"],"dblp":"journals/ral/YuanRWL19","venue":"IEEE Robotics and Automation Letters","abstract":"In this letter, we address the problem of providing human-assisted quadrotor navigation using a set of eye tracking glasses. The advent of these devices (i.e., eye tracking glasses, virtual reality tools, etc.) provides the opportunity to create new, noninvasive forms of interaction between humans and robots. We show how a set of glasses equipped with gaze tracker, a camera, and an inertial measurement unit (IMU) can be used to estimate the relative position of the human with respect to a quadrotor, and decouple the gaze direction from the head orientation, which allows the human to spatially task (i.e., send new 3-D navigation waypoints to) the robot in an uninstrumented environment. We decouple the gaze direction from head motion by tracking the human\'s head orientation using a combination of camera and IMU data. In order to detect the flying robot, we train and use a deep neural network. We experimentally evaluate the proposed approach, and show that our pipeline has the potential to enable gaze-driven autonomy for spatial tasking. The proposed approach can be employed in multiple scenarios including inspection and first response, as well as by people with disabilities that affect their mobility.","paperId":"8873d45cf2c93e1788612b9f98c7bb8e74cbef3f","paperTitle":"Human Gaze-Driven Spatial Tasking of an Autonomous MAV","paperUrl":"https://www.semanticscholar.org/paper/8873d45cf2c93e1788612b9f98c7bb8e74cbef3f","images":["https://d3i71xaburhd42.cloudfront.net/8873d45cf2c93e1788612b9f98c7bb8e74cbef3f/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/8873d45cf2c93e1788612b9f98c7bb8e74cbef3f/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/8873d45cf2c93e1788612b9f98c7bb8e74cbef3f/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/8873d45cf2c93e1788612b9f98c7bb8e74cbef3f/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/8873d45cf2c93e1788612b9f98c7bb8e74cbef3f/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/8873d45cf2c93e1788612b9f98c7bb8e74cbef3f/7-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/8873d45cf2c93e1788612b9f98c7bb8e74cbef3f/5-TableI-1.png","https://d3i71xaburhd42.cloudfront.net/8873d45cf2c93e1788612b9f98c7bb8e74cbef3f/6-TableII-1.png"],"pdf":"https://doi.org/10.1109/LRA.2019.2895419"},{"type":"inproceedings","key":"bambusek2019combining","title":"Combining Interactive Spatial Augmented Reality with Head-Mounted Display for End-User Collaborative Robot Programming","author":"Bambu^sek, Daniel and Materna, Zden\u0117k and Kapinus, Michal and Beran, V\'it\u0117zslav and Smr\u017c, Pavel","booktitle":"2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","pages":"1--8","year":"2019","organization":"IEEE","doi":"10.1109/RO-MAN46459.2019.8956315","authors":["Daniel Bambu^sek","Zden\u0117k Materna","Michal Kapinus","V\'it\u0117zslav Beran","Pavel Smr\u017c"],"dblp":"conf/ro-man/BambusekMKBS19","venue":"2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","abstract":"This paper proposes an intuitive approach for collaborative robot end-user programming using a combination of interactive spatial augmented reality (ISAR) and headmounted display (HMD). It aims to reduce user\u2019s workload and to let the user program the robot faster than in classical approaches (e.g. kinesthetic teaching). The proposed approach, where user is using a mixed-reality HMD \u2013 Microsoft HoloLens \u2013 and touch-enabled table with SAR projected interface as input devices, is compared to a baseline approach, where robot\u2019s arms and a touch-enabled table are used as input devices. Main advantages of the proposed approach are the possibility to program the collaborative workspace without the presence of the robot, its speed in comparison to the kinesthetic teaching and an ability to quickly visualize learned program instructions, in form of virtual objects, to enhance the users\u2019 orientation within those programs. The approach was evaluated on a set of 20 users using the within-subject experiment design. Evaluation consisted of two pick and place tasks, where users had to start from the scratch as well as to update the existing program. Based on the experiment results, the proposed approach is better in qualitative measures by 33.84% and by 28.46% in quantitative measures over the baseline approach for both tasks.","paperId":"60a84e7736a7c8841808a00e4445025867b397f5","paperTitle":"Combining Interactive Spatial Augmented Reality with Head-Mounted Display for End-User Collaborative Robot Programming","paperUrl":"https://www.semanticscholar.org/paper/60a84e7736a7c8841808a00e4445025867b397f5","images":["https://d3i71xaburhd42.cloudfront.net/168f447f687004e65ad35dc8d78ab7be6f00bfd0/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/168f447f687004e65ad35dc8d78ab7be6f00bfd0/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/168f447f687004e65ad35dc8d78ab7be6f00bfd0/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/168f447f687004e65ad35dc8d78ab7be6f00bfd0/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/168f447f687004e65ad35dc8d78ab7be6f00bfd0/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/168f447f687004e65ad35dc8d78ab7be6f00bfd0/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/168f447f687004e65ad35dc8d78ab7be6f00bfd0/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/168f447f687004e65ad35dc8d78ab7be6f00bfd0/7-TableI-1.png"],"pdf":"https://doi.org/10.1109/RO-MAN46459.2019.8956315"},{"type":"inproceedings","key":"huy2017see","title":"See-through and Spatial Augmented Reality-a Novel Framework for Human-Robot Interaction","author":"Huy, Dinh Quang and Vietcheslav, I and Lee, Gerald Seet Gim","booktitle":"2017 3rd International Conference on Control, Automation and Robotics (ICCAR)","pages":"719--726","year":"2017","organization":"IEEE","doi":"10.1109/ICCAR.2017.7942791","authors":["Dinh Quang Huy","I Vietcheslav","Gerald Seet Gim Lee"],"venue":"2017 3rd International Conference on Control, Automation and Robotics (ICCAR)","abstract":"Autonomous and semi-autonomous mobile robots have been deployed to cooperate with humans in many industrial applications. These tasks require human and robot to communicate and present information quickly and effectively. Recent human-robot interfaces usually use a setup including a camera and a projector attached to the mobile robot to project the information to the floor or to the wall during the interaction process. However, there are some limitations to these interfaces. First, using a projector for projecting information seems to be fine for an indoor application. On the contrary, it is very difficult or even impossible for users to view this source of information in outdoor contexts. This makes the current framework inappropriate for many outdoor industrial tasks. Secondly, as the projector is the only device for exchanging information between human and robot, the human-robot interacting process is insecure and people who work in the same environment can control the robot in the same manner as the main operator. Finally, the current interfaces normally use mouse, keyboard or a teach pendant to provide task information to the robot. This approach poses some difficulties if the main operator is working in an industrial context where he is supposed to wear protective equipment such as gloves or helmets which make it hard to control a mouse or to type on a keyboard. This work proposes a new interface framework for human - computer interaction in industry that can overcome the current limitations of previous works. The framework uses a laser-writer instead of a projector which is suitable for both indoor and outdoor applications. Furthermore, the combination of see-through head-mounted display augmented reality and spatial augmented reality would provide the system a novel way to enhance the security level of exchanging information since the system now can separate the information presenting to the main user and to people working in the same environment. Finally, a novel hand-held device is incorporated to the framework which provides various input modalities for users to interact with the mobile robot. The device will allows the elimination of mouse and keyboard or teach pendants in industrial contexts.","paperId":"7b55c531b205a91294e0f28e8cdf663e0a149720","paperTitle":"See-through and spatial augmented reality - a novel framework for human-robot interaction","paperUrl":"https://www.semanticscholar.org/paper/7b55c531b205a91294e0f28e8cdf663e0a149720","images":[],"pdf":"https://dr.ntu.edu.sg/bitstream/10356/83720/1/See-through%20and%20spatial%20augmented%20reality%20-%20a%20novel%20framework%20for%20human-robot%20interaction.pdf"},{"type":"inproceedings","key":"maly2016augmented","title":"Augmented Reality Experiments with Industrial Robot in Industry 4.0 Environment","author":"Mal`y, Ivo and Sedl\'a\u010bek, David and Leitao, Paulo","booktitle":"2016 IEEE 14th international conference on industrial informatics (INDIN)","pages":"176--181","year":"2016","organization":"IEEE","doi":"10.1109/INDIN.2016.7819154","authors":["Ivo Mal`y","David Sedl\'a\u010bek","Paulo Leitao"],"dblp":"conf/indin/MalySL16","venue":"2016 IEEE 14th International Conference on Industrial Informatics (INDIN)","abstract":"The role of human in the Industrie 4.0 vision is still considered as irreplaceable. Therefore, user interfaces of cyber-physical systems involved in the production automation need to be well designed and taking into consideration the industrial application requirements. With the advances in augmented and virtual reality data visualization and novel interaction techniques like mid-air gestures, these approaches seem to be suitable for integration into the industry environment. This paper describes the implementation of an augmented reality application for smart glasses with mid-air gestures and smart phone with touch interaction to compare and evaluate the usage of such interfaces in a production cell comprising an industrial robot.","paperId":"09f6f0e9918e81eed115dae7aa6fde3222ae99a0","paperTitle":"Augmented reality experiments with industrial robot in industry 4.0 environment","paperUrl":"https://www.semanticscholar.org/paper/09f6f0e9918e81eed115dae7aa6fde3222ae99a0","images":["https://d3i71xaburhd42.cloudfront.net/09f6f0e9918e81eed115dae7aa6fde3222ae99a0/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/09f6f0e9918e81eed115dae7aa6fde3222ae99a0/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/09f6f0e9918e81eed115dae7aa6fde3222ae99a0/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/09f6f0e9918e81eed115dae7aa6fde3222ae99a0/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/09f6f0e9918e81eed115dae7aa6fde3222ae99a0/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/09f6f0e9918e81eed115dae7aa6fde3222ae99a0/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/09f6f0e9918e81eed115dae7aa6fde3222ae99a0/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/09f6f0e9918e81eed115dae7aa6fde3222ae99a0/5-Figure8-1.png"],"pdf":"https://bibliotecadigital.ipb.pt/bitstream/10198/13308/1/390.pdf"},{"type":"article","key":"mourtzis2017augmented","title":"Augmented Reality Application to Support Remote Maintenance as a Service in the Robotics Industry","author":"Mourtzis, Dimitris and Zogopoulos, Vasilios and Vlachou, E","journal":"Procedia Cirp","volume":"63","pages":"46--51","year":"2017","publisher":"Elsevier","authors":["Dimitris Mourtzis","Vasilios Zogopoulos","E Vlachou"],"venue":"","abstract":"Maintenance of manufactured products is among the most common services in industry and its cost often exceed 30% of the operating costs. Modern manufacturing companies are shifting their focus from products to combined ecosystem of ProductsService Systems (PSS). Towards that end, the main objective of this work is to develop a cloud-based service-oriented system that implements AR technology for remote maintenance by enabling cooperation between the onspot technician and the manufacturer. The proposed system includes the methods for the record of the malfunction by the end user, the actions required by the expert so as to provide instructions in an Augmented Reality application for maintenance, as well as the cloudbased platform that will allow their communication and the exchange of information. In addition to the above, the proposed system consists of smart assembly/disassembly algorithms for automated generation of assembly sequences and AR scenes and improved interface, aiming to maximize existing knowledge usage while creating vivid AR service instructions. The proposed system is validated in a real-life case study following the requirements of a robotics SME. \xa9 2017 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the scientific committee of The 50th CIRP Conference on Manufacturing Systems.","paperId":"5cd59e57c5d4e3bdb6051f2754fcb13e476c5a8a","paperTitle":"Augmented reality application to support remote maintenance as a service in the Robotics industry","paperUrl":"https://www.semanticscholar.org/paper/5cd59e57c5d4e3bdb6051f2754fcb13e476c5a8a","images":["https://d3i71xaburhd42.cloudfront.net/5cd59e57c5d4e3bdb6051f2754fcb13e476c5a8a/2-Figure1-1.png"],"pdf":"http://isiarticles.com/bundles/Article/pre/pdf/83002.pdf"},{"type":"article","key":"aleotti2017detection","title":"Detection of Nuclear Sources by UAV Teleoperation Using a Visuo-Haptic Augmented Reality Interface","author":"Aleotti, Jacopo and Micconi, Giorgio and Caselli, Stefano and Benassi, Giacomo and Zambelli, Nicola and Bettelli, Manuele and Zappettini, Andrea","journal":"Sensors","volume":"17","number":"10","pages":"2234","year":"2017","publisher":"Multidisciplinary Digital Publishing Institute","doi":"10.3390/s17102234","authors":["Jacopo Aleotti","Giorgio Micconi","Stefano Caselli","Giacomo Benassi","Nicola Zambelli","Manuele Bettelli","Andrea Zappettini"],"dblp":"journals/sensors/AleottiMCBZBZ17","venue":"Sensors","abstract":"A visuo-haptic augmented reality (VHAR) interface is presented enabling an operator to teleoperate an unmanned aerial vehicle (UAV) equipped with a custom CdZnTe-based spectroscopic gamma-ray detector in outdoor environments. The task is to localize nuclear radiation sources, whose location is unknown to the user, without the close exposure of the operator. The developed detector also enables identification of the localized nuclear sources. The aim of the VHAR interface is to increase the situation awareness of the operator. The user teleoperates the UAV using a 3DOF haptic device that provides an attractive force feedback around the location of the most intense detected radiation source. Moreover, a fixed camera on the ground observes the environment where the UAV is flying. A 3D augmented reality scene is displayed on a computer screen accessible to the operator. Multiple types of graphical overlays are shown, including sensor data acquired by the nuclear radiation detector, a virtual cursor that tracks the UAV and geographical information, such as buildings. Experiments performed in a real environment are reported using an intense nuclear source.","paperId":"5498d1bf913cdf1ba2d7525c6adad22538c1da9e","paperTitle":"Detection of Nuclear Sources by UAV Teleoperation Using a Visuo-Haptic Augmented Reality Interface","paperUrl":"https://www.semanticscholar.org/paper/5498d1bf913cdf1ba2d7525c6adad22538c1da9e","images":["https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/6-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/5-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/7-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/7-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/8-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/10-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/12-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/13-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/14-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/14-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/15-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/16-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/16-Figure14-1.png","https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/16-Figure15-1.png","https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/17-Figure16-1.png","https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/17-Figure17-1.png","https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/18-Figure18-1.png","https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/18-Figure19-1.png","https://d3i71xaburhd42.cloudfront.net/5498d1bf913cdf1ba2d7525c6adad22538c1da9e/18-Figure20-1.png"],"pdf":null},{"type":"inproceedings","key":"papachristos2016augmented","title":"Augmented Reality-Enhanced Structural Inspection Using Aerial Robots","author":"Papachristos, Christos and Alexis, Kostas","booktitle":"2016 IEEE international symposium on intelligent control (ISIC)","pages":"1--6","year":"2016","organization":"IEEE","doi":"10.1109/ISIC.2016.7579983","authors":["Christos Papachristos","Kostas Alexis"],"dblp":"conf/IEEEisic/PapachristosA16","venue":"2016 IEEE International Symposium on Intelligent Control (ISIC)","abstract":"This paper investigates the arising potential when automated path planning for aerial robotic structural inspection is combined with an Augmented Reality interface that provides live feed of stereo views fused with real-time 3D reconstruction data of the environment, while allowing seamless on-the-fly adaptation of the next robot viewpoints using intuitive head motions. The proposed solution aims to address the problem of accurate inspection and mapping of structures and environments for which a prior model exists but is not accurate, potentially outdated, or does not encode important features and semantics such as human-readable indications and other texture information. To approach the problem, the robot computes an optimized inspection path given any prior knowledge of the environment, while the human operator utilizes the live camera views and the real-time derived 3D map data to locally adjust the reference trajectory of the robot, such that it visits an updated set of viewpoints which provides the desired coverage of the real environment and sufficient focus on certain features and details. An autonomous aerial robot capable of navigation and mapping in GPS-denied environments is employed and combined with the Augmented Reality interface to experimentally demonstrate the potential of the approach in structural inspection applications.","paperId":"c1f6589b92f57aef04fbd0d1185e912396b7a186","paperTitle":"Augmented reality-enhanced structural inspection using aerial robots","paperUrl":"https://www.semanticscholar.org/paper/c1f6589b92f57aef04fbd0d1185e912396b7a186","images":["https://d3i71xaburhd42.cloudfront.net/c1f6589b92f57aef04fbd0d1185e912396b7a186/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/c1f6589b92f57aef04fbd0d1185e912396b7a186/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/c1f6589b92f57aef04fbd0d1185e912396b7a186/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/c1f6589b92f57aef04fbd0d1185e912396b7a186/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/c1f6589b92f57aef04fbd0d1185e912396b7a186/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/c1f6589b92f57aef04fbd0d1185e912396b7a186/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/c1f6589b92f57aef04fbd0d1185e912396b7a186/5-Figure7-1.png"],"pdf":"https://doi.org/10.1109/ISIC.2016.7579983"},{"type":"inproceedings","key":"follmer2013inform","title":"inFORM: Dynamic Physical Affordances and Constraints through Shape and Object Actuation.","author":"Follmer, Sean and Leithinger, Daniel and Olwal, Alex and Hogge, Akimitsu and Ishii, Hiroshi","booktitle":"Uist","volume":"13","number":"10.1145","pages":"2501988--2502032","year":"2013","doi":"10.1145/2501988.2502032","authors":["Sean Follmer","Daniel Leithinger","Alex Olwal","Akimitsu Hogge","Hiroshi Ishii"],"dblp":"conf/uist/FollmerLOHI13","venue":"UIST","abstract":"Past research on shape displays has primarily focused on rendering content and user interface elements through shape output, with less emphasis on dynamically changing UIs. We propose utilizing shape displays in three different ways to mediate interaction: to facilitate by providing dynamic physical affordances through shape change, to restrict by guiding users with dynamic physical constraints, and to manipulate by actuating physical objects. We outline potential interaction techniques and introduce Dynamic Physical Affordances and Constraints with our inFORM system, built on top of a state-of-the-art shape display, which provides for variable stiffness rendering and real-time user input through direct touch and tangible interaction. A set of motivating examples demonstrates how dynamic affordances, constraints and object actuation can create novel interaction possibilities.","paperId":"83b281ee37928539111a4e72478f25101446184f","paperTitle":"inFORM: dynamic physical affordances and constraints through shape and object actuation","paperUrl":"https://www.semanticscholar.org/paper/83b281ee37928539111a4e72478f25101446184f","images":["https://d3i71xaburhd42.cloudfront.net/83b281ee37928539111a4e72478f25101446184f/5-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/83b281ee37928539111a4e72478f25101446184f/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/83b281ee37928539111a4e72478f25101446184f/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/83b281ee37928539111a4e72478f25101446184f/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/83b281ee37928539111a4e72478f25101446184f/7-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/83b281ee37928539111a4e72478f25101446184f/8-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/83b281ee37928539111a4e72478f25101446184f/8-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/83b281ee37928539111a4e72478f25101446184f/8-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/83b281ee37928539111a4e72478f25101446184f/9-Figure10-1.png"],"pdf":"https://dspace.mit.edu/bitstream/1721.1/92273/2/Follmer_inFORM.pdf"},{"type":"inproceedings","key":"watanabe2015communicating","title":"Communicating Robotic Navigational Intentions","author":"Watanabe, Atsushi and Ikeda, Tetsushi and Morales, Yoichi and Shinozawa, Kazuhiko and Miyashita, Takahiro and Hagita, Norihiro","booktitle":"2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","pages":"5763--5769","year":"2015","organization":"IEEE","doi":"10.1109/IROS.2015.7354195","authors":["Atsushi Watanabe","Tetsushi Ikeda","Yoichi Morales","Kazuhiko Shinozawa","Takahiro Miyashita","Norihiro Hagita"],"dblp":"conf/iros/WatanabeIMSMH15","venue":"2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","abstract":"This paper presents a study on intention communication in a navigational context using a robotic wheelchair. The robotic wheelchair uses light projection to communicate its motion intentions. The novelty of the work is threefold: the communication of robot intentions to the passenger, the consideration of passenger and robot as a group (\u201cin-group\u201d) [1] who share motion intentions and the communication of the in-group intentions to other pedestrians (the \u201cout-group\u201d). A comparison in an autonomous navigation task where the robotic wheelchair autonomously navigates the environment with and without intention communication was performed showing that passengers and walking people found intention communication intuitive and helpful for passing by actions. Evaluation results significantly show human participant preference for having navigational intention communication for the wheelchair passenger and the person passing by it. Quantitative results show the motion of the person passing by the wheelchair with intention communication was significantly smoother compared to without intention communication.","paperId":"340a5efb0a808c0971a4667ecb89103ffdb4818a","paperTitle":"Communicating robotic navigational intentions","paperUrl":"https://www.semanticscholar.org/paper/340a5efb0a808c0971a4667ecb89103ffdb4818a","images":["https://d3i71xaburhd42.cloudfront.net/340a5efb0a808c0971a4667ecb89103ffdb4818a/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/340a5efb0a808c0971a4667ecb89103ffdb4818a/1-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/340a5efb0a808c0971a4667ecb89103ffdb4818a/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/340a5efb0a808c0971a4667ecb89103ffdb4818a/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/340a5efb0a808c0971a4667ecb89103ffdb4818a/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/340a5efb0a808c0971a4667ecb89103ffdb4818a/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/340a5efb0a808c0971a4667ecb89103ffdb4818a/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/340a5efb0a808c0971a4667ecb89103ffdb4818a/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/340a5efb0a808c0971a4667ecb89103ffdb4818a/6-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/340a5efb0a808c0971a4667ecb89103ffdb4818a/6-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/340a5efb0a808c0971a4667ecb89103ffdb4818a/6-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/340a5efb0a808c0971a4667ecb89103ffdb4818a/6-Figure13-1.png"],"pdf":"https://doi.org/10.1109/IROS.2015.7354195"},{"type":"inproceedings","key":"andersen2016projecting","title":"Projecting Robot Intentions into Human Environments","author":"Andersen, Rasmus S and Madsen, Ole and Moeslund, Thomas B and Amor, Heni Ben","booktitle":"2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)","pages":"294--301","year":"2016","organization":"IEEE","doi":"10.1109/ROMAN.2016.7745145","authors":["Rasmus S Andersen","Ole Madsen","Thomas B Moeslund","Heni Ben Amor"],"dblp":"conf/ro-man/AndersenMMA16","venue":"2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)","abstract":"Trained human co-workers can often easily predict each other\'s intentions based on prior experience. When collaborating with a robot coworker, however, intentions are hard or impossible to infer. This difficulty of mental introspection makes human-robot collaboration challenging and can lead to dangerous misunderstandings. In this paper, we present a novel, object-aware projection technique that allows robots to visualize task information and intentions on physical objects in the environment. The approach uses modern object tracking methods in order to display information at specific spatial locations taking into account the pose and shape of surrounding objects. As a result, a human co-worker can be informed in a timely manner about the safety of the workspace, the site of next robot manipulation tasks, and next subtasks to perform. A preliminary usability study compares the approach to collaboration approaches based on monitors and printed text. The study indicates that, on average, the user effectiveness and satisfaction is higher with the projection based approach.","paperId":"dec78adc23d1371406d8f709c6715f71b78b4190","paperTitle":"Projecting robot intentions into human environments","paperUrl":"https://www.semanticscholar.org/paper/dec78adc23d1371406d8f709c6715f71b78b4190","images":["https://d3i71xaburhd42.cloudfront.net/dec78adc23d1371406d8f709c6715f71b78b4190/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/dec78adc23d1371406d8f709c6715f71b78b4190/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/dec78adc23d1371406d8f709c6715f71b78b4190/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/dec78adc23d1371406d8f709c6715f71b78b4190/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/dec78adc23d1371406d8f709c6715f71b78b4190/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/dec78adc23d1371406d8f709c6715f71b78b4190/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/dec78adc23d1371406d8f709c6715f71b78b4190/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/dec78adc23d1371406d8f709c6715f71b78b4190/6-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/dec78adc23d1371406d8f709c6715f71b78b4190/7-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/dec78adc23d1371406d8f709c6715f71b78b4190/5-TableI-1.png"],"pdf":"https://doi.org/10.1109/ROMAN.2016.7745145"},{"type":"incollection","key":"rosen2020communicating","title":"Communicating Robot Arm Motion Intent through Mixed Reality Head-Mounted Displays","author":"Rosen, Eric and Whitney, David and Phillips, Elizabeth and Chien, Gary and Tompkin, James and Konidaris, George and Tellex, Stefanie","booktitle":"Robotics research","pages":"301--316","year":"2020","publisher":"Springer","doi":"10.1007/978-3-030-28619-4_26","authors":["Eric Rosen","David Whitney","Elizabeth Phillips","Gary Chien","James Tompkin","George Konidaris","Stefanie Tellex"],"dblp":"journals/corr/abs-1708-03655","venue":"ISRR","abstract":"Efficient motion intent communication is necessary for safe and collaborative work environments with collocated humans and robots. Humans efficiently communicate their motion intent to other humans through gestures, gaze, and social cues. However, robots often have difficulty efficiently communicating their motion intent to humans via these methods. Many existing methods for robot motion intent communication rely on 2D displays, which require the human to continually pause their work and check a visualization. We propose a mixed reality head-mounted display visualization of the proposed robot motion over the wearer\'s real-world view of the robot and its environment. To evaluate the effectiveness of this system against a 2D display visualization and against no visualization, we asked 32 participants to labeled different robot arm motions as either colliding or non-colliding with blocks on a table. We found a 16% increase in accuracy with a 62% decrease in the time it took to complete the task compared to the next best system. This demonstrates that a mixed-reality HMD allows a human to more quickly and accurately tell where the robot is going to move than the compared baselines.","paperId":"9653d6bd4ba9118e7491a07c02c685075b766ad6","paperTitle":"Communicating Robot Arm Motion Intent Through Mixed Reality Head-mounted Displays","paperUrl":"https://www.semanticscholar.org/paper/9653d6bd4ba9118e7491a07c02c685075b766ad6","images":["https://d3i71xaburhd42.cloudfront.net/9653d6bd4ba9118e7491a07c02c685075b766ad6/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/9653d6bd4ba9118e7491a07c02c685075b766ad6/10-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/9653d6bd4ba9118e7491a07c02c685075b766ad6/11-Figure3-1.png"],"pdf":null},{"type":"inproceedings","key":"walker2018communicating","title":"Communicating Robot Motion Intent with Augmented Reality","author":"Walker, Michael and Hedayati, Hooman and Lee, Jennifer and Szafir, Daniel","booktitle":"Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction","pages":"316--324","year":"2018","doi":"10.1145/3171221.3171253","authors":["Michael Walker","Hooman Hedayati","Jennifer Lee","Daniel Szafir"],"dblp":"conf/hri/WalkerHLS18","venue":"2018 13th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","abstract":"Humans coordinate teamwork by conveying intent through social cues, such as gestures and gaze behaviors. However, these methods may not be possible for appearance-constrained robots that lack anthropomorphic or zoomorphic features, such as aerial robots. We explore a new design space for communicating robot motion in-tent by investigating how augmented reality (AR) might mediate human-robot interactions. We develop a series of explicit and implicit designs for visually signaling robot motion intent using AR, which we evaluate in a user study. We found that several of our AR designs significantly improved objective task efficiency over a base-line in which users only received physically-embodied orientation cues. In addition, our designs offer several trade-offs in terms of intent clarity and user perceptions of the robot as a teammate.","paperId":"e2295562039f0033ca8e0a6c5ad1d30bb468e8cc","paperTitle":"Communicating Robot Motion Intent with Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/e2295562039f0033ca8e0a6c5ad1d30bb468e8cc","images":["https://d3i71xaburhd42.cloudfront.net/e2295562039f0033ca8e0a6c5ad1d30bb468e8cc/3-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/e2295562039f0033ca8e0a6c5ad1d30bb468e8cc/5-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/e2295562039f0033ca8e0a6c5ad1d30bb468e8cc/6-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/e2295562039f0033ca8e0a6c5ad1d30bb468e8cc/8-Figure4-1.png"],"pdf":"http://www.cs.columbia.edu/~allen/S19/Student_Papers/robot_intent_AR.pdf"},{"type":"inproceedings","key":"arevalo2021assisting","title":"Assisting Manipulation and Grasping in Robot Teleoperation with Augmented Reality Visual Cues","author":"Arevalo Arboleda, Stephanie and R\\"ucker, Franziska and Dierks, Tim and Gerken, Jens","booktitle":"Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems","pages":"1--14","year":"2021","doi":"10.1145/3411764.3445398","authors":["Stephanie Arevalo Arboleda","Franziska R\\"ucker","Tim Dierks","Jens Gerken"],"dblp":"conf/chi/ArboledaRDG21","venue":"CHI","abstract":"Teleoperating industrial manipulators in co-located spaces can be challenging. Facilitating robot teleoperation by providing additional visual information about the environment and the robot affordances using augmented reality (AR), can improve task performance in manipulation and grasping. In this paper, we present two designs of augmented visual cues, that aim to enhance the visual space of the robot operator through hints about the position of the robot gripper in the workspace and in relation to the target. These visual cues aim to improve the distance perception and thus, the task performance. We evaluate both designs against a baseline in an experiment where participants teleoperate a robotic arm to perform pick-and-place tasks. Our results show performance improvements in different levels, reflecting in objective and subjective measures with trade-offs in terms of time, accuracy, and participants\u2019 views of teleoperation. These findings show the potential of AR not only in teleoperation, but in understanding the human-robot workspace.","paperId":"bc7d7bacda6b04190a96be78e37d7b59a0c7b27a","paperTitle":"Assisting Manipulation and Grasping in Robot Teleoperation with Augmented Reality Visual Cues","paperUrl":"https://www.semanticscholar.org/paper/bc7d7bacda6b04190a96be78e37d7b59a0c7b27a","images":["https://d3i71xaburhd42.cloudfront.net/bc7d7bacda6b04190a96be78e37d7b59a0c7b27a/9-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/bc7d7bacda6b04190a96be78e37d7b59a0c7b27a/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/bc7d7bacda6b04190a96be78e37d7b59a0c7b27a/9-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/bc7d7bacda6b04190a96be78e37d7b59a0c7b27a/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/bc7d7bacda6b04190a96be78e37d7b59a0c7b27a/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/bc7d7bacda6b04190a96be78e37d7b59a0c7b27a/7-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/bc7d7bacda6b04190a96be78e37d7b59a0c7b27a/8-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/bc7d7bacda6b04190a96be78e37d7b59a0c7b27a/10-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/bc7d7bacda6b04190a96be78e37d7b59a0c7b27a/10-Figure8-1.png"],"pdf":"https://doi.org/10.1145/3411764.3445398"},{"type":"inproceedings","key":"chen2021pinpointfly","title":"PinpointFly: An Egocentric Position-Control Drone Interface Using Mobile AR","author":"Chen, Linfeng and Takashima, Kazuki and Fujita, Kazuyuki and Kitamura, Yoshifumi","booktitle":"Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems","pages":"1--13","year":"2021","doi":"10.1145/3411764.3445110","authors":["Linfeng Chen","Kazuki Takashima","Kazuyuki Fujita","Yoshifumi Kitamura"],"dblp":"conf/chi/ChenTFK21","venue":"CHI","abstract":"Accurate drone positioning is challenging because pilots only have a limited position and direction perception of a flying drone from their perspective. This makes conventional joystick-based speed control inaccurate and more complicated and significantly degrades piloting performance. We propose PinpointFly, an egocentric drone interface that allows pilots to arbitrarily position and rotate a drone using position-control direct interactions on a see-through mobile AR where the drone position and direction are visualized with a virtual cast shadow (i.e., the drone\u2019s orthogonal projection onto the floor). Pilots can point to the next position or draw the drone\u2019s flight trajectory by manipulating the virtual cast shadow and the direction/height slider bar on the touchscreen. We design and implement a prototype of PinpointFly for indoor and visual line of sight scenarios, which are comprised of real-time and predefined motion-control techniques. We conduct two user studies with simple positioning and inspection tasks. Our results demonstrate that PinpointFly makes the drone positioning and inspection operations faster, more accurate, simpler and fewer workload than a conventional joystick interface with a speed-control method.","paperId":"651d2542ef774518b83b5bcb3ad8f65a38c022d6","paperTitle":"PinpointFly: An Egocentric Position-control Drone Interface using Mobile AR","paperUrl":"https://www.semanticscholar.org/paper/651d2542ef774518b83b5bcb3ad8f65a38c022d6","images":["https://d3i71xaburhd42.cloudfront.net/651d2542ef774518b83b5bcb3ad8f65a38c022d6/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/651d2542ef774518b83b5bcb3ad8f65a38c022d6/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/651d2542ef774518b83b5bcb3ad8f65a38c022d6/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/651d2542ef774518b83b5bcb3ad8f65a38c022d6/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/651d2542ef774518b83b5bcb3ad8f65a38c022d6/7-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/651d2542ef774518b83b5bcb3ad8f65a38c022d6/8-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/651d2542ef774518b83b5bcb3ad8f65a38c022d6/9-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/651d2542ef774518b83b5bcb3ad8f65a38c022d6/9-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/651d2542ef774518b83b5bcb3ad8f65a38c022d6/11-Figure10-1.png"],"pdf":"https://dl.acm.org/doi/pdf/10.1145/3411764.3445110"},{"type":"inproceedings","key":"kasahara2013extouch","title":"exTouch: Spatially-Aware Embodied Manipulation of Actuated Objects Mediated by Augmented Reality","author":"Kasahara, Shunichi and Niiyama, Ryuma and Heun, Valentin and Ishii, Hiroshi","booktitle":"Proceedings of the 7th International Conference on Tangible, Embedded and Embodied Interaction","pages":"223--228","year":"2013","doi":"10.1145/2460625.2460661","authors":["Shunichi Kasahara","Ryuma Niiyama","Valentin Heun","Hiroshi Ishii"],"dblp":"conf/tei/KasaharaNHI13","venue":"TEI \'13","abstract":"As domestic robots and smart appliances become increasingly common, they require a simple, universal interface to control their motion. Such an interface must support a simple selection of a connected device, highlight its capabilities and allow for an intuitive manipulation. We propose \\"exTouch\\", an embodied spatially-aware approach to touch and control devices through an augmented reality mediated mobile interface. The \\"exTouch\\" system extends the users touchscreen interactions into the real world by enabling spatial control over the actuated object. When users touch a device shown in live video on the screen, they can change its position and orientation through multi-touch gestures or by physically moving the screen in relation to the controlled object. We demonstrate that the system can be used for applications such as an omnidirectional vehicle, a drone, and moving furniture for reconfigurable room.","paperId":"ee65bc9d35b3434175f4fe78686beda122d0f1b1","paperTitle":"exTouch: spatially-aware embodied manipulation of actuated objects mediated by augmented reality","paperUrl":"https://www.semanticscholar.org/paper/ee65bc9d35b3434175f4fe78686beda122d0f1b1","images":["https://d3i71xaburhd42.cloudfront.net/ee65bc9d35b3434175f4fe78686beda122d0f1b1/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/ee65bc9d35b3434175f4fe78686beda122d0f1b1/3-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/ee65bc9d35b3434175f4fe78686beda122d0f1b1/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/ee65bc9d35b3434175f4fe78686beda122d0f1b1/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/ee65bc9d35b3434175f4fe78686beda122d0f1b1/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/ee65bc9d35b3434175f4fe78686beda122d0f1b1/5-Figure5-1.png"],"pdf":"https://dspace.mit.edu/bitstream/1721.1/79861/1/Ishii_ExTouch%20spatially.pdf"},{"type":"inproceedings","key":"gong2019projection","title":"Projection-Based Augmented Reality Interface for Robot Grasping Tasks","author":"Gong, LL and Ong, SK and Nee, AYC","booktitle":"Proceedings of the 2019 4th International Conference on Robotics, Control and Automation","pages":"100--104","year":"2019","doi":"10.1145/3351180.3351204","authors":["LL Gong","SK Ong","AYC Nee"],"venue":"Proceedings of the 2019 4th International Conference on Robotics, Control and Automation","abstract":"This paper presents an augmented reality (AR) interface for robot programming of pick-and-place tasks as well as assembly operations. The aim of the AR interface is to increase the intuitiveness and ease of robot programming. Marker tracking is used to automate sensor registration for easier workspace setup. Object recognition is employed to transform robot programming from an absolute coordinate system to an object-based system. This transformation provides flexibility to robot programming as the AR interface can be applied to workspaces that are configured differently to accomplish the same task. A spatially immersive display method is adopted to provide a projection-based direct overlay virtual workspace, so as to give users a better frame of reference in relation to the real workspace.","paperId":"4c609bd8bcfc55dac612fb5c6633b62a9f715013","paperTitle":"Projection-based Augmented Reality Interface for Robot Grasping Tasks","paperUrl":"https://www.semanticscholar.org/paper/4c609bd8bcfc55dac612fb5c6633b62a9f715013","images":["https://d3i71xaburhd42.cloudfront.net/d705b75f9c8aef7c76f4a08d63e73d173798cd87/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/d705b75f9c8aef7c76f4a08d63e73d173798cd87/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/d705b75f9c8aef7c76f4a08d63e73d173798cd87/4-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/d705b75f9c8aef7c76f4a08d63e73d173798cd87/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/d705b75f9c8aef7c76f4a08d63e73d173798cd87/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/d705b75f9c8aef7c76f4a08d63e73d173798cd87/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/d705b75f9c8aef7c76f4a08d63e73d173798cd87/4-Figure6-1.png"],"pdf":"https://doi.org/10.1145/3351180.3351204"},{"type":"inproceedings","key":"ostanin2020human","title":"Human-Robot Interaction for Robotic Manipulator Programming in Mixed Reality","author":"Ostanin, Mikhail and Mikhel, Stanislav and Evlampiev, Alexey and Skvortsova, Valeria and Klimchik, Alexandr","booktitle":"2020 IEEE International Conference on Robotics and Automation (ICRA)","pages":"2805--2811","year":"2020","organization":"IEEE","doi":"10.1109/ICRA40945.2020.9196965","authors":["Mikhail Ostanin","Stanislav Mikhel","Alexey Evlampiev","Valeria Skvortsova","Alexandr Klimchik"],"dblp":"conf/icra/OstaninMESK20","venue":"2020 IEEE International Conference on Robotics and Automation (ICRA)","abstract":"The paper presents an approach for interactive programming of the robotic manipulator using mixed reality. The developed system is based on the HoloLens glasses connected through Robotic Operation System to Unity engine and robotic manipulators. The system gives a possibility to recognize the real robot location by the point cloud analysis, to use virtual markers and menus for the task creation, to generate a trajectory for execution in the simulator or on the real manipulator. It also provides the possibility of scaling virtual and real worlds for more accurate planning. The proposed framework has been tested on pick-and-place and contact operations execution by UR10e and KUKA iiwa robots.","paperId":"8190644bae763d8f508d89a7f204768ff47f121a","paperTitle":"Human-robot interaction for robotic manipulator programming in Mixed Reality","paperUrl":"https://www.semanticscholar.org/paper/8190644bae763d8f508d89a7f204768ff47f121a","images":["https://d3i71xaburhd42.cloudfront.net/8190644bae763d8f508d89a7f204768ff47f121a/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/8190644bae763d8f508d89a7f204768ff47f121a/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/8190644bae763d8f508d89a7f204768ff47f121a/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/8190644bae763d8f508d89a7f204768ff47f121a/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/8190644bae763d8f508d89a7f204768ff47f121a/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/8190644bae763d8f508d89a7f204768ff47f121a/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/8190644bae763d8f508d89a7f204768ff47f121a/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/8190644bae763d8f508d89a7f204768ff47f121a/6-Figure8-1.png"],"pdf":"https://doi.org/10.1109/ICRA40945.2020.9196965"},{"type":"article","key":"ni2017haptic","title":"Haptic and Visual Augmented Reality Interface for Programming Welding Robots","author":"Ni, D and Yew, AWW and Ong, SK and Nee, AYC","journal":"Advances in Manufacturing","volume":"5","number":"3","pages":"191--198","year":"2017","publisher":"Springer","doi":"10.1007/S40436-017-0184-7","authors":["D Ni","AWW Yew","SK Ong","AYC Nee"],"venue":"","abstract":"It is a challenging task for operators to program a remote robot for welding manipulation depending only on the visual information from the remote site. This paper proposes an intuitive user interface for programming welding robots remotely using augmented reality (AR) with haptic feedback. The proposed system uses a depth camera to reconstruct the surfaces of workpieces. A haptic input device is used to allow users to define welding paths along these surfaces. An AR user interface is developed to allow users to visualize and adjust the orientation of the welding torch. Compared with the traditional robotic welding path programming methods which rely on prior CAD models or contact between the robot end-effector and the workpiece, this proposed approach allows for fast and intuitive remote robotic welding path programming without prior knowledge of CAD models of the workpieces. The experimental results show that the proposed approach is a user-friendly interface and can assist users in obtaining an accurate welding path.","paperId":"26c6d2f90160f45f3c61a0029bc9e677ff06d749","paperTitle":"Haptic and visual augmented reality interface for programming welding robots","paperUrl":"https://www.semanticscholar.org/paper/26c6d2f90160f45f3c61a0029bc9e677ff06d749","images":[],"pdf":"https://link.springer.com/content/pdf/10.1007%2Fs40436-017-0184-7.pdf"},{"type":"inproceedings","key":"quintero2018robot","title":"Robot Programming through Augmented Trajectories in Augmented Reality","author":"Quintero, Camilo Perez and Li, Sarah and Pan, Matthew KXJ and Chan, Wesley P and Van der Loos, HF Machiel and Croft, Elizabeth","booktitle":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","pages":"1838--1844","year":"2018","organization":"IEEE","doi":"10.1109/IROS.2018.8593700","authors":["Camilo Perez Quintero","Sarah Li","Matthew KXJ Pan","Wesley P Chan","HF Machiel Van der Loos","Elizabeth Croft"],"dblp":"conf/iros/QuinteroLPCLC18","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","abstract":"This paper presents a future-focused approach for robot programming based on augmented trajectories. Using a mixed reality head-mounted display (Microsoft Hololens) and a 7-DOF robot arm, we designed an augmented reality (AR) robotic interface with four interactive functions to ease the robot programming task: 1) Trajectory specification. 2) Virtual previews of robot motion. 3) Visualization of robot parameters. 4) Online reprogramming during simulation and execution. We validate our AR-robot teaching interface by comparing it with a kinesthetic teaching interface in two different scenarios as part of a pilot study: creation of contact surface path and free space path. Furthermore, we present an industrial case study that illustrates our AR manufacturing paradigm by interacting with a 7-DOF robot arm to reduce wrinkles during the pleating step of the carbon-fiber-reinforcement-polymer vacuum bagging process in a simulated scenario.","paperId":"45048984874e7d8951fd470a4b9d4736fc2e9c0d","paperTitle":"Robot Programming Through Augmented Trajectories in Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/45048984874e7d8951fd470a4b9d4736fc2e9c0d","images":["https://d3i71xaburhd42.cloudfront.net/45048984874e7d8951fd470a4b9d4736fc2e9c0d/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/45048984874e7d8951fd470a4b9d4736fc2e9c0d/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/45048984874e7d8951fd470a4b9d4736fc2e9c0d/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/45048984874e7d8951fd470a4b9d4736fc2e9c0d/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/45048984874e7d8951fd470a4b9d4736fc2e9c0d/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/45048984874e7d8951fd470a4b9d4736fc2e9c0d/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/45048984874e7d8951fd470a4b9d4736fc2e9c0d/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/45048984874e7d8951fd470a4b9d4736fc2e9c0d/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/45048984874e7d8951fd470a4b9d4736fc2e9c0d/5-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/45048984874e7d8951fd470a4b9d4736fc2e9c0d/6-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/45048984874e7d8951fd470a4b9d4736fc2e9c0d/6-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/45048984874e7d8951fd470a4b9d4736fc2e9c0d/6-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/45048984874e7d8951fd470a4b9d4736fc2e9c0d/7-Figure13-1.png"],"pdf":"https://doi.org/10.1109/IROS.2018.8593700"},{"type":"inproceedings","key":"cao2019ghostar","title":"GhostAR: A Time-Space Editor for Embodied Authoring of Human-Robot Collaborative Task with Augmented Reality","author":"Cao, Yuanzhi and Wang, Tianyi and Qian, Xun and Rao, Pawan S and Wadhawan, Manav and Huo, Ke and Ramani, Karthik","booktitle":"Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology","pages":"521--534","year":"2019","doi":"10.1145/3332165.3347902","authors":["Yuanzhi Cao","Tianyi Wang","Xun Qian","Pawan S Rao","Manav Wadhawan","Ke Huo","Karthik Ramani"],"dblp":"conf/uist/CaoWQRWHR19","venue":"UIST","abstract":"We present GhostAR, a time-space editor for authoring and acting Human-Robot-Collaborative (HRC) tasks in-situ. Our system adopts an embodied authoring approach in Augmented Reality (AR), for spatially editing the actions and programming the robots through demonstrative role-playing. We propose a novel HRC workflow that externalizes user\'s authoring as demonstrative and editable AR ghost, allowing for spatially situated visual referencing, realistic animated simulation, and collaborative action guidance. We develop a dynamic time warping (DTW) based collaboration model which takes the real-time captured motion as inputs, maps it to the previously authored human actions, and outputs the corresponding robot actions to achieve adaptive collaboration. We emphasize an in-situ authoring and rapid iterations of joint plans without an offline training process. Further, we demonstrate and evaluate the effectiveness of our workflow through HRC use cases and a three-session user study.","paperId":"376faf2aea9a6ceba994f916eb3edcde9e3b1dd4","paperTitle":"GhostAR: A Time-space Editor for Embodied Authoring of Human-Robot Collaborative Task with Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/376faf2aea9a6ceba994f916eb3edcde9e3b1dd4","images":["https://d3i71xaburhd42.cloudfront.net/810ad5fa0fd7baa93e35c0284a874e88317046ea/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/810ad5fa0fd7baa93e35c0284a874e88317046ea/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/810ad5fa0fd7baa93e35c0284a874e88317046ea/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/810ad5fa0fd7baa93e35c0284a874e88317046ea/7-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/810ad5fa0fd7baa93e35c0284a874e88317046ea/8-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/810ad5fa0fd7baa93e35c0284a874e88317046ea/8-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/810ad5fa0fd7baa93e35c0284a874e88317046ea/8-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/810ad5fa0fd7baa93e35c0284a874e88317046ea/9-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/810ad5fa0fd7baa93e35c0284a874e88317046ea/9-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/810ad5fa0fd7baa93e35c0284a874e88317046ea/10-Figure11-1.png"],"pdf":"https://dl.acm.org/doi/pdf/10.1145/3332165.3347902"},{"type":"inproceedings","key":"makhataeva2019safety","title":"Safety Aura Visualization for Variable Impedance Actuated Robots","author":"Makhataeva, Zhanat and Zhakatayev, Altay and Varol, Huseyin Atakan","booktitle":"2019 IEEE/SICE International Symposium on System Integration (SII)","pages":"805--810","year":"2019","organization":"IEEE","doi":"10.1109/SII.2019.8700332","authors":["Zhanat Makhataeva","Altay Zhakatayev","Huseyin Atakan Varol"],"dblp":"conf/sii/MakhataevaZV19","venue":"2019 IEEE/SICE International Symposium on System Integration (SII)","abstract":"This paper presents an augmented reality-based method to increase human cognitive awareness of the danger/safety in the workspace of a variable impedance actuated (VIA) robot. Specifically, the \\"safety aura\\" around the robot was created using a safety/index defined for each point of the workspace. The safety-index is a composite metric incorporating the measurements of the distance from a point in the workspace to the VIA robot and the corresponding velocity during its motion. The safety aura is task-specific and is able to represent the safety levels in the workspace of the robot as it follows different trajectories. We developed a robot simulator which allows the visualization of the robot\u2019s motion and assessment of the corresponding safety aura in 3D space. The generated safety aura was overlaid into mixed reality using Unity 3D platform together with Vuforia and web camera. Simulation results demonstrated the ability of the safety metric based on distance and velocity measurements to qualitatively assess the amount of danger. Our method has the potential to make the physical human robot interaction safer.","paperId":"a67843e6a223e7b978ce297376f4c3520e87cb30","paperTitle":"Safety Aura Visualization for Variable Impedance Actuated Robots","paperUrl":"https://www.semanticscholar.org/paper/a67843e6a223e7b978ce297376f4c3520e87cb30","images":["https://d3i71xaburhd42.cloudfront.net/a67843e6a223e7b978ce297376f4c3520e87cb30/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/a67843e6a223e7b978ce297376f4c3520e87cb30/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/a67843e6a223e7b978ce297376f4c3520e87cb30/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/a67843e6a223e7b978ce297376f4c3520e87cb30/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/a67843e6a223e7b978ce297376f4c3520e87cb30/6-Figure5-1.png"],"pdf":"https://doi.org/10.1109/SII.2019.8700332"},{"type":"inproceedings","key":"le2016zooids","title":"Zooids: Building Blocks for Swarm User Interfaces","author":"Le Goc, Mathieu and Kim, Lawrence H and Parsaei, Ali and Fekete, Jean-Daniel and Dragicevic, Pierre and Follmer, Sean","booktitle":"Proceedings of the 29th Annual Symposium on User Interface Software and Technology","pages":"97--109","year":"2016","doi":"10.1145/2984511.2984547","authors":["Mathieu Le Goc","Lawrence H Kim","Ali Parsaei","Jean-Daniel Fekete","Pierre Dragicevic","Sean Follmer"],"dblp":"conf/uist/GocKPFDF16","venue":"UIST","abstract":"This paper introduces swarm user interfaces, a new class of human-computer interfaces comprised of many autonomous robots that handle both display and interaction. We describe the design of Zooids, an open-source open-hardware platform for developing tabletop swarm interfaces. The platform consists of a collection of custom-designed wheeled micro robots each 2.6 cm in diameter, a radio base-station, a high-speed DLP structured light projector for optical tracking, and a software framework for application development and control. We illustrate the potential of tabletop swarm user interfaces through a set of application scenarios developed with Zooids, and discuss general design considerations unique to swarm user interfaces.","paperId":"a68fc8871593e3fc9b0e5d754f533ba8b89c8c83","paperTitle":"Zooids: Building Blocks for Swarm User Interfaces","paperUrl":"https://www.semanticscholar.org/paper/a68fc8871593e3fc9b0e5d754f533ba8b89c8c83","images":["https://d3i71xaburhd42.cloudfront.net/a68fc8871593e3fc9b0e5d754f533ba8b89c8c83/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/a68fc8871593e3fc9b0e5d754f533ba8b89c8c83/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/a68fc8871593e3fc9b0e5d754f533ba8b89c8c83/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/a68fc8871593e3fc9b0e5d754f533ba8b89c8c83/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/a68fc8871593e3fc9b0e5d754f533ba8b89c8c83/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/a68fc8871593e3fc9b0e5d754f533ba8b89c8c83/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/a68fc8871593e3fc9b0e5d754f533ba8b89c8c83/7-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/a68fc8871593e3fc9b0e5d754f533ba8b89c8c83/7-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/a68fc8871593e3fc9b0e5d754f533ba8b89c8c83/7-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/a68fc8871593e3fc9b0e5d754f533ba8b89c8c83/7-Figure11-1.png"],"pdf":"https://hal.inria.fr/hal-01391281/file/SwarmUIs_no_copyright.pdf"},{"type":"inproceedings","key":"nowacka2013touchbugs","title":"Touchbugs: Actuated Tangibles on Multi-Touch Tables","author":"Nowacka, Diana and Ladha, Karim and Hammerla, Nils Y and Jackson, Daniel and Ladha, Cassim and Rukzio, Enrico and Olivier, Patrick","booktitle":"Proceedings of the SIGCHI conference on human factors in computing systems","pages":"759--762","year":"2013","doi":"10.1145/2470654.2470761","authors":["Diana Nowacka","Karim Ladha","Nils Y Hammerla","Daniel Jackson","Cassim Ladha","Enrico Rukzio","Patrick Olivier"],"dblp":"conf/chi/NowackaLHJLRO13","venue":"CHI","abstract":"We present a novel approach to graspable interfaces using Touchbugs, actuated physical objects for interacting with interactive surface computing applications. Touchbugs are active tangibles that are able to move across surfaces by employing vibrating motors and can communicate with camera based multi-touch surfaces using infrared LEDs. Touchbug\'s embedded inertial sensors and computational capabilities open a new interaction space by providing autonomous capabilities for tangibles that allow goal directed behavior.","paperId":"d2ab123b73e2ff1ecd3f324c6944463b1cfda6d3","paperTitle":"Touchbugs: actuated tangibles on multi-touch tables","paperUrl":"https://www.semanticscholar.org/paper/d2ab123b73e2ff1ecd3f324c6944463b1cfda6d3","images":["https://d3i71xaburhd42.cloudfront.net/d2ab123b73e2ff1ecd3f324c6944463b1cfda6d3/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/d2ab123b73e2ff1ecd3f324c6944463b1cfda6d3/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/d2ab123b73e2ff1ecd3f324c6944463b1cfda6d3/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/d2ab123b73e2ff1ecd3f324c6944463b1cfda6d3/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/d2ab123b73e2ff1ecd3f324c6944463b1cfda6d3/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/d2ab123b73e2ff1ecd3f324c6944463b1cfda6d3/3-Figure6-1.png"],"pdf":"http://www.researchgate.net/profile/Enrico_Rukzio/publication/262168032_Touchbugs_actuated_tangibles_on_multi-touch_tables/links/0046353c8ee4258b40000000.pdf"},{"type":"incollection","key":"kato2009multi","title":"Multi-Touch Interface for Controlling Multiple Mobile Robots","author":"Kato, Jun and Sakamoto, Daisuke and Inami, Masahiko and Igarashi, Takeo","booktitle":"CHI\'09 Extended Abstracts on Human Factors in Computing Systems","pages":"3443--3448","year":"2009","doi":"10.1145/1520340.1520500","authors":["Jun Kato","Daisuke Sakamoto","Masahiko Inami","Takeo Igarashi"],"dblp":"conf/chi/KatoSII09","venue":"CHI Extended Abstracts","abstract":"We must give some form of a command to robots in order to have the robots do a complex task. An initial instruction is required even if they do their tasks autonomously. We therefore need interfaces for the operation and teaching of robots. Natural languages, joysticks, and other pointing devices are currently used for this purpose. These interfaces, however, have difficulty in operating multiple robots simultaneously. We developed a multi-touch interface with a top-down view from a ceiling camera for controlling multiple mobile robots. The user specifies a vector field followed by all robots on the view. This paper describes the user interface and its implementation, and future work of the project.","paperId":"646b6d715e26e1b70387417930d7eeea50b281cc","paperTitle":"Multi-touch interface for controlling multiple mobile robots","paperUrl":"https://www.semanticscholar.org/paper/646b6d715e26e1b70387417930d7eeea50b281cc","images":["https://d3i71xaburhd42.cloudfront.net/646b6d715e26e1b70387417930d7eeea50b281cc/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/646b6d715e26e1b70387417930d7eeea50b281cc/4-Figure2-1.png"],"pdf":"http://www.cs.ucf.edu/courses/cap6105/fall09/readings/multi_robot.pdf"},{"type":"inproceedings","key":"urbani2018exploring","title":"Exploring Augmented Reality Interaction for Everyday Multipurpose Wearable Robots","author":"Urbani, Jaryd and Al-Sada, Mohammed and Nakajima, Tatsuo and H\\"oglund, Thomas","booktitle":"2018 IEEE 24th International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA)","pages":"209--216","year":"2018","organization":"IEEE","doi":"10.1109/RTCSA.2018.00033","authors":["Jaryd Urbani","Mohammed Al-Sada","Tatsuo Nakajima","Thomas H\\"oglund"],"dblp":"conf/rtcsa/UrbaniANH18","venue":"2018 IEEE 24th International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA)","abstract":"Multipurpose wearable robots are an emerging class of devices, which offer intriguing interaction potential. The interoperability of multipurpose wearable robots and other types of wearable devices remains largely uninvestigated. We take the first steps to bridge this gap by presenting a framework for integrating augmented reality (AR) and multipurpose wearable robots. Our framework uses the publisher-subscriber model to expose different robot functionalities as services on a network. These can be invoked by the AR system. This model is advantageous in coping with different robot morphologies and various interaction methods. We implemented a prototype system using our framework by integrating an AR head-mounted display (HMD) and a wrist-worn robot, and demonstrate four experiences utilizing our prototype solution: 1) Robot status display, 2) shape-changing menus, 3) a media player and 4) a robot pose controller. To evaluate our approach, we performed a user study, which gauged user impressions and usability of developed experiences. Results indicate that our approach was well received, though participants highlighted a number of challenges in AR tracking when interacting within some of the experiences. Lastly, we discuss limitations and future research direction for our project.","paperId":"ff95ffba2b209af42fe3002b8876cae12e7da257","paperTitle":"Exploring Augmented Reality Interaction for Everyday Multipurpose Wearable Robots","paperUrl":"https://www.semanticscholar.org/paper/ff95ffba2b209af42fe3002b8876cae12e7da257","images":["https://d3i71xaburhd42.cloudfront.net/ff95ffba2b209af42fe3002b8876cae12e7da257/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/ff95ffba2b209af42fe3002b8876cae12e7da257/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/ff95ffba2b209af42fe3002b8876cae12e7da257/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/ff95ffba2b209af42fe3002b8876cae12e7da257/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/ff95ffba2b209af42fe3002b8876cae12e7da257/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/ff95ffba2b209af42fe3002b8876cae12e7da257/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/ff95ffba2b209af42fe3002b8876cae12e7da257/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/ff95ffba2b209af42fe3002b8876cae12e7da257/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/ff95ffba2b209af42fe3002b8876cae12e7da257/5-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/ff95ffba2b209af42fe3002b8876cae12e7da257/5-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/ff95ffba2b209af42fe3002b8876cae12e7da257/6-Figure11-1.png"],"pdf":"http://doi.ieeecomputersociety.org/10.1109/RTCSA.2018.00033"},{"type":"article","key":"li2019augmented","title":"An Augmented Reality Based Human-Robot Interaction Interface Using Kalman Filter Sensor Fusion","author":"Li, Chunxu and Fahmy, Ashraf and Sienz, Johann","journal":"Sensors","volume":"19","number":"20","pages":"4586","year":"2019","publisher":"Multidisciplinary Digital Publishing Institute","doi":"10.3390/s19204586","authors":["Chunxu Li","Ashraf Fahmy","Johann Sienz"],"dblp":"journals/sensors/LiFS19","venue":"Sensors","abstract":"In this paper, the application of Augmented Reality (AR) for the control and adjustment of robots has been developed, with the aim of making interaction and adjustment of robots easier and more accurate from a remote location. A LeapMotion sensor based controller has been investigated to track the movement of the operator hands. The data from the controller allows gestures and the position of the hand palm\u2019s central point to be detected and tracked. A Kinect V2 camera is able to measure the corresponding motion velocities in x, y, z directions after our investigated post-processing algorithm is fulfilled. Unreal Engine 4 is used to create an AR environment for the user to monitor the control process immersively. Kalman filtering (KF) algorithm is employed to fuse the position signals from the LeapMotion sensor with the velocity signals from the Kinect camera sensor, respectively. The fused/optimal data are sent to teleoperate a Baxter robot in real-time by User Datagram Protocol (UDP). Several experiments have been conducted to test the validation of the proposed method.","paperId":"3b5c493fd7550432f30ec488b008199e19a288a7","paperTitle":"An Augmented Reality Based Human-Robot Interaction Interface Using Kalman Filter Sensor Fusion","paperUrl":"https://www.semanticscholar.org/paper/3b5c493fd7550432f30ec488b008199e19a288a7","images":["https://d3i71xaburhd42.cloudfront.net/3456df751053c07271f6ce0a2dc587589de22bfc/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/3456df751053c07271f6ce0a2dc587589de22bfc/15-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/3456df751053c07271f6ce0a2dc587589de22bfc/5-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/3456df751053c07271f6ce0a2dc587589de22bfc/6-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/3456df751053c07271f6ce0a2dc587589de22bfc/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/3456df751053c07271f6ce0a2dc587589de22bfc/7-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/3456df751053c07271f6ce0a2dc587589de22bfc/8-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/3456df751053c07271f6ce0a2dc587589de22bfc/11-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/3456df751053c07271f6ce0a2dc587589de22bfc/12-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/3456df751053c07271f6ce0a2dc587589de22bfc/13-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/3456df751053c07271f6ce0a2dc587589de22bfc/14-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/3456df751053c07271f6ce0a2dc587589de22bfc/14-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/3456df751053c07271f6ce0a2dc587589de22bfc/14-Figure12-1.png"],"pdf":null},{"type":"inproceedings","key":"morita2020extension","title":"Extension of Projection Area Using Head Orientation in Projected Virtual Hand Interface for Wheelchair Users","author":"Morita, Kohei and Hiraki, Takefumi and Matsukura, Haruka and Iwai, Daisuke and Sato, Kosuke","booktitle":"2020 59th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE)","pages":"421--426","year":"2020","organization":"IEEE","doi":"10.23919/SICE48898.2020.9240271","authors":["Kohei Morita","Takefumi Hiraki","Haruka Matsukura","Daisuke Iwai","Kosuke Sato"],"dblp":"conf/sice/MoritaHMIS20","venue":"2020 59th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE)","abstract":"A projected virtual hand interface, which visually extends arms of users using projection images, enables wheelchair users to reach unreachable objects; however, its projection area is narrower than the reaching area required for the users. To address this problem, we propose a wheelchair system enhanced with a projected virtual hand that allows controlling a projection area using user\u2019s head orientation. The proposed system estimates the current orientation of a user\u2019s head and controls the pan and tilt of a projector accordingly to move a projection area considering the positional relationship with a projection plane. As users can operate the projection area simply by turning their head, this operation can be executed simultaneously with operations of a virtual hand using their hands. We propose a control model for projector rotation according to the user\u2019s head direction. The conducted user experience study has revealed that the proposed method enables users to perform pointing tasks in a shorter time compared with the existing method, and moreover, it has acceptable interface usability.","paperId":"2d09f6f432255568a166d3323163ad60769ca611","paperTitle":"Extension of Projection Area using Head Orientation in Projected Virtual Hand Interface for Wheelchair Users","paperUrl":"https://www.semanticscholar.org/paper/2d09f6f432255568a166d3323163ad60769ca611","images":["https://d3i71xaburhd42.cloudfront.net/2d09f6f432255568a166d3323163ad60769ca611/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/2d09f6f432255568a166d3323163ad60769ca611/4-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/2d09f6f432255568a166d3323163ad60769ca611/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/2d09f6f432255568a166d3323163ad60769ca611/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/2d09f6f432255568a166d3323163ad60769ca611/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/2d09f6f432255568a166d3323163ad60769ca611/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/2d09f6f432255568a166d3323163ad60769ca611/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/2d09f6f432255568a166d3323163ad60769ca611/5-Figure7-1.png"],"pdf":"https://takefumihiraki.com/wp-content/uploads/2020/12/morita_SICE2020.pdf"},{"type":"inproceedings","key":"ochiai2011homunculus","title":"Homunculus: The Vehicle as Augmented Clothes","author":"Ochiai, Yoichi and Toyoshima, Keisuke","booktitle":"Proceedings of the 2nd Augmented Human International Conference","pages":"1--4","year":"2011","doi":"10.1145/1959826.1959829","authors":["Yoichi Ochiai","Keisuke Toyoshima"],"dblp":"conf/aughuman/OchiaiT11","venue":"AH \'11","abstract":"In this paper we propose to add a new system with valuable functionalities to vehicles. We call it \\"Homunculus\\". It is based on a new concept of interactions between humans and vehicles. It promotes and augments nonverbal communicability of humans in the vehicles.\\n It is difficult to communicate with the drivers in the vehicles by eye contact, hand gestures or touching behavior. Our \\"Homunculus\\" is a system to solve these problems. The instruments of \\"Homunculus\\" are composed of three system modules. The First is Robotic Eyes System which is a set of robotic eyes that follows drivers eye movements & head rotations. The Second is Projection System which shows drivers hand gestures on the road. The Third is Haptic Communication System which consists of IR Distance Sensors Array on the vehicle and Vibration motors attached to the driver. It gives drivers the haptic sense to approaching objects to the vehicle. These three Systems are set on vehicle\'s hood or side.\\n We propose the situation that humans and vehicles can be unified as one unit by Homunculus. This system works as a middleman for communications between men and vehicles, people in other cars, or even people just walking the street. We suggest the new relationship of men and their vehicles could be like men and their clothes.","paperId":"508d053d03326f988ac8e227358405fc54011555","paperTitle":"Homunculus: the vehicle as augmented clothes","paperUrl":"https://www.semanticscholar.org/paper/508d053d03326f988ac8e227358405fc54011555","images":["https://d3i71xaburhd42.cloudfront.net/508d053d03326f988ac8e227358405fc54011555/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/508d053d03326f988ac8e227358405fc54011555/3-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/508d053d03326f988ac8e227358405fc54011555/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/508d053d03326f988ac8e227358405fc54011555/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/508d053d03326f988ac8e227358405fc54011555/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/508d053d03326f988ac8e227358405fc54011555/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/508d053d03326f988ac8e227358405fc54011555/3-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/508d053d03326f988ac8e227358405fc54011555/4-Figure7-1.png"],"pdf":"https://doi.org/10.1145/1959826.1959829"},{"type":"inproceedings","key":"cauchard2019drone","title":"Drone. Io: A Gestural and Visual Interface for Human-Drone Interaction","author":"Cauchard, Jessica R and Tamkin, Alex and Wang, Cheng Yao and Vink, Luke and Park, Michelle and Fang, Tommy and Landay, James A","booktitle":"2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","pages":"153--162","year":"2019","organization":"IEEE","doi":"10.1109/HRI.2019.8673011","authors":["Jessica R Cauchard","Alex Tamkin","Cheng Yao Wang","Luke Vink","Michelle Park","Tommy Fang","James A Landay"],"dblp":"conf/hri/CauchardTWVPFL19","venue":"2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","abstract":"Drones are becoming ubiquitous and offer support to people in various tasks, such as photography, in increasingly interactive social contexts. We introduce drone.io, a projected body-centric graphical user interface for human-drone interaction. Using two simple gestures, users can interact with a drone in a natural manner. drone.io is the first human-drone graphical user interface embedded on a drone to provide both input and output capabilities. This paper describes the design process of drone.io. We present a proof of concept, drone-based implementation, as well as a fully functional prototype for a drone tour-guide scenario. We report drone.io\'s evaluation in three user studies $(\\\\mathbf{N}=27)$ and show that people were able to use the interface with little prior training. We contribute to the field of human-robot interaction and the growing field of human-drone interaction.","paperId":"ca93bfec1199c46459bcdfff74d5dfc746bc4f7f","paperTitle":"Drone.io: A Gestural and Visual Interface for Human-Drone Interaction","paperUrl":"https://www.semanticscholar.org/paper/ca93bfec1199c46459bcdfff74d5dfc746bc4f7f","images":["https://d3i71xaburhd42.cloudfront.net/ca93bfec1199c46459bcdfff74d5dfc746bc4f7f/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/ca93bfec1199c46459bcdfff74d5dfc746bc4f7f/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/ca93bfec1199c46459bcdfff74d5dfc746bc4f7f/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/ca93bfec1199c46459bcdfff74d5dfc746bc4f7f/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/ca93bfec1199c46459bcdfff74d5dfc746bc4f7f/5-TableI-1.png","https://d3i71xaburhd42.cloudfront.net/ca93bfec1199c46459bcdfff74d5dfc746bc4f7f/6-TableII-1.png"],"pdf":"https://doi.org/10.1109/HRI.2019.8673011"},{"type":"inproceedings","key":"hedayati2018improving","title":"Improving Collocated Robot Teleoperation with Augmented Reality","author":"Hedayati, Hooman and Walker, Michael and Szafir, Daniel","booktitle":"Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction","pages":"78--86","year":"2018","doi":"10.1145/3171221.3171251","authors":["Hooman Hedayati","Michael Walker","Daniel Szafir"],"dblp":"conf/hri/HedayatiWS18","venue":"2018 13th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","abstract":"Robot teleoperation can be a challenging task, often requiring a great deal of user training and expertise, especially for platforms with high degrees-of-freedom (e.g., industrial manipulators and aerial robots). Users often struggle to synthesize information robots collect (e.g., a camera stream) with contextual knowledge of how the robot is moving in the environment. We explore how advances in augmented reality (AR) technologies are creating a new design space for mediating robot teleoperation by enabling novel forms of intuitive, visual feedback. We prototype several aerial robot teleoperation interfaces using AR, which we evaluate in a 48-participant user study where participants completed an environmental inspection task. Our new interface designs provided several objective and subjective performance benefits over existing systems, which often force users into an undesirable paradigm that divides user attention between monitoring the robot and monitoring the robot\u2019s camera feed(s).","paperId":"5627bcd7b6859735d2479c62ae4326dbb4a59e3f","paperTitle":"Improving Collocated Robot Teleoperation with Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/5627bcd7b6859735d2479c62ae4326dbb4a59e3f","images":["https://d3i71xaburhd42.cloudfront.net/5627bcd7b6859735d2479c62ae4326dbb4a59e3f/5-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/5627bcd7b6859735d2479c62ae4326dbb4a59e3f/7-Figure3-1.png"],"pdf":"http://atlas.colorado.edu/iron/pdf/p78-hedayati.pdf"},{"type":"inproceedings","key":"qian2019augmented","title":"Augmented Reality Assisted Instrument Insertion and Tool Manipulation for the First Assistant in Robotic Surgery","author":"Qian, Long and Deguet, Anton and Wang, Zerui and Liu, Yun-Hui and Kazanzides, Peter","booktitle":"2019 International Conference on Robotics and Automation (ICRA)","pages":"5173--5179","year":"2019","organization":"IEEE","doi":"10.1109/ICRA.2019.8794263","authors":["Long Qian","Anton Deguet","Zerui Wang","Yun-Hui Liu","Peter Kazanzides"],"dblp":"conf/icra/QianDWLK19","venue":"2019 International Conference on Robotics and Automation (ICRA)","abstract":"In robotic-assisted laparoscopic surgery, the first assistant (FA) stands at the bedside assisting the intervention, while the surgeon sits at the console teleoperating the robot. Tasks for the FA include navigating new instruments into the surgeon\u2019s field-of-view and passing in or retracting materials from the body using hand-held tools. We previously developed ARssist, an augmented reality application based on an optical see-through head-mounted display, to aid the FA. In this paper, we refine the system and first perform a pilot study with three experienced surgeons for two specific tasks: instrument insertion and tool manipulation. The results suggest that ARssist would be especially useful for less experienced assistants and for difficult hand-eye configurations. We then perform a multi-user study with inexperienced subjects. The results show that ARssist can reduce navigation time by 34.57%, enhance insertion path consistency by 41.74%, reduce root-mean-square path deviation by 40.04%, and reduce tool manipulation time by 72.25%. Thus, ARssist has the potential to improve efficiency, safety and hand-eye coordination, especially for novice assistants.","paperId":"02fd8331f4219cfca9e5cfd57365ddb546e435eb","paperTitle":"Augmented Reality Assisted Instrument Insertion and Tool Manipulation for the First Assistant in Robotic Surgery","paperUrl":"https://www.semanticscholar.org/paper/02fd8331f4219cfca9e5cfd57365ddb546e435eb","images":["https://d3i71xaburhd42.cloudfront.net/02fd8331f4219cfca9e5cfd57365ddb546e435eb/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/02fd8331f4219cfca9e5cfd57365ddb546e435eb/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/02fd8331f4219cfca9e5cfd57365ddb546e435eb/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/02fd8331f4219cfca9e5cfd57365ddb546e435eb/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/02fd8331f4219cfca9e5cfd57365ddb546e435eb/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/02fd8331f4219cfca9e5cfd57365ddb546e435eb/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/02fd8331f4219cfca9e5cfd57365ddb546e435eb/6-Figure8-1.png"],"pdf":"https://doi.org/10.1109/ICRA.2019.8794263"},{"type":"inproceedings","key":"gao2019pati","title":"PATI: A Projection-Based Augmented Table-Top Interface for Robot Programming","author":"Gao, Yuxiang and Huang, Chien-Ming","booktitle":"Proceedings of the 24th international conference on intelligent user interfaces","pages":"345--355","year":"2019","doi":"10.1145/3301275.3302326","authors":["Yuxiang Gao","Chien-Ming Huang"],"dblp":"conf/iui/GaoH19","venue":"IUI","abstract":"As robots begin to provide daily assistance to individuals in human environments, their end-users, who do not necessarily have substantial technical training or backgrounds in robotics or programming, will ultimately need to program and \\"re-task\\" their robots to perform a variety of custom tasks. In this work, we present PATI---a Projection-based Augmented Table-top Interface for robot programming---through which users are able to use simple, common gestures (e.g., pinch gestures) and tools (e.g., shape tools) to specify table-top manipulation tasks (e.g., pick-and-place) for a robot manipulator. PATI allows users to interact with the environment directly when providing task specifications; for example, users can utilize gestures and tools to annotate the environment with task-relevant information, such as specifying target landmarks and selecting objects of interest. We conducted a user study to compare PATI with a state-of-the-art, standard industrial method for end-user robot programming. Our results show that participants needed significantly less training time before they felt confident in using our system than they did for the industrial method. Moreover, participants were able to program a robot manipulator to complete a pick-and-place task significantly faster with PATI. This work indicates a new direction for end-user robot programming.","paperId":"602f4d61a9e4272767a42c48b8e2d44f6dfa0811","paperTitle":"PATI: a projection-based augmented table-top interface for robot programming","paperUrl":"https://www.semanticscholar.org/paper/602f4d61a9e4272767a42c48b8e2d44f6dfa0811","images":["https://d3i71xaburhd42.cloudfront.net/602f4d61a9e4272767a42c48b8e2d44f6dfa0811/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/602f4d61a9e4272767a42c48b8e2d44f6dfa0811/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/602f4d61a9e4272767a42c48b8e2d44f6dfa0811/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/602f4d61a9e4272767a42c48b8e2d44f6dfa0811/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/602f4d61a9e4272767a42c48b8e2d44f6dfa0811/7-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/602f4d61a9e4272767a42c48b8e2d44f6dfa0811/9-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/602f4d61a9e4272767a42c48b8e2d44f6dfa0811/10-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/602f4d61a9e4272767a42c48b8e2d44f6dfa0811/10-Figure9-1.png"],"pdf":"http://dl.acm.org/ft_gateway.cfm?id=3302326&type=pdf"},{"type":"inproceedings","key":"gronbaek2020kirigamitable","title":"KirigamiTable: Designing for Proxemic Transitions with a Shape-Changing Tabletop","author":"Gronbaek, Jens Emil and Rasmussen, Majken Kirkegaard and Halskov, Kim and Petersen, Marianne Graves","booktitle":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","pages":"1--15","year":"2020","doi":"10.1145/3313831.3376834","authors":["Jens Emil Gronbaek","Majken Kirkegaard Rasmussen","Kim Halskov","Marianne Graves Petersen"],"dblp":"conf/chi/GronbaekRHP20","venue":"CHI","abstract":"A core challenge in tabletop research is to support transitions between individual activities and team work. Shape-changing tabletops open up new opportunities for addressing this challenge. However, interaction design for shape-changing furniture is in its early stages - so far, research has mainly focused on triggering shape-changes, and less on the actual interface transitions. We present KirigamiTable - a novel actuated shape-changing tabletop for supporting transitions in collaborative work. Our work builds on the concept of Proxemic Transitions, considering the dynamic interplay between social interactions, interactive technologies and furniture. With KirigamiTable, we demonstrate the potential of interactions for proxemic transitions that combine transformation of shape and digital contents. We highlight challenges for shape-changing tabletops: initiating shape and content transformations, cooperative control, and anticipating shape-change. To address these challenges, we propose a set of novel interaction techniques, including shape-first and content-first interaction, cooperative gestures, and physical and digital preview of shape-changes.","paperId":"ac8d24bf8b32dc2e8e35d46ae66a88b1604195c9","paperTitle":"KirigamiTable: Designing for Proxemic Transitions with a Shape-Changing Tabletop","paperUrl":"https://www.semanticscholar.org/paper/ac8d24bf8b32dc2e8e35d46ae66a88b1604195c9","images":["https://d3i71xaburhd42.cloudfront.net/ac8d24bf8b32dc2e8e35d46ae66a88b1604195c9/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/ac8d24bf8b32dc2e8e35d46ae66a88b1604195c9/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/ac8d24bf8b32dc2e8e35d46ae66a88b1604195c9/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/ac8d24bf8b32dc2e8e35d46ae66a88b1604195c9/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/ac8d24bf8b32dc2e8e35d46ae66a88b1604195c9/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/ac8d24bf8b32dc2e8e35d46ae66a88b1604195c9/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/ac8d24bf8b32dc2e8e35d46ae66a88b1604195c9/6-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/ac8d24bf8b32dc2e8e35d46ae66a88b1604195c9/7-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/ac8d24bf8b32dc2e8e35d46ae66a88b1604195c9/7-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/ac8d24bf8b32dc2e8e35d46ae66a88b1604195c9/7-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/ac8d24bf8b32dc2e8e35d46ae66a88b1604195c9/7-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/ac8d24bf8b32dc2e8e35d46ae66a88b1604195c9/8-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/ac8d24bf8b32dc2e8e35d46ae66a88b1604195c9/8-Figure14-1.png","https://d3i71xaburhd42.cloudfront.net/ac8d24bf8b32dc2e8e35d46ae66a88b1604195c9/8-Figure15-1.png","https://d3i71xaburhd42.cloudfront.net/ac8d24bf8b32dc2e8e35d46ae66a88b1604195c9/9-Figure16-1.png","https://d3i71xaburhd42.cloudfront.net/ac8d24bf8b32dc2e8e35d46ae66a88b1604195c9/9-Figure17-1.png"],"pdf":"https://pure.au.dk/ws/files/178417199/KirigamiTable_CR_final.pdf"},{"type":"inproceedings","key":"peng2018roma","title":"RoMA: Interactive Fabrication with Augmented Reality and a Robotic 3D Printer","author":"Peng, Huaishu and Briggs, Jimmy and Wang, Cheng-Yao and Guo, Kevin and Kider, Joseph and Mueller, Stefanie and Baudisch, Patrick and Guimbreti`ere, Fran\xe7ois","booktitle":"Proceedings of the 2018 CHI conference on human factors in computing systems","pages":"1--12","year":"2018","doi":"10.1145/3173574.3174153","authors":["Huaishu Peng","Jimmy Briggs","Cheng-Yao Wang","Kevin Guo","Joseph Kider","Stefanie Mueller","Patrick Baudisch","Fran\xe7ois Guimbreti`ere"],"dblp":"conf/chi/PengBWGKMBG18","venue":"CHI","abstract":"We present the Robotic Modeling Assistant (RoMA), an interactive fabrication system providing a fast, precise, hands-on and in-situ modeling experience. As a designer creates a new model using RoMA AR CAD editor, features are constructed concurrently by a 3D printing robotic arm sharing the same design volume. The partially printed physical model then serves as a tangible reference for the designer as she adds new elements to her design. RoMA\'s proxemics-inspired handshake mechanism between the designer and the 3D printing robotic arm allows the designer to quickly interrupt printing to access a printed area or to indicate that the robot can take full control of the model to finish printing. RoMA lets users integrate real-world constraints into a design rapidly, allowing them to create well-proportioned tangible artifacts or to extend existing objects. We conclude by presenting the strengths and limitations of our current design.","paperId":"005a8bd324a000c5646fa278271223073a7dce30","paperTitle":"RoMA: Interactive Fabrication with Augmented Reality and a Robotic 3D Printer","paperUrl":"https://www.semanticscholar.org/paper/005a8bd324a000c5646fa278271223073a7dce30","images":["https://d3i71xaburhd42.cloudfront.net/005a8bd324a000c5646fa278271223073a7dce30/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/005a8bd324a000c5646fa278271223073a7dce30/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/005a8bd324a000c5646fa278271223073a7dce30/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/005a8bd324a000c5646fa278271223073a7dce30/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/005a8bd324a000c5646fa278271223073a7dce30/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/005a8bd324a000c5646fa278271223073a7dce30/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/005a8bd324a000c5646fa278271223073a7dce30/5-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/005a8bd324a000c5646fa278271223073a7dce30/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/005a8bd324a000c5646fa278271223073a7dce30/6-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/005a8bd324a000c5646fa278271223073a7dce30/6-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/005a8bd324a000c5646fa278271223073a7dce30/7-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/005a8bd324a000c5646fa278271223073a7dce30/7-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/005a8bd324a000c5646fa278271223073a7dce30/8-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/005a8bd324a000c5646fa278271223073a7dce30/9-Figure14-1.png"],"pdf":"http://dl.acm.org/ft_gateway.cfm?id=3174153&type=pdf"},{"type":"inproceedings","key":"takashima2016study","title":"Study and Design of a Shape-Shifting Wall Display","author":"Takashima, Kazuki and Oyama, Takafumi and Asari, Yusuke and Sharlin, Ehud and Greenberg, Saul and Kitamura, Yoshifumi","booktitle":"Proceedings of the 2016 ACM Conference on Designing Interactive Systems","pages":"796--806","year":"2016","doi":"10.1145/2901790.2901892","authors":["Kazuki Takashima","Takafumi Oyama","Yusuke Asari","Ehud Sharlin","Saul Greenberg","Yoshifumi Kitamura"],"dblp":"conf/ACMdis/TakashimaOASGK16","venue":"Conference on Designing Interactive Systems","abstract":"Wall displays almost universally assume a mostly flat and static shape. We ask two questions: Would people choose a flat display for a given interaction scenario and, if not, what are the display shapes they actually prefer? We conducted a design study around these two questions. Our results show that participants designed different screen shapes that varied based upon peoples\' distance from the display and the content shown. Shapes ranged primarily between flat, separated, concave, L-shape and convex displays. Based on our findings, we designed a dynamic display that changes to these and other configurations. Shape-shifting is controlled either by explicit interaction (where the display responds to hand gestures) or implicitly (where the display infers a shape based both on its content and the sensed positions of the people around it). Overall, we contribute: a study that motivates research on shape-shifting wall displays, and a shape-shifting display system that responds to explicit and implicit controls to match particular activities.","paperId":"c79f5578c788533056772602f2ca12847721d5fc","paperTitle":"Study and Design of a Shape-Shifting Wall Display","paperUrl":"https://www.semanticscholar.org/paper/c79f5578c788533056772602f2ca12847721d5fc","images":["https://d3i71xaburhd42.cloudfront.net/c79f5578c788533056772602f2ca12847721d5fc/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/c79f5578c788533056772602f2ca12847721d5fc/5-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/c79f5578c788533056772602f2ca12847721d5fc/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/c79f5578c788533056772602f2ca12847721d5fc/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/c79f5578c788533056772602f2ca12847721d5fc/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/c79f5578c788533056772602f2ca12847721d5fc/7-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/c79f5578c788533056772602f2ca12847721d5fc/8-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/c79f5578c788533056772602f2ca12847721d5fc/8-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/c79f5578c788533056772602f2ca12847721d5fc/9-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/c79f5578c788533056772602f2ca12847721d5fc/9-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/c79f5578c788533056772602f2ca12847721d5fc/10-Figure11-1.png"],"pdf":"http://grouplab.cpsc.ucalgary.ca/grouplab/uploads/Publications/Publications/2016-ShapeShifting.DIS.pdf"},{"type":"inproceedings","key":"lindlbauer2016combining","title":"Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance","author":"Lindlbauer, David and Gronbaek, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\\"uller, J\\"org","booktitle":"Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems","pages":"791--802","year":"2016","doi":"10.1145/2858036.2858457","authors":["David Lindlbauer","Jens Emil Gronbaek","Morten Birk","Kim Halskov","Marc Alexa","J\\"org M\\"uller"],"dblp":"conf/chi/LindlbauerGBHAM16","venue":"CHI","abstract":"We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects\' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.","paperId":"4f17c1850d28e68ba23d854a74be8a00c69eaa94","paperTitle":"Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance","paperUrl":"https://www.semanticscholar.org/paper/4f17c1850d28e68ba23d854a74be8a00c69eaa94","images":["https://d3i71xaburhd42.cloudfront.net/4f17c1850d28e68ba23d854a74be8a00c69eaa94/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/4f17c1850d28e68ba23d854a74be8a00c69eaa94/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/4f17c1850d28e68ba23d854a74be8a00c69eaa94/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/4f17c1850d28e68ba23d854a74be8a00c69eaa94/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/4f17c1850d28e68ba23d854a74be8a00c69eaa94/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/4f17c1850d28e68ba23d854a74be8a00c69eaa94/7-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/4f17c1850d28e68ba23d854a74be8a00c69eaa94/7-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/4f17c1850d28e68ba23d854a74be8a00c69eaa94/8-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/4f17c1850d28e68ba23d854a74be8a00c69eaa94/8-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/4f17c1850d28e68ba23d854a74be8a00c69eaa94/8-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/4f17c1850d28e68ba23d854a74be8a00c69eaa94/9-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/4f17c1850d28e68ba23d854a74be8a00c69eaa94/9-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/4f17c1850d28e68ba23d854a74be8a00c69eaa94/9-Figure14-1.png"],"pdf":"https://pure.au.dk/ws/files/107469097/p791_lindlbauer.pdf"},{"type":"inproceedings","key":"darbar2019dronesar","title":"DroneSAR: Extending Physical Spaces in Spatial Augmented Reality Using Projection on a Drone","author":"Darbar, Rajkumar and Roo, Joan Sol and Lain\'e, Thibault and Hachet, Martin","booktitle":"Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia","pages":"1--7","year":"2019","doi":"10.1145/3365610.3365631","authors":["Rajkumar Darbar","Joan Sol Roo","Thibault Lain\'e","Martin Hachet"],"dblp":"conf/mum/DarbarRLH19","venue":"MUM","abstract":"Spatial Augmented Reality (SAR) transforms real-world objects into interactive displays by projecting digital content using video projectors. SAR enables co-located collaboration immediately between multiple viewers without the need to wear any special glasses. Unfortunately, one major limitation of SAR is that visual content can only be projected onto its physical supports. As a result, displaying User Interfaces (UI) widgets such as menus and pop-up windows in SAR is very challenging. We are trying to address this limitation by extending SAR space in mid-air. In this paper, we propose Drone-SAR, which extends the physical space of SAR by projecting digital information dynamically on the tracked panels mounted on a drone. DroneSAR is a proof of concept of novel SAR User Interface (UI), which provides support for 2D widgets (i.e., label, menu, interactive tools, etc.) to enrich SAR interactive experience. We also describe the implementation details of our proposed approach.","paperId":"625d74a1ded3ffb392590e439ce889c7f10b24a5","paperTitle":"DroneSAR: extending physical spaces in spatial augmented reality using projection on a drone","paperUrl":"https://www.semanticscholar.org/paper/625d74a1ded3ffb392590e439ce889c7f10b24a5","images":["https://d3i71xaburhd42.cloudfront.net/625d74a1ded3ffb392590e439ce889c7f10b24a5/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/625d74a1ded3ffb392590e439ce889c7f10b24a5/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/625d74a1ded3ffb392590e439ce889c7f10b24a5/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/625d74a1ded3ffb392590e439ce889c7f10b24a5/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/625d74a1ded3ffb392590e439ce889c7f10b24a5/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/625d74a1ded3ffb392590e439ce889c7f10b24a5/6-Figure7-1.png"],"pdf":"https://hal.inria.fr/hal-02409351/file/mum19paper.pdf"},{"type":"incollection","key":"al2012furhat","title":"Furhat: A Back-Projected Human-like Robot Head for Multiparty Human-Machine Interaction","author":"Al Moubayed, Samer and Beskow, Jonas and Skantze, Gabriel and Granstr\\"om, Bj\\"orn","booktitle":"Cognitive behavioural systems","pages":"114--130","year":"2012","publisher":"Springer","doi":"10.1007/978-3-642-34584-5_9","authors":["Samer Al Moubayed","Jonas Beskow","Gabriel Skantze","Bj\\"orn Granstr\\"om"],"dblp":"conf/cost/MoubayedBSG11","venue":"COST 2102 Training School","abstract":"In this chapter, we first present a summary of findings from two previous studies on the limitations of using flat displays with embodied conversational agents (ECAs) in the contexts of face-to-face human-agent interaction. We then motivate the need for a three dimensional display of faces to guarantee accurate delivery of gaze and directional movements and present Furhat, a novel, simple, highly effective, and human-like back-projected robot head that utilizes computer animation to deliver facial movements, and is equipped with a pan-tilt neck. After presenting a detailed summary on why and how Furhat was built, we discuss the advantages of using optically projected animated agents for interaction. We discuss using such agents in terms of situatedness, environment, context awareness, and social, human-like face-to-face interaction with robots where subtle nonverbal and social facial signals can be communicated. At the end of the chapter, we present a recent application of Furhat as a multimodal multiparty interaction system that was presented at the London Science Museum as part of a robot festival,. We conclude the paper by discussing future developments, applications and opportunities of this technology.","paperId":"65c9b2cdf9ba8ba5696170a82fb5465ff1c3d4ea","paperTitle":"Furhat: A Back-Projected Human-Like Robot Head for Multiparty Human-Machine Interaction","paperUrl":"https://www.semanticscholar.org/paper/65c9b2cdf9ba8ba5696170a82fb5465ff1c3d4ea","images":["https://d3i71xaburhd42.cloudfront.net/65c9b2cdf9ba8ba5696170a82fb5465ff1c3d4ea/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/65c9b2cdf9ba8ba5696170a82fb5465ff1c3d4ea/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/65c9b2cdf9ba8ba5696170a82fb5465ff1c3d4ea/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/65c9b2cdf9ba8ba5696170a82fb5465ff1c3d4ea/6-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/65c9b2cdf9ba8ba5696170a82fb5465ff1c3d4ea/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/65c9b2cdf9ba8ba5696170a82fb5465ff1c3d4ea/8-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/65c9b2cdf9ba8ba5696170a82fb5465ff1c3d4ea/9-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/65c9b2cdf9ba8ba5696170a82fb5465ff1c3d4ea/10-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/65c9b2cdf9ba8ba5696170a82fb5465ff1c3d4ea/11-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/65c9b2cdf9ba8ba5696170a82fb5465ff1c3d4ea/13-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/65c9b2cdf9ba8ba5696170a82fb5465ff1c3d4ea/14-Figure11-1.png"],"pdf":"http://www.speech.kth.se/furhat/publication/Furhat_LNCS.pdf"},{"type":"inproceedings","key":"kasetani2015projection","title":"Projection Mapping by Mobile Projector Robot","author":"Kasetani, Misaki and Noguchi, Tomonobu and Yamazoe, Hirotake and Lee, Joo-Ho","booktitle":"2015 12th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)","pages":"13--17","year":"2015","organization":"IEEE","doi":"10.1109/URAI.2015.7358918","authors":["Misaki Kasetani","Tomonobu Noguchi","Hirotake Yamazoe","Joo-Ho Lee"],"dblp":"conf/urai/KasetaniNYL15","venue":"2015 12th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)","abstract":"In this paper, we propose a method to achieve projection mapping by mobile projector robot. Projection mapping technology become popular and is used in various situations. However, since most of the current projection mappings are based on pre-calibrated fixed projectors, projectable areas become small fixed regions. We have been researching Ubiquitous Display (UD) that is a mobile-projection robot and aims to support the users by projecting various information. In this paper, we propose projection mapping technology by the mobile projector robot. UD has sensors for self-localization, but their accuracy are not enough for achieving projection mapping. Thus, in our method, we first models projection target beforehand. Then, UD moves to the front of the target and obtains point cloud observations of the scene. After that, the pose and position of the model is estimated. By using the estimated pose and position, we can realize the accurate projection mapping based on the mobile projector. Preliminary experiment shows the effectiveness of the proposed method.","paperId":"c7f02521ca3c1d43a325e1358da406a07ce93aff","paperTitle":"Projection mapping by mobile projector robot","paperUrl":"https://www.semanticscholar.org/paper/c7f02521ca3c1d43a325e1358da406a07ce93aff","images":["https://d3i71xaburhd42.cloudfront.net/c7f02521ca3c1d43a325e1358da406a07ce93aff/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/c7f02521ca3c1d43a325e1358da406a07ce93aff/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/c7f02521ca3c1d43a325e1358da406a07ce93aff/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/c7f02521ca3c1d43a325e1358da406a07ce93aff/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/c7f02521ca3c1d43a325e1358da406a07ce93aff/3-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/c7f02521ca3c1d43a325e1358da406a07ce93aff/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/c7f02521ca3c1d43a325e1358da406a07ce93aff/4-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/c7f02521ca3c1d43a325e1358da406a07ce93aff/5-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/c7f02521ca3c1d43a325e1358da406a07ce93aff/5-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/c7f02521ca3c1d43a325e1358da406a07ce93aff/5-Figure12-1.png"],"pdf":"https://doi.org/10.1109/URAI.2015.7358918"},{"type":"inproceedings","key":"guo2009touch","title":"Touch and Toys: New Techniques for Interaction with a Remote Group of Robots","author":"Guo, Cheng and Young, James Everett and Sharlin, Ehud","booktitle":"Proceedings of the SIGCHI conference on human factors in computing systems","pages":"491--500","year":"2009","doi":"10.1145/1518701.1518780","authors":["Cheng Guo","James Everett Young","Ehud Sharlin"],"dblp":"conf/chi/GuoYS09","venue":"CHI","abstract":"Interaction with a remote team of robots in real time is a difficult human-robot interaction (HRI) problem exacerbated by the complications of unpredictable real-world environments, with solutions often resorting to a larger-than-desirable ratio of operators to robots. We present two innovative interfaces that allow a single operator to interact with a group of remote robots. Using a tabletop computer the user can configure and manipulate groups of robots directly by either using their fingers (touch) or by manipulating a set of physical toys (tangible user interfaces). We recruited participants to partake in a user study that required them to interact with a small group of remote robots in simple tasks, and present our findings as a set of design considerations.","paperId":"1b93e3b4f2eea9991876db43dc161c30eb2bed86","paperTitle":"Touch and toys: new techniques for interaction with a remote group of robots","paperUrl":"https://www.semanticscholar.org/paper/1b93e3b4f2eea9991876db43dc161c30eb2bed86","images":["https://d3i71xaburhd42.cloudfront.net/1b93e3b4f2eea9991876db43dc161c30eb2bed86/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/1b93e3b4f2eea9991876db43dc161c30eb2bed86/5-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/1b93e3b4f2eea9991876db43dc161c30eb2bed86/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/1b93e3b4f2eea9991876db43dc161c30eb2bed86/6-Table2-1.png","https://d3i71xaburhd42.cloudfront.net/1b93e3b4f2eea9991876db43dc161c30eb2bed86/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/1b93e3b4f2eea9991876db43dc161c30eb2bed86/7-Table3-1.png","https://d3i71xaburhd42.cloudfront.net/1b93e3b4f2eea9991876db43dc161c30eb2bed86/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/1b93e3b4f2eea9991876db43dc161c30eb2bed86/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/1b93e3b4f2eea9991876db43dc161c30eb2bed86/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/1b93e3b4f2eea9991876db43dc161c30eb2bed86/7-Figure7-1.png"],"pdf":"http://hci.cs.umanitoba.ca/assets/publication_files/p491-guo.pdf"},{"type":"inproceedings","key":"leithinger2013sublimate","title":"Sublimate: State-Changing Virtual and Physical Rendering to Augment Interaction with Shape Displays","author":"Leithinger, Daniel and Follmer, Sean and Olwal, Alex and Luescher, Samuel and Hogge, Akimitsu and Lee, Jinha and Ishii, Hiroshi","booktitle":"Proceedings of the SIGCHI conference on human factors in computing systems","pages":"1441--1450","year":"2013","doi":"10.1145/2470654.2466191","authors":["Daniel Leithinger","Sean Follmer","Alex Olwal","Samuel Luescher","Akimitsu Hogge","Jinha Lee","Hiroshi Ishii"],"dblp":"conf/chi/LeithingerFOLHLI13","venue":"CHI","abstract":"Recent research in 3D user interfaces pushes towards immersive graphics and actuated shape displays. Our work explores the hybrid of these directions, and we introduce sublimation and deposition, as metaphors for the transitions between physical and virtual states. We discuss how digital models, handles and controls can be interacted with as virtual 3D graphics or dynamic physical shapes, and how user interfaces can rapidly and fluidly switch between those representations. To explore this space, we developed two systems that integrate actuated shape displays and augmented reality (AR) for co-located physical shapes and 3D graphics. Our spatial optical see-through display provides a single user with head-tracked stereoscopic augmentation, whereas our handheld devices enable multi-user interaction through video seethrough AR. We describe interaction techniques and applications that explore 3D interaction for these new modalities. We conclude by discussing the results from a user study that show how freehand interaction with physical shape displays and co-located graphics can outperform wand-based interaction with virtual 3D graphics.","paperId":"5fa0c49fee42542db6554f8c000aeec05a874871","paperTitle":"Sublimate: state-changing virtual and physical rendering to augment interaction with shape displays","paperUrl":"https://www.semanticscholar.org/paper/5fa0c49fee42542db6554f8c000aeec05a874871","images":["https://d3i71xaburhd42.cloudfront.net/5fa0c49fee42542db6554f8c000aeec05a874871/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/5fa0c49fee42542db6554f8c000aeec05a874871/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/5fa0c49fee42542db6554f8c000aeec05a874871/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/5fa0c49fee42542db6554f8c000aeec05a874871/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/5fa0c49fee42542db6554f8c000aeec05a874871/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/5fa0c49fee42542db6554f8c000aeec05a874871/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/5fa0c49fee42542db6554f8c000aeec05a874871/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/5fa0c49fee42542db6554f8c000aeec05a874871/7-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/5fa0c49fee42542db6554f8c000aeec05a874871/7-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/5fa0c49fee42542db6554f8c000aeec05a874871/8-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/5fa0c49fee42542db6554f8c000aeec05a874871/9-Figure11-1.png"],"pdf":"https://dspace.mit.edu/bitstream/1721.1/80421/1/Ishii_Sublimate%20state-changing.pdf"},{"type":"incollection","key":"hiraki2019navigatorch","title":"NavigaTorch: Projection-Based Robot Control Interface Using High-Speed Handheld Projector","author":"Hiraki, Takefumi and Fukushima, Shogo and Kawahara, Yoshihiro and Naemura, Takeshi","booktitle":"SIGGRAPH Asia 2019 Emerging Technologies","pages":"31--33","year":"2019","doi":"10.1145/3355049.3360538","authors":["Takefumi Hiraki","Shogo Fukushima","Yoshihiro Kawahara","Takeshi Naemura"],"dblp":"conf/siggrapha/HirakiFKN19","venue":"SIGGRAPH ASIA Emerging Technologies","abstract":"We propose \u201cNavigaTorch\u201d, a projection-based robot control interface that enables the user to operate a robot quickly and intuitively. The user can control and navigate a robot from the third-person viewpoint using the projected video as visual feedback. We achieved flickerless image feedback and a quick response in robot operation by developing a handheld pixel-level visible light communication (PVLC) projector. Our contribution is the development of a robot control interface based on high-speed projection technology and the exploration of the design methodology of projection-based robot control using a handheld projector.","paperId":"a45d588743d58b702a85039426bc3b10c0f059a6","paperTitle":"NavigaTorch: Projection-based Robot Control Interface using High-speed Handheld Projector","paperUrl":"https://www.semanticscholar.org/paper/a45d588743d58b702a85039426bc3b10c0f059a6","images":["https://d3i71xaburhd42.cloudfront.net/a45d588743d58b702a85039426bc3b10c0f059a6/2-Figure2-1.png"],"pdf":"https://dl.acm.org/doi/pdf/10.1145/3355049.3360538"},{"type":"inproceedings","key":"suzuki2019shapebots","title":"Shapebots: Shape-Changing Swarm Robots","author":"Suzuki, Ryo and Zheng, Clement and Kakehi, Yasuaki and Yeh, Tom and Do, Ellen Yi-Luen and Gross, Mark D and Leithinger, Daniel","booktitle":"Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology","pages":"493--505","doi":"10.1145/3332165.3347911","year":"2019","authors":["Ryo Suzuki","Clement Zheng","Yasuaki Kakehi","Tom Yeh","Ellen Yi-Luen Do","Mark D Gross","Daniel Leithinger"],"dblp":"conf/uist/SuzukiZKYDGL19","venue":"UIST","abstract":"We introduce shape-changing swarm robots. A swarm of self-transformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions. The modular design of each actuator enables various shapes and geometries of self-transformation. We illustrate potential application scenarios and discuss how this type of interface opens up possibilities for the future of ubiquitous and distributed shape-changing interfaces.","paperId":"2b638759c060630c182bd398238ecd7b2385d27e","paperTitle":"ShapeBots: Shape-changing Swarm Robots","paperUrl":"https://www.semanticscholar.org/paper/2b638759c060630c182bd398238ecd7b2385d27e","images":["https://d3i71xaburhd42.cloudfront.net/2b638759c060630c182bd398238ecd7b2385d27e/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/2b638759c060630c182bd398238ecd7b2385d27e/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/2b638759c060630c182bd398238ecd7b2385d27e/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/2b638759c060630c182bd398238ecd7b2385d27e/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/2b638759c060630c182bd398238ecd7b2385d27e/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/2b638759c060630c182bd398238ecd7b2385d27e/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/2b638759c060630c182bd398238ecd7b2385d27e/5-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/2b638759c060630c182bd398238ecd7b2385d27e/5-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/2b638759c060630c182bd398238ecd7b2385d27e/5-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/2b638759c060630c182bd398238ecd7b2385d27e/6-Figure11-1.png","https://d3i71xaburhd42.cloudfront.net/2b638759c060630c182bd398238ecd7b2385d27e/6-Figure12-1.png","https://d3i71xaburhd42.cloudfront.net/2b638759c060630c182bd398238ecd7b2385d27e/7-Figure13-1.png","https://d3i71xaburhd42.cloudfront.net/2b638759c060630c182bd398238ecd7b2385d27e/7-Figure15-1.png","https://d3i71xaburhd42.cloudfront.net/2b638759c060630c182bd398238ecd7b2385d27e/7-Figure17-1.png","https://d3i71xaburhd42.cloudfront.net/2b638759c060630c182bd398238ecd7b2385d27e/7-Figure18-1.png","https://d3i71xaburhd42.cloudfront.net/2b638759c060630c182bd398238ecd7b2385d27e/8-Figure20-1.png","https://d3i71xaburhd42.cloudfront.net/2b638759c060630c182bd398238ecd7b2385d27e/8-Figure21-1.png","https://d3i71xaburhd42.cloudfront.net/2b638759c060630c182bd398238ecd7b2385d27e/8-Figure22-1.png","https://d3i71xaburhd42.cloudfront.net/2b638759c060630c182bd398238ecd7b2385d27e/8-Figure23-1.png","https://d3i71xaburhd42.cloudfront.net/2b638759c060630c182bd398238ecd7b2385d27e/9-Figure24-1.png"],"pdf":"http://arxiv.org/pdf/1909.03372"},{"type":"incollection","key":"siu2018investigating","title":"Investigating Tangible Collaboration for Design towards Augmented Physical Telepresence","author":"Siu, Alexa F and Yuan, Shenli and Pham, Hieu and Gonzalez, Eric and Kim, Lawrence H and Le Goc, Mathieu and Follmer, Sean","booktitle":"Design thinking research","pages":"131--145","year":"2018","publisher":"Springer","doi":"10.1007/978-3-319-60967-6_7","authors":["Alexa F Siu","Shenli Yuan","Hieu Pham","Eric Gonzalez","Lawrence H Kim","Mathieu Le Goc","Sean Follmer"],"venue":"","abstract":"While many systems have been designed to support collaboration around visual thinking tools, much less work has investigated how to share and collaboratively design physical prototypes\u2014an important part of the design process. We describe preliminary results from a formative study on how designers communicate and collaborate in design meetings around physical and digital artifacts. Addressing some limitations in current collaboration platforms and drawing guidelines from our study, we introduce a new prototype platform for remote collaboration. This platform leverages the use of augmented reality (AR) for rendering of the remote participant and a pair of linked actuated tabletop tangible interfaces that acts as the participant\u2019s shared physical workspace. We propose the use of actuated tabletop tangibles to synchronously render complex shapes and to act as physical input.","paperId":"0be89ab18cdc963cc00419ffbdc23d1044178d30","paperTitle":"Investigating Tangible Collaboration for Design Towards Augmented Physical Telepresence","paperUrl":"https://www.semanticscholar.org/paper/0be89ab18cdc963cc00419ffbdc23d1044178d30","images":[],"pdf":null},{"type":"article","key":"erat2018drone","title":"Drone-Augmented Human Vision: Exocentric Control for Drones Exploring Hidden Areas","author":"Erat, Okan and Isop, Werner Alexander and Kalkofen, Denis and Schmalstieg, Dieter","journal":"IEEE transactions on visualization and computer graphics","volume":"24","number":"4","pages":"1437--1446","year":"2018","publisher":"IEEE","doi":"10.1109/TVCG.2018.2794058","authors":["Okan Erat","Werner Alexander Isop","Denis Kalkofen","Dieter Schmalstieg"],"dblp":"journals/tvcg/EratIKS18","venue":"IEEE Transactions on Visualization and Computer Graphics","abstract":"Drones allow exploring dangerous or impassable areas safely from a distant point of view. However, flight control from an egocentric view in narrow or constrained environments can be challenging. Arguably, an exocentric view would afford a better overview and, thus, more intuitive flight control of the drone. Unfortunately, such an exocentric view is unavailable when exploring indoor environments. This paper investigates the potential of drone-augmented human vision, i.e., of exploring the environment and controlling the drone indirectly from an exocentric viewpoint. If used with a see-through display, this approach can simulate X-ray vision to provide a natural view into an otherwise occluded environment. The user\'s view is synthesized from a three-dimensional reconstruction of the indoor environment using image-based rendering. This user interface is designed to reduce the cognitive load of the drone\'s flight control. The user can concentrate on the exploration of the inaccessible space, while flight control is largely delegated to the drone\'s autopilot system. We assess our system with a first experiment showing how drone-augmented human vision supports spatial understanding and improves natural interaction with the drone.","paperId":"f60167d9d21181017604598aaedc432570de4458","paperTitle":"Drone-Augmented Human Vision: Exocentric Control for Drones Exploring Hidden Areas","paperUrl":"https://www.semanticscholar.org/paper/f60167d9d21181017604598aaedc432570de4458","images":["https://d3i71xaburhd42.cloudfront.net/f60167d9d21181017604598aaedc432570de4458/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/f60167d9d21181017604598aaedc432570de4458/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/f60167d9d21181017604598aaedc432570de4458/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/f60167d9d21181017604598aaedc432570de4458/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/f60167d9d21181017604598aaedc432570de4458/7-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/f60167d9d21181017604598aaedc432570de4458/8-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/f60167d9d21181017604598aaedc432570de4458/9-Figure8-1.png"],"pdf":"https://doi.org/10.1109/TVCG.2018.2794058"},{"type":"inproceedings","key":"young2007robot","title":"Robot Expressionism through Cartooning","author":"Young, James E and Xin, Min and Sharlin, Ehud","booktitle":"2007 2nd ACM/IEEE International Conference on Human-Robot Interaction (HRI)","pages":"309--316","year":"2007","organization":"IEEE","doi":"10.1145/1228716.1228758","authors":["James E Young","Min Xin","Ehud Sharlin"],"dblp":"conf/hri/YoungXS07","venue":"2007 2nd ACM/IEEE International Conference on Human-Robot Interaction (HRI)","abstract":"We present a new technique for human-robot interaction called robot expressionism through cartooning. We suggest that robots utilise cartoon-art techniques such as simplified and exaggerated facial expressions, stylised text, and icons for intuitive social interaction with humans. We discuss practical mixed reality solutions that allow robots to augment themselves or their surroundings with cartoon art content. Our effort is part of what we call robot expressionism, a conceptual approach to the design and analysis of robotic interfaces that focuses on providing intuitive insight into robotic states as well as the artistic quality of interaction. Our paper discusses a variety of ways that allow robots to use cartoon art and details a test bed design, implementation, and exploratory evaluation. We describe our test bed, Jeeves, which uses a Roomba, an iRobot vacuum cleaner robot, and a mixed-reality system as a platform for rapid prototyping of cartoon-art interfaces. Finally, we present a set of interaction content scenarios which use the Jeeves prototype: trash Roomba, the recycle police, and clean tracks, as well as initial exploratory evaluation of our approach.","paperId":"bc1103fef9394881babb088fbfd2d3ddb63539e8","paperTitle":"Robot expressionism through cartooning","paperUrl":"https://www.semanticscholar.org/paper/bc1103fef9394881babb088fbfd2d3ddb63539e8","images":["https://d3i71xaburhd42.cloudfront.net/bc1103fef9394881babb088fbfd2d3ddb63539e8/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/bc1103fef9394881babb088fbfd2d3ddb63539e8/5-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/bc1103fef9394881babb088fbfd2d3ddb63539e8/6-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/bc1103fef9394881babb088fbfd2d3ddb63539e8/7-Figure4-1.png"],"pdf":"http://dspace.ucalgary.ca/bitstream/1880/45633/2/2006-842-35.pdf"},{"type":"inproceedings","key":"jones2020vroom","title":"VROOM: Virtual Robot Overlay for Online Meetings","author":"Jones, Brennan and Zhang, Yaying and Wong, Priscilla NY and Rintel, Sean","booktitle":"Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems","pages":"1--10","year":"2020","doi":"10.1145/3334480.3382820","authors":["Brennan Jones","Yaying Zhang","Priscilla NY Wong","Sean Rintel"],"dblp":"conf/chi/JonesZWR20","venue":"CHI Extended Abstracts","abstract":"Telepresence robots allow users to freely explore a remote space and provide a physical embodiment in that space. However, they lack a compelling representation of the remote user in the local space. We present VROOM (Virtual Robot Overlay for Online Meetings), a two-way system for exploring how to improve the social experience of robotic telepresence. For the local user, an augmented-reality (AR) interface shows a life-size avatar of the remote user overlaid on a telepresence robot. For the remote user, a head-mounted virtual-reality (VR) interface presents an immersive 360\xb0 view of the local space with mobile autonomy. The VR system tracks the remote user\'s head pose and hand movements, which are applied to an avatar. This provides the remote user with an identifiable self-embodiment and allows the local user to see the remote user\'s head direction and arm gestures.","paperId":"d11b9dd4200b80bf523e7a836ec9a69da5af0502","paperTitle":"VROOM: Virtual Robot Overlay for Online Meetings","paperUrl":"https://www.semanticscholar.org/paper/d11b9dd4200b80bf523e7a836ec9a69da5af0502","images":["https://d3i71xaburhd42.cloudfront.net/88420ec13032ac6843133f5e8fe183435dd8d5df/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/88420ec13032ac6843133f5e8fe183435dd8d5df/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/88420ec13032ac6843133f5e8fe183435dd8d5df/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/88420ec13032ac6843133f5e8fe183435dd8d5df/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/88420ec13032ac6843133f5e8fe183435dd8d5df/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/88420ec13032ac6843133f5e8fe183435dd8d5df/6-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/88420ec13032ac6843133f5e8fe183435dd8d5df/6-Figure8-1.png"],"pdf":"https://www.microsoft.com/en-us/research/uploads/prod/2020/02/chi20e-sub1105-i7.pdf"},{"type":"inproceedings","key":"cha2018effects","title":"Effects of Robot Sound on Auditory Localization in Human-Robot Collaboration","author":"Cha, Elizabeth and Fitter, Naomi T and Kim, Yunkyung and Fong, Terrence and Matari\'c, Maja J","booktitle":"Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction","pages":"434--442","year":"2018","organization":"ACM","doi":"10.1145/3171221.3171285","authors":["Elizabeth Cha","Naomi T Fitter","Yunkyung Kim","Terrence Fong","Maja J Matari\'c"],"dblp":"conf/hri/ChaFKFM18","venue":"2018 13th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","abstract":"Auditory cues facilitate situational awareness by enabling humans to infer what is happening in the nearby environment. Unlike humans, many robots do not continuously produce perceivable state-expressive sounds. In this work, we propose the use of iconic auditory signals that mimic the sounds produced by a robot\u2019s operations. In contrast to artificial sounds (e.g., beeps and whistles), these signals are primarily functional, providing information about the robot\u2019s actions and state. We analyze the effects of two variations of robot sound, tonal and broadband, on auditory localization during a human-robot collaboration task. Results from 24 participants show that both signals significantly improve auditory localization, but the broadband variation is preferred by participants. We then present a computational formulation for auditory signaling and apply it to the problem of auditory localization using a human-subjects data collection with 18 participants to learn optimal signaling policies.","paperId":"f6ed5bdd1bc18b668c924438c111644e42b41bd6","paperTitle":"Effects of Robot Sound on Auditory Localization in Human-Robot Collaboration","paperUrl":"https://www.semanticscholar.org/paper/f6ed5bdd1bc18b668c924438c111644e42b41bd6","images":["https://d3i71xaburhd42.cloudfront.net/f6ed5bdd1bc18b668c924438c111644e42b41bd6/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/f6ed5bdd1bc18b668c924438c111644e42b41bd6/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/f6ed5bdd1bc18b668c924438c111644e42b41bd6/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/f6ed5bdd1bc18b668c924438c111644e42b41bd6/4-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/f6ed5bdd1bc18b668c924438c111644e42b41bd6/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/f6ed5bdd1bc18b668c924438c111644e42b41bd6/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/f6ed5bdd1bc18b668c924438c111644e42b41bd6/7-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/f6ed5bdd1bc18b668c924438c111644e42b41bd6/8-Figure8-1.png"],"pdf":"http://robotics.usc.edu/~echa/publications/hri_2018.pdf"},{"type":"misc","key":"nintendo-mklive","title":"Nintendo Mario Kart Live: Home Circuit","url":"https://mklive.nintendo.com/","year":"2020","lastaccessed":"on January 5, 2022","authors":[],"doi":"10.3138/CART.52.2.3823","dblp":"journals/cartographica/EdlerD17","venue":"Cartogr. Int. J. Geogr. Inf. Geovisualization","abstract":"The video game industry revolutionized the game market from the 1970s onwards. Stationary video game machines, such as \u201ccoin-ops\u201d and, later, consoles for home entertainment made it possible to experience and interact with new virtual environments. Based on technical innovations, early video games already included different graphic and auditory effects that were used to present and emphasize the spatial dimension of game stories. One of the most famous and successful video game series that \u201ctold\u201d spatial stories and included many visualizations of virtual topographies was Nintendo\u2019s Super Mario series. Nintendo developed diverse video game topographies including different interactive and animated cartographic media throughout the Super Mario series. These maps were early and fundamental examples that were user-friendly and suitable for children. Moreover, they established a basis for future video game spaces, and the techniques used to create, animate, and visualize these maps have also found their ways into other applications of cartography and geomatics. It seems that the early worlds of Super Mario animated cartographers to animate cartographic visualizations. This article presents the characteristic spatial structures and cartographic techniques found in early Super Mario games, from the arcade classic Donkey Kong (1981) to the Super Nintendo classic Super Mario Kart (1992). The meaning of these structures and techniques for other cartographic applications is discussed. R\xc9SUM\xc9: L\u2019industrie des jeux vid\xe9o a r\xe9volutionn\xe9 le march\xe9 des jeux depuis les ann\xe9es 1970. Les appareils de jeux vid\xe9o fixes, comme les \xab bornes d\u2019arcade \xbb, et les consoles de jeux domestiques par la suite ont permis d\u2019exp\xe9rimenter de nouveaux environnements virtuels et d\u2019interagir avec ces environnements. Gr\xe2ce aux innovations techniques, les premiers jeux vid\xe9o comportaient d\xe9j\xe0 diff\xe9rents \xe9l\xe9ments graphiques et auditifs servant \xe0 pr\xe9senter et \xe0 mettre en valeur la dimension spatiale des sc\xe9narios de jeu. L\u2019une des s\xe9ries vid\xe9oludiques les plus connues et les plus populaires proposant des sc\xe9narios spatiaux et de multiples visualisations de topographies virtuelles \xe9tait la s\xe9rie de Nintendo Super Mario. Nintendo a mis au point diverses topographies de jeux vid\xe9o, dont diff\xe9rents outils cartographiques interactifs et anim\xe9s dans la s\xe9rie Super Mario. Ces cartes \xe9taient la manifestation pr\xe9coce et fondamentale d\u2019un souci de convivialit\xe9 et d\u2019adaptation aux enfants. Elles jetaient en outre les bases des univers futurs des jeux vid\xe9o, et les techniques permettant de cr\xe9er, d\u2019animer et de visualiser ces cartes ont \xe9galement \xe9t\xe9 transpos\xe9es \xe0 d\u2019autres applications de la cartographie et de la g\xe9omatique. Il semble que les premiers pas de Super Mario aient incit\xe9 les cartographes \xe0 animer les repr\xe9sentations cartographiques. Les auteurs pr\xe9sentent les structures spatiales et les techniques cartographiques caract\xe9ristiques des premi\xe8res versions des jeux Super Mario, du classique des jeux d\u2019arcade Donkey Kong (1981) au classique du Super Nintendo Super Mario Kart (1992). Ils analysent la signification de ces structures et de ces techniques dans la perspective d\u2019autres applications techniques.","paperId":"13c2f6d783706d5b752058579f5819630b498f3e","paperTitle":"The Impact of 1980s and 1990s Video Games on Multimedia Cartography","paperUrl":"https://www.semanticscholar.org/paper/13c2f6d783706d5b752058579f5819630b498f3e","images":[],"pdf":"https://doi.org/10.3138/cart.52.2.3823"},{"type":"misc","key":"mercedes-f15","title":"The Mercedes-Benz F 015 Luxury in Motion","url":"https://www.mercedes-benz.com/en/innovation/autonomous/research-vehicle-f-015-luxury-in-motion/","year":"2015","lastaccessed":"on January 5, 2022","authors":[],"venue":"","abstract":null,"paperId":"5374dbdd964182625c00fc18d9bbc70afe3fc58a","paperTitle":"Mercedes-Benz F 015 Luxury in Motion, probamos el auto que no se conduce","paperUrl":"https://www.semanticscholar.org/paper/5374dbdd964182625c00fc18d9bbc70afe3fc58a","images":[],"pdf":null},{"type":"inproceedings","key":"abbas2012augmented","title":"Augmented Reality Based Teaching Pendant for Industrial Robot","author":"Abbas, Syed Mohsin and Hassan, Syed and Yun, Jongwon","booktitle":"2012 12th International Conference on Control, Automation and Systems","pages":"2210--2213","year":"2012","organization":"IEEE","authors":["Syed Mohsin Abbas","Syed Hassan","Jongwon Yun"],"venue":"2012 12th International Conference on Control, Automation and Systems","abstract":"Robots have been used in various fields such as homes, manufacturing, and business. Remote robot control has been actively studied. Most of industrial robots are still programmed using the typical teaching process, through the use of the robot teach pendant. New and more intuitive ways for robot programming and control are required, smart phone are being used for controlling and programming industrial robot. In this research work, we present an idea of augmented reality based teaching pendant on smart phone. Incorporation of augmented reality into Smartphone based teaching pendant will help user to program industrial robot more intuitively.","paperId":"ddc9f20711d6d18de8281bc027c15ff190f1a4b5","paperTitle":"Augmented reality based teaching pendant for industrial robot","paperUrl":"https://www.semanticscholar.org/paper/ddc9f20711d6d18de8281bc027c15ff190f1a4b5","images":["https://d3i71xaburhd42.cloudfront.net/ddc9f20711d6d18de8281bc027c15ff190f1a4b5/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/ddc9f20711d6d18de8281bc027c15ff190f1a4b5/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/ddc9f20711d6d18de8281bc027c15ff190f1a4b5/2-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/ddc9f20711d6d18de8281bc027c15ff190f1a4b5/2-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/ddc9f20711d6d18de8281bc027c15ff190f1a4b5/2-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/ddc9f20711d6d18de8281bc027c15ff190f1a4b5/3-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/ddc9f20711d6d18de8281bc027c15ff190f1a4b5/3-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/ddc9f20711d6d18de8281bc027c15ff190f1a4b5/3-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/ddc9f20711d6d18de8281bc027c15ff190f1a4b5/3-Figure8-2.png"],"pdf":null},{"type":"inproceedings","key":"filipenko2020virtual","title":"Virtual Commissioning with Mixed Reality for next-Generation Robot-Based Mechanical Component Testing","author":"Filipenko, Michael and Poeppel, Alexander and Hoffmann, Alwin and Reif, Wolfgang and Monden, Andreas and Sause, Markus","booktitle":"ISR 2020; 52th International Symposium on Robotics","pages":"1--6","year":"2020","organization":"VDE","authors":["Michael Filipenko","Alexander Poeppel","Alwin Hoffmann","Wolfgang Reif","Andreas Monden","Markus Sause"],"doi":"10.14236/EWIC/EVA2008.3","dblp":"conf/eva/BertonciniB08","venue":"EVA","abstract":"CALLAS project aims at designing and developing an integrated multimodal architecture able to include emotional aspects to support applications in the new media business scenario with an \u201cambient intelligence\u201d paradigm. The project is structured in three main areas: the \\"Shelf\\", collecting multimodal affective components (speech, facial expression and gesture recognition); the \\"Framework\\", a software infrastructure enabling the cooperation of multiple components with an easy interface addressed to final users; and three \\"Showcases\\" addressing three main fields of new media domain: AR art, Entertainment and Digital Theatre, Interactive Installation in public spaces and Next Generation Interactive TV. INTRODUCTION \u2013 THE CALLAS PROJECT CALLAS Conveying Affectiveness in Leading-edge Living Adaptive Systems is an Integrated Project founded by the European Commission within the 6 Framework Programme Information Society Technologies priority, in the strategic objective Multimodal Interfaces (2.5.7). The project started in November 2006 and will end in May 2010. The project consortium is composed of universities and private research laboratories working on multimodal applications, together with artists, broadcasts and theatres, involved as final users [1]. MULTIMODAL AFFECTIVE INTERFACES: OBJECTIVES AND DOMAIN In everyday life, human communication combines speech with gestures, movements, and non-verbal expressions: each of those communication channels is affected by emotions. Taking in consideration the role of emotions and affectiveness is therefore fundamental to enrich naturalness also in human-machine interaction and communication. The CALLAS project face the challenges of implementing innovative affective interfaces to comprehend emotional input within the domain of interactive media. Affective and emotional interfaces are generally concerned with the real-time identification of user emotions to determine system response. They usually rely on Ekmanian emotions such as anger, fear, sadness, enjoyment, disgust and surprise. The domain of interactive new media, such as interactive narratives, digital theatre or digital arts, involves different ranges of emotions on the user\u2019s side, some of which correspond to responses to aesthetic properties of the media, or characterize the user experience itself in terms of enjoyment and entertainment. To identify these ranges of EVA 2008 London Conference ~ 22-24 July Massimo Bertoncini Irene Buonazia _____________________________________________________________________ 20 emotions, more complex articulations of modalities are required across semantic dimensions as well as across temporal combinations. For instance, input from emotional language and paralinguistic speech (laughter, cries) must be categorized as indicators of user attention and must be integrated across interaction sessions of variable durations, instead of analyzing a single emotional status in real-time. The first scientific CALLAS objective is to advance the state-of-the-art in Multimodal Affective Interfaces, by creating new emotional models, able to take into account a comprehensive user experience in Digital Arts and Entertainment applications, and by developing new modalities to capture these new emotional categories. The main technological research (developed by the universities and research labs participating in the project) consists in the development and integration of advanced software components for semantic recognition of emotions. Those components will be available through a \u201cliving\u201d repository, called the CALLAS \u201cshelf\u201d. On the other side, the project (mainly with the effort of the software and engineering companies of the consortium) aims at establishing a software methodology for the development and the engineering of a \\"framework\\" for Multimodal Interfaces that will make their development accessible to a larger community of users (even without a deep understanding of the theories of Multimodality), represented in the consortium by theatres, broadcasts and digital artists. Finally, the effectiveness of CALLAS approach will be validated developing research prototypes in the domain of digital media, arts and entertainment. In recent years, new media has developed largely in terms of richness of digital contents (combining text, video and sound) and of technical sophistication. On the other hand, emerging technologies, such as ubiquitous computing, augmented and virtual reality, human-computer interaction, and context and location-awareness, are making possible a paradigm embracing users\u2019 natural behaviour as the centre of humancomputer interaction. Most New Media are actually interactive, and rely on digital content for which user interaction plays a central role. The domain of digital cultural content (digital theatre, mixed reality arts, ubiquitous systems supporting interactive storytelling and TV) is specially challenging, involving a wide range and combination of sophisticated users\u2019 emotions and feelings. This particular domain chosen by CALLAS imposes to advance the understanding of emotional interaction, taking into account also non-Ekmanian emotional categories. CALLAS COMPONENTS: THE SHELF The Shelf consists of a dynamic pool of advanced multimodal interface technologies, selected taking into special account efficiency and robustness, in order to guarantee a consistent performance for many contexts and scenarios, specially for the use in uncontrolled \u201cproduction\u201d scenarios, while many technologies are developed and tested in controlled settings. CALLAS shelf components include: Emotional Speech Recognition: combines keyword spotting in utterances with information about the emotional state of the speaker, according to correspondences with a list of Ekmanian and non-Ekmanian emotions (component specially developed by Facult\xe9 Polytechnique de Mons); Emotional Natural Language Understanding (developed by University of Augsburg): includes acoustic as well as linguistic features, relying on a corpus-driven approach; EVA 2008 London Conference ~ 22-24 July Massimo Bertoncini Irene Buonazia _____________________________________________________________________ 21 Sound Capture and Analysis (component developed by VTT): maps on emotional patterns speech, surrounding sounds (music, crowd cheering), to guarantee natural and adaptive interaction with physical and virtual environment, as well as in the creation of MR/AR environments; Video Feature Extraction: extracts contextual and emotional information about users, environment and media from video streams combining audio and visual information. The component especially analyses video streaming on wide spaces, tracking speed, direction and quantity of movement of items in the space; Gesture and Body Motion Tracking (developed by VTT): provides information about body movement and gestures interpreted as thresholds to different emotional states. The tracking, especially focussing on hands movements, will be performed with different sensors positioned from upper limb to the whole body; Haptic Tracking component is a 3D haptic tracker for virtual environment navigation, based on interpretation of force/tactile feedback (developed by Humanware); it will be further developed into Wearable Interfaces for Motion Capture, embedding different miniaturized transducers for gesture recognition and motion tracking; Multimodal Interpretation of User Experience (developed by University of Teesside) researches on the emotional categorisation of the user experience, aiming at defining a new paradigm for investigating emotions in multimodal interfaces; Affective Multimodal Interpreter / Facial Expression Recognition extracts expressivity from gaze detection, facial features (measured through coordinates of interest point in the face), and gesture recognition (measured through head-hands coordinates). Such components, operating on high-resolution images of frontal faces and signals coming from many sensors, are developed by ICCS. The output of such researches should be an Expressivity Synthesis, able to generate, from image sequences, sensors, history and personality details, an expressive model of user\u2019s behaviour, to be performed by ECA. Emotional Natural Language Generation component (to be developed as a research output of the project by University of Augsburg) is responsible for generating natural language without disregarding the affective aspects of a conversation. It is based on an annotated corpus consisting of sentences that present typical expressions used in a conversation. The corpus has to be annotated with categories and topics that the sentences are about and also with the emotional state that they denote. Affective Music Synthesis (developed by University of Reading) component aims at enhancing musicality and music expression of virtual actors according to the user\u2019s mood, making users\' experience of sound and music less mechanical. Emotional Attentive ECA (Embodied conversational agents, based on achievements of ECA developed by University of Paris8), will investigate on three core capabilities of ECA in emotional and social context: emotional communication through gesture, facial expression, gaze, body; emotional expression in a social context by blending or masking emotions; modelling perceptual-attentive social behaviours that are a basis for interaction, such as mutual, joint and shared attention. THE CALLAS INTEGRATED APPROACH: THE FRAMEWORK The aim of the CALLAS project is to develop a system able to combine emotional components and features in new modalities, enabling different modes of integration, as required by various applications, offering pre-assembled, re-usable, and semantic fusion EVA 2008 London Conference ~ 22-24 July Massimo Bertoncini Irene Buonazia _____________________________________________________________________ 22 components. The CALLAS Framework is being designed as a software infrastructure that will allow a number of She","paperId":"88b2b8ba1bd8f28b06446c97e2b3881ee6d07769","paperTitle":"Emotional interfaces in performing arts: the Callas project","paperUrl":"https://www.semanticscholar.org/paper/88b2b8ba1bd8f28b06446c97e2b3881ee6d07769","images":["https://d3i71xaburhd42.cloudfront.net/88b2b8ba1bd8f28b06446c97e2b3881ee6d07769/6-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/88b2b8ba1bd8f28b06446c97e2b3881ee6d07769/7-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/88b2b8ba1bd8f28b06446c97e2b3881ee6d07769/8-Figure3-1.png"],"pdf":"https://www.scienceopen.com/document_file/5f5a271c-d5ef-4208-80dc-b5f3bab211bd/ScienceOpen/019_Bertoncini.pdf"},{"type":"inproceedings","key":"tian2021adroid","title":"Adroid: Augmenting Hands-on Making with a Collaborative Robot","author":"Tian, Rundong and Paulos, Eric","booktitle":"Proceedings of the 34th Annual ACM Symposium on User Interface Software and Technology","year":"2021","authors":["Rundong Tian","Eric Paulos"],"doi":"10.1145/3472749.3474749","dblp":"conf/uist/TianP21","venue":"UIST","abstract":"Adroid1 enables users to borrow precision and accuracy from a robotic arm when using hand-held tools. When a tool is mounted to the robot, the user can hold and move the tool directly\u2014Adroid measures the user\u2019s applied forces and commands the robot to move in response. Depending on the tool and scenario, Adroid can selectively restrict certain motions. In the resulting interaction, the robot acts like a virtual \u201cjig\u201d which constrains the tool\u2019s motion, augmenting the user\u2019s accuracy, technique, and strength, while not diminishing their agency during open-ended fabrication tasks. We complement these hands-on interactions with projected augmented reality for visual feedback about the state of the system. We show how tools augmented by Adroid can support hands-on making and discuss how it can be configured to support other tasks within and beyond fabrication.","paperId":"cc1d61b8a93d6a17fb13727548ded0acf469d7a9","paperTitle":"Adroid: Augmenting Hands-on Making with a Collaborative Robot","paperUrl":"https://www.semanticscholar.org/paper/cc1d61b8a93d6a17fb13727548ded0acf469d7a9","images":["https://d3i71xaburhd42.cloudfront.net/cc1d61b8a93d6a17fb13727548ded0acf469d7a9/4-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/cc1d61b8a93d6a17fb13727548ded0acf469d7a9/5-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/cc1d61b8a93d6a17fb13727548ded0acf469d7a9/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/cc1d61b8a93d6a17fb13727548ded0acf469d7a9/6-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/cc1d61b8a93d6a17fb13727548ded0acf469d7a9/6-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/cc1d61b8a93d6a17fb13727548ded0acf469d7a9/7-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/cc1d61b8a93d6a17fb13727548ded0acf469d7a9/7-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/cc1d61b8a93d6a17fb13727548ded0acf469d7a9/7-Figure9-1.png","https://d3i71xaburhd42.cloudfront.net/cc1d61b8a93d6a17fb13727548ded0acf469d7a9/8-Figure10-1.png","https://d3i71xaburhd42.cloudfront.net/cc1d61b8a93d6a17fb13727548ded0acf469d7a9/8-Figure11-1.png"],"pdf":"https://dl.acm.org/doi/pdf/10.1145/3472749.3474749"},{"type":"inproceedings","key":"hashimoto2011touchme","title":"Touchme: An Augmented Reality Based Remote Robot Manipulation","author":"Hashimoto, Sunao and Ishida, Akihiko and Inami, Masahiko and Igarashi, Takeo","booktitle":"The 21st International Conference on Artificial Reality and Telexistence, Proceedings of ICAT2011","volume":"2","year":"2011","authors":["Sunao Hashimoto","Akihiko Ishida","Masahiko Inami","Takeo Igarashi"],"venue":"","abstract":"A general remote controlled robot is manipulated by a joystick and a gamepad. However, these methods are difficult for inexperienced users because the mapping between the user input and resulting robot motion is not always intuitive (e.g. tilt a joystick to the right to rotate the robot to the left). To solve this problem, we propose a touch-based interface for remotely controlling a robot from a third-person view, which is called \u201cTouchMe\u201d. This system allows the user to manipulate each part of the robot by directly touching it on a view of the world as seen by a camera looking at the robot from a third-person view. Our system provides intuitive operation, and the user can use our system with minimal user training. In this paper we describe the TouchMe interaction and its prototype implementation. We also introduce three scheduling methods for controlling the robot in response to user interaction and report on the results of empirical comparisons of these methods.","paperId":"6b7aabaf3309470c1caae680e135a9b84cb9f0e2","paperTitle":"TouchMe : An Augmented Reality Based Remote Robot Manipulation","paperUrl":"https://www.semanticscholar.org/paper/6b7aabaf3309470c1caae680e135a9b84cb9f0e2","images":["https://d3i71xaburhd42.cloudfront.net/6b7aabaf3309470c1caae680e135a9b84cb9f0e2/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/6b7aabaf3309470c1caae680e135a9b84cb9f0e2/5-Table1-1.png","https://d3i71xaburhd42.cloudfront.net/6b7aabaf3309470c1caae680e135a9b84cb9f0e2/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/6b7aabaf3309470c1caae680e135a9b84cb9f0e2/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/6b7aabaf3309470c1caae680e135a9b84cb9f0e2/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/6b7aabaf3309470c1caae680e135a9b84cb9f0e2/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/6b7aabaf3309470c1caae680e135a9b84cb9f0e2/4-Figure8-1.png"],"pdf":"http://www.jst.go.jp/erato/igarashi/projects/TouchMe/ICAT11_TouchMe.pdf"},{"type":"inproceedings","key":"gradmann2018augmented","title":"Augmented Reality Robot Operation Interface with Google Tango","author":"Gradmann, Michael and Orendt, Eric M and Schmidt, Edgar and Schweizer, Stephan and Henrich, Dominik","booktitle":"ISR 2018; 50th International Symposium on Robotics","pages":"1--8","year":"2018","organization":"VDE","authors":["Michael Gradmann","Eric M Orendt","Edgar Schmidt","Stephan Schweizer","Dominik Henrich"],"venue":"","abstract":"In this paper we present a novel smart-device-based robot operation interface providing augmented reality (AR) and utilizing a device equipped with self-tracking functionality and a depth camera according to Google Tango reference design. The robot can be both programmed remotely by our Java application and taught by kinesthetic programming. The system also provides an object detection that is capable of recognizing objects. Additionally we designed and investigated an adaptive program execution. The ease of use is ensured by AR features such as the highlighting of detected objects and the preview of robot operations. The main contribution is a robot operation interface that can be multifariously programmed and is capable of adapting to changing surroundings based on commercially available devices.","paperId":"0da52803dae9269aa409d5b06ed053cf7d2054e6","paperTitle":"Augmented Reality Robot Operation Interface with Google Tango","paperUrl":"https://www.semanticscholar.org/paper/0da52803dae9269aa409d5b06ed053cf7d2054e6","images":[],"pdf":null},{"type":"inproceedings","key":"luebbers2019augmented","title":"Augmented Reality Interface for Constrained Learning from Demonstration","author":"Luebbers, Matthew B and Brooks, Connor and Kim, Minjae John and Szafir, Daniel and Hayes, Bradley","booktitle":"Proceedings of the 2nd International Workshop on Virtual, Augmented, and Mixed Reality for HRI (VAM-HRI)","year":"2019","authors":["Matthew B Luebbers","Connor Brooks","Minjae John Kim","Daniel Szafir","Bradley Hayes"],"venue":"","abstract":"This paper presents a novel augmented reality (AR) interface for the visualization and directed control of robot skill Learning from Demonstration (LfD). This system is designed for use with the Concept-Constrained Learning from Demonstration (CC-LfD) algorithm for general robotic arm manipulation tasks, in which trajectories in an LfD system are subjected to various constraints during the learning process to ensure that the desired skill is learned properly. This system provides an interactive visualization of observed and learned robot trajectories, as well as the active predicate constraints applied for CC-LfD. In this paper, we describe current work on a system for helping users give improved trajectory demonstrations using AR visualizations of constraints, as well as a planned human subjects experiment to evaluate the usefulness of this system. Additionally, we discuss future extensions of this work involving using an AR interface to modify existing trajectory demonstrations.","paperId":"fedf368f57d8fdb76f13429d6108e6355f354d0b","paperTitle":"Augmented Reality Interface for Constrained Learning from Demonstration","paperUrl":"https://www.semanticscholar.org/paper/fedf368f57d8fdb76f13429d6108e6355f354d0b","images":["https://d3i71xaburhd42.cloudfront.net/fedf368f57d8fdb76f13429d6108e6355f354d0b/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/fedf368f57d8fdb76f13429d6108e6355f354d0b/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/fedf368f57d8fdb76f13429d6108e6355f354d0b/3-Figure3-1.png"],"pdf":"https://m-luebbers.github.io/pdfs/vamhri19.pdf"},{"type":"article","key":"hoang2021virtual","title":"Virtual Barriers in Augmented Reality for Safe and Effective Human-Robot Cooperation in Manufacturing","author":"Hoang, Khoa Cong and Chan, Wesley P and Lay, Steven and Cosgun, Akansel and Croft, Elizabeth","journal":"arXiv preprint arXiv:2104.05211","year":"2021","authors":["Khoa Cong Hoang","Wesley P Chan","Steven Lay","Akansel Cosgun","Elizabeth Croft"],"dblp":"journals/corr/abs-2104-05211","venue":"ArXiv","abstract":"Safety is a fundamental requirement in any human-robot collaboration scenario. To ensure the safety of users for such scenarios, we propose a novel Virtual Barrier system facilitated by an augmented reality interface. Our system provides two kinds of Virtual Barriers to ensure safety: 1) a Virtual Person Barrier which encapsulates and follows the user to protect them from colliding with the robot, and 2) Virtual Obstacle Barriers which users can spawn to protect objects or regions that the robot should not enter. To enable effective human-robot collaboration, our system includes an intuitive robot programming interface utilizing speech commands and hand gestures, and features the capability of automatic path re-planning when potential collisions are detected as a result of a barrier intersecting the robot\u2019s planned path. We compared our novel system with a standard 2D display interface through a user study, where participants performed a task mimicking an industrial manufacturing procedure. Results show that our system increases the user\u2019s sense of safety and task efficiency, and makes the interaction more intuitive.","paperId":"7a18ff56abb0317ce853e44cfbcfc4e28fa124b4","paperTitle":"Virtual Barriers in Augmented Reality for Safe and Effective Human-Robot Cooperation in Manufacturing","paperUrl":"https://www.semanticscholar.org/paper/7a18ff56abb0317ce853e44cfbcfc4e28fa124b4","images":["https://d3i71xaburhd42.cloudfront.net/7ab8c72c1cab2f8daa0fc8ab438338df3f436c55/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/7ab8c72c1cab2f8daa0fc8ab438338df3f436c55/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/7ab8c72c1cab2f8daa0fc8ab438338df3f436c55/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/7ab8c72c1cab2f8daa0fc8ab438338df3f436c55/3-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/7ab8c72c1cab2f8daa0fc8ab438338df3f436c55/4-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/7ab8c72c1cab2f8daa0fc8ab438338df3f436c55/4-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/7ab8c72c1cab2f8daa0fc8ab438338df3f436c55/4-Figure7-1.png","https://d3i71xaburhd42.cloudfront.net/7ab8c72c1cab2f8daa0fc8ab438338df3f436c55/4-Figure8-1.png","https://d3i71xaburhd42.cloudfront.net/7ab8c72c1cab2f8daa0fc8ab438338df3f436c55/5-TableI-1.png"],"pdf":null},{"type":"inproceedings","key":"chan2018virtual","title":"Virtual Barriers in Augmented Reality for Safe Human-Robot Collaboration in Manufacturing","author":"Chan, Wesley P and Karim, Adnan and Quintero, Camilo P and Van der Loos, HF Machiel and Croft, Elizabeth","booktitle":"Robotic Co-Workers 4.0 2018: Human Safety and Comfort in Human-Robot Interactive Social Environments","year":"2018","authors":["Wesley P Chan","Adnan Karim","Camilo P Quintero","HF Machiel Van der Loos","Elizabeth Croft"],"venue":"","abstract":"We present a system for safe human-robot collaboration through the use of virtual barriers created in augmented reality (AR). In any human-robot cooperative scenario, safety is foremost important and must be guaranteed. Using a Microsoft HoloLens AR headset, our system provides two mechanisms for ensuring safety 1) a person barrier that encapsulates and follows the user to protect the user from collision with the robot. 2) virtual barriers that can be created and placed arbitrarily by the user to protect surrounding objects or regions from the robot. We demonstrate the usefulness of our system through two case studies representing tasks in collaborative manufacturing, showing how our system achieves seamless operation through two different response behaviours to barrier collisions.","paperId":"cff3f55f8479ace74e695f5a9ed363c361dbaf94","paperTitle":"Virtual barriers in augmented reality for safe human-robot collaboration in manufacturing","paperUrl":"https://www.semanticscholar.org/paper/cff3f55f8479ace74e695f5a9ed363c361dbaf94","images":["https://d3i71xaburhd42.cloudfront.net/e21388bda267c3857ac732f1cf7c112d45ea6668/1-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/e21388bda267c3857ac732f1cf7c112d45ea6668/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/e21388bda267c3857ac732f1cf7c112d45ea6668/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/e21388bda267c3857ac732f1cf7c112d45ea6668/4-Figure4-1.png"],"pdf":null},{"type":"book","key":"tran2020exploring","title":"Exploring Mixed Reality Robot Communication under Different Types of Mental Workload","author":"Tran, Nhan","year":"2020","publisher":"Colorado School of Mines","authors":["Nhan Tran"],"doi":"10.31219/osf.io/f3a8c","venue":"","abstract":"This paper explores the tradeoffs between different types of mixed reality robotic communication under different levels of user workload. We present the results of a within-subjects experiment in which we systematically and jointly vary robot communication style alongside level and type of cognitive load, and measure subsequent impacts on accuracy, reaction time, and perceived workload and effectiveness. Our preliminary results suggest that although humans may not notice differences, the manner of load a user is under and the type of communication style used by a robot they interact with do in fact interact to determine their task effectiveness","paperId":"cf385ecbc81aeb27ee5c0f67e49eb4ce484164fa","paperTitle":"Exploring Mixed Reality Robot Communication Under Different types of Mental Workload","paperUrl":"https://www.semanticscholar.org/paper/cf385ecbc81aeb27ee5c0f67e49eb4ce484164fa","images":["https://d3i71xaburhd42.cloudfront.net/cf385ecbc81aeb27ee5c0f67e49eb4ce484164fa/3-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/cf385ecbc81aeb27ee5c0f67e49eb4ce484164fa/3-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/cf385ecbc81aeb27ee5c0f67e49eb4ce484164fa/4-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/cf385ecbc81aeb27ee5c0f67e49eb4ce484164fa/5-Figure4-1.png","https://d3i71xaburhd42.cloudfront.net/cf385ecbc81aeb27ee5c0f67e49eb4ce484164fa/5-Figure5-1.png","https://d3i71xaburhd42.cloudfront.net/cf385ecbc81aeb27ee5c0f67e49eb4ce484164fa/5-Figure6-1.png","https://d3i71xaburhd42.cloudfront.net/cf385ecbc81aeb27ee5c0f67e49eb4ce484164fa/7-Figure7-1.png"],"pdf":"https://osf.io/f3a8c/download"},{"type":"article","key":"argyle1976gaze","title":"Gaze and Mutual Gaze.","author":"Argyle, Michael and Cook, Mark","year":"1976","publisher":"Cambridge U Press","authors":["Michael Argyle","Mark Cook"],"doi":"10.1017/S0007125000073980","venue":"British Journal of Psychiatry","abstract":"One of the first psychologists to investigate experimentally the role of gaze in human behaviour was Michael Argyle. In 1963 he set up a research group at Oxford with Ted Crossman and Adam Kendon, to study non-verbal communication in human social interaction, which included gaze as an important aspect of this behaviour. Shortly afterwards, Mark Cook joined this group which was funded until 1975, during which time considerable research on gaze had been carried out both at Oxford and elsewhere. This book summarises much of the work done in this field up until that time.","paperId":"0429dd2f0b5ce8f5ab3aae53bb744e26c884de6f","paperTitle":"Gaze and Mutual Gaze","paperUrl":"https://www.semanticscholar.org/paper/0429dd2f0b5ce8f5ab3aae53bb744e26c884de6f","images":[],"pdf":null},{"type":"inproceedings","key":"suzuki2012development","title":"Development of New Augmented Reality Function Using Intraperitoneal Multi-View Camera","author":"Suzuki, Naoki and Hattori, Asaki","booktitle":"Workshop on Augmented Environments for Computer-Assisted Interventions","pages":"67--76","year":"2012","organization":"Springer","authors":["Naoki Suzuki","Asaki Hattori"],"doi":"10.1007/978-3-642-38085-3_8","dblp":"conf/miccai/SuzukiH12","venue":"AE-CAI","abstract":"We developed a multi-view video camera system that works inside the abdominal cavity to obtain a wider range of information during laparoscopic and robotic surgery. This video camera system is able to enter the abdominal cavity and surround an organ in a fixed degree. We conducted in vitro and in vivo experiments to clarify the functions of the system. The advantage of the system is that it can alter the viewpoint without physically moving the camera and can obtain diversified intraperitoneal information. Moreover, by taking advantage of its video image array with geometrical regularity, we build a novel augmented reality function compared with the images for conventional navigation surgery.","paperId":"5ef4a9d79eafadcc20ecc5c89d6f16656e3689f8","paperTitle":"Development of New Augmented Reality Function Using Intraperitoneal Multi-view Camera","paperUrl":"https://www.semanticscholar.org/paper/5ef4a9d79eafadcc20ecc5c89d6f16656e3689f8","images":[],"pdf":null},{"type":"article","key":"botev2021immersive","title":"Immersive Robotic Telepresence for Remote Educational Scenarios","author":"Botev, Jean and Rodr\'iguez Lera, Francisco J","journal":"Sustainability","volume":"13","number":"9","pages":"4717","year":"2021","publisher":"Multidisciplinary Digital Publishing Institute","authors":["Jean Botev","Francisco J Rodr\'iguez Lera"],"doi":"10.3390/SU13094717","venue":"","abstract":"Social robots have an enormous potential for educational applications and allow for cognitive outcomes that are similar to those with human involvement. Remotely controlling a social robot to interact with students and peers in an immersive fashion opens up new possibilities for instructors and learners alike. Using immersive approaches can promote engagement and have beneficial effects on remote lesson delivery and participation. However, the performance and power consumption associated with the involved devices are often not sufficiently contemplated, despite being particularly important in light of sustainability considerations. The contributions of this research are thus twofold. On the one hand, we present telepresence solutions for a social robot\u2019s location-independent operation using (a) a virtual reality headset with controllers and (b) a mobile augmented reality application. On the other hand, we perform a thorough analysis of their power consumption and system performance, discussing the impact of employing the various technologies. Using the QTrobot as a platform, direct and immersive control via different interaction modes, including motion, emotion, and voice output, is possible. By not focusing on individual subsystems or motor chains, but the cumulative energy consumption of an unaltered robot performing remote tasks, this research provides orientation regarding the actual cost of deploying immersive robotic telepresence solutions.","paperId":"0d12b2969ec7443ac329df10c94aacc9befb1f67","paperTitle":"Immersive Robotic Telepresence for Remote Educational Scenarios","paperUrl":"https://www.semanticscholar.org/paper/0d12b2969ec7443ac329df10c94aacc9befb1f67","images":[],"pdf":"https://www.mdpi.com/2071-1050/13/9/4717/pdf"},{"type":"article","key":"porpiglia2020three","title":"Three-Dimensional Augmented Reality Robot-Assisted Partial Nephrectomy in Case of Complex Tumours (PADUA\u2265 10): A New Intraoperative Tool Overcoming the Ultrasound Guidance","author":"Porpiglia, Francesco and Checcucci, Enrico and Amparore, Daniele and Piramide, Federico and Volpi, Gabriele and Granato, Stefano and Verri, Paolo and Manfredi, Matteo and Bellin, Andrea and Piazzolla, Pietro and others","journal":"European urology","volume":"78","number":"2","pages":"229--238","year":"2020","publisher":"Elsevier","authors":["Francesco Porpiglia","Enrico Checcucci","Daniele Amparore","Federico Piramide","Gabriele Volpi","Stefano Granato","Paolo Verri","Matteo Manfredi","Andrea Bellin","Pietro Piazzolla","undefined others"],"doi":"10.1016/j.eururo.2019.11.024","venue":"European urology","abstract":null,"paperId":"1306dcaef04cfcd207d56896ed1083e4aac1efdf","paperTitle":"Three-dimensional Augmented Reality Robot-assisted Partial Nephrectomy in Case of Complex Tumours (PADUA \u226510): A New Intraoperative Tool Overcoming the Ultrasound Guidance.","paperUrl":"https://www.semanticscholar.org/paper/1306dcaef04cfcd207d56896ed1083e4aac1efdf","images":[],"pdf":"https://doi.org/10.1016/j.eururo.2019.11.024"},{"type":"article","key":"bianchi2021use","title":"The Use of Augmented Reality to Guide the Intraoperative Frozen Section during Robot-Assisted Radical Prostatectomy","author":"Bianchi, Lorenzo and Chessa, Francesco and Angiolini, Andrea and Cercenelli, Laura and Lodi, Simone and Bortolani, Barbara and Molinaroli, Enrico and Casablanca, Carlo and Droghetti, Matteo and Gaudiano, Caterina and others","journal":"European Urology","volume":"80","number":"4","pages":"480--488","year":"2021","publisher":"Elsevier","authors":["Lorenzo Bianchi","Francesco Chessa","Andrea Angiolini","Laura Cercenelli","Simone Lodi","Barbara Bortolani","Enrico Molinaroli","Carlo Casablanca","Matteo Droghetti","Caterina Gaudiano","undefined others"],"doi":"10.1016/j.eururo.2021.06.020","venue":"European urology","abstract":null,"paperId":"aab78d158a535a58ca3ff2c266f6dbc723c5f606","paperTitle":"The Use of Augmented Reality to Guide the Intraoperative Frozen Section During Robot-assisted Radical Prostatectomy.","paperUrl":"https://www.semanticscholar.org/paper/aab78d158a535a58ca3ff2c266f6dbc723c5f606","images":[],"pdf":"https://doi.org/10.1016/j.eururo.2021.06.020"},{"type":"article","key":"kansaku2010my","title":"My Thoughts through a Robot\'s Eyes: An Augmented Reality-Brain--Machine Interface","author":"Kansaku, Kenji and Hata, Naoki and Takano, Kouji","journal":"Neuroscience research","volume":"66","number":"2","pages":"219--222","year":"2010","publisher":"Elsevier","authors":["Kenji Kansaku","Naoki Hata","Kouji Takano"],"doi":"10.1016/j.neures.2009.10.006","venue":"Neuroscience Research","abstract":null,"paperId":"f7af2e92a9ebce047c5bdeb9d49918d4b7e604ed","paperTitle":"My thoughts through a robot\'s eyes: An augmented reality-brain\u2013machine interface","paperUrl":"https://www.semanticscholar.org/paper/f7af2e92a9ebce047c5bdeb9d49918d4b7e604ed","images":["https://d3i71xaburhd42.cloudfront.net/be1f428b533da838b694dffd954795f1e7ebbe5f/2-Figure1-1.png","https://d3i71xaburhd42.cloudfront.net/be1f428b533da838b694dffd954795f1e7ebbe5f/2-Figure2-1.png","https://d3i71xaburhd42.cloudfront.net/be1f428b533da838b694dffd954795f1e7ebbe5f/3-Figure3-1.png","https://d3i71xaburhd42.cloudfront.net/be1f428b533da838b694dffd954795f1e7ebbe5f/4-Figure4-1.png"],"pdf":"https://doi.org/10.1016/j.neures.2009.10.006"},{"type":"article","key":"blankemeyer2018intuitive","title":"Intuitive Robot Programming Using Augmented Reality","author":"Blankemeyer, Sebastian and Wiemann, Rolf and Posniak, Lukas and Pregizer, Christoph and Raatz, Annika","journal":"Procedia CIRP","volume":"76","pages":"155--160","year":"2018","publisher":"Elsevier","authors":["Sebastian Blankemeyer","Rolf Wiemann","Lukas Posniak","Christoph Pregizer","Annika Raatz"],"doi":"10.1016/J.PROCIR.2018.02.028","venue":"","abstract":null,"paperId":"e6cace1053e6c421794e56b8340b984b91361d00","paperTitle":"Intuitive Robot Programming Using Augmented Reality","paperUrl":"https://www.semanticscholar.org/paper/e6cace1053e6c421794e56b8340b984b91361d00","images":[],"pdf":"https://doi.org/10.1016/j.procir.2018.02.028"}]'),y=JSON.parse('["jones2021belonging","young2007robot","takashima2016study","al2012furhat","peng2018roma","frank2017mobile","gao2019pati","elsharkawy2021uwb","chen2020combination","darbar2019dronesar","wu2020mixed","tran2021get","ochiai2011homunculus","leithinger2011direct","de2019intuitive","mueller2012interactive","gradmann2018augmented","frank2017toward","takashima2013transformtable","ozgur2017cellulo","hiraki2015phygital","linder2010luminar","materna2018interactive","mercedes-f15","walker2019robot","erat2018drone","ruiz2015immersive","bambusek2019combining","zollmann2014flyar","ostanin2020human","rosen2020communicating","groechel2019using","young2006mixed","chakraborti2018projection","andersen2016projecting","fuste2020kinetic","chen2021pinpointfly","seifert2014hover","nintendo-mklive","cauchard2019drone","arevalo2021assisting","aleotti2017detection","ghiringhelli2014interactive","zhu2016virtually","hashimoto2011touchme","pedersen2011tangible","hedayati2018improving","urbani2018exploring","quintero2018robot","tian2021adroid","ostanin2018interactive","walker2018communicating","kapinus2019spatially","ishii2009designing","hoang2021virtual","kato2009multi","hiraki2019navigatorch","makhataeva2019safety","omidshafiei2016measurable","hololens-robot","hiroi2010evaluation","jones2020vroom","lee2018physical","xiao2016andantino","kasahara2013extouch","guo2009touch","boeing-uav","nakagaki2016materiable","roudaut2013morphees","suzuki2019shapebots","taher2015exploring","watanabe2015communicating","piumatti2017spatial","nowacka2013touchbugs","yuan2019human","huang2019flight","jost2018safe","liu2011roboshop","prattico2019user","villanueva2021robotar","yamaoka2016mirageprinter","qian2019augmented","jaguar-rover","reardon2019communicating","gronbaek2020kirigamitable"]'),v=a(184),w=function(e){l(a,e);var t=h(a);function a(e){return i(this,a),t.call(this,e)}return d(a,[{key:"componentDidMount",value:function(){}},{key:"onClick",value:function(e){console.log(e),window.App.setState({current:e})}},{key:"render",value:function(){var e,t=this.props.item;e||(e=t.journal),e||(e=t.booktitle),e||(e=t.venue);var a=t.authors[0];return a&&(a=g().last(a.split(" "))),(0,v.jsx)("div",{className:"column",onClick:this.onClick.bind(this,t),children:(0,v.jsxs)("div",{className:"paper-card ui raised link card",children:[(0,v.jsx)("div",{className:"content",children:(0,v.jsxs)("div",{className:"meta",children:[(0,v.jsx)("div",{className:"left floated",children:(0,v.jsx)("span",{className:"date",children:t.year})}),(0,v.jsx)("div",{className:"right floated",children:(0,v.jsx)("span",{className:"date",children:"".concat(a," et al.")})})]})}),(0,v.jsx)("div",{className:"image",children:(0,v.jsx)("img",{className:"ui small image",src:"/ar-and-robotics/cover/".concat(t.key,".jpg")})}),(0,v.jsxs)("div",{className:"content main-content",children:[(0,v.jsx)("div",{className:"description max-three-lines",children:(0,v.jsx)("b",{children:t.title})}),(0,v.jsx)("div",{className:"meta max-two-lines",children:e})]}),(0,v.jsx)("div",{className:"extra content",children:(0,v.jsxs)("div",{className:"right floated",children:[t.pdf&&(0,v.jsx)("a",{className:"ui mini button",href:t.pdf,target:"_blank",children:"PDF"}),(0,v.jsx)("a",{className:"ui mini button",href:"https://doi.org/".concat(t.doi),target:"_blank",children:"DOI"})]})})]})},this.props.key)}}]),a}(e.Component),x=function(e){l(a,e);var t=h(a);function a(e){return i(this,a),t.call(this,e)}return d(a,[{key:"componentDidMount",value:function(){}},{key:"render",value:function(){var e,t,a=this.props.item;return e||(e=a.journal),e||(e=a.booktitle),e||(e=a.venue),a.authors&&(t=a.authors[0]),t&&(t=g().last(t.split(" "))),(0,v.jsx)("div",{id:"modal",children:(0,v.jsxs)("div",{className:"ui large modal",children:[(0,v.jsxs)("div",{className:"header",children:["".concat(t," et al. ").concat(a.year),(0,v.jsx)("div",{className:"actions",style:{float:"right",cursor:"pointer",color:"grey"},children:(0,v.jsx)("i",{className:"ui right cancel close icon"})})]}),(0,v.jsxs)("div",{className:"card image content",children:[(0,v.jsx)("div",{className:"ui medium image",children:(0,v.jsx)("img",{src:"/ar-and-robotics/cover/".concat(a.key,".jpg")})}),(0,v.jsxs)("div",{className:"description",children:[(0,v.jsx)("div",{className:"ui header",children:(0,v.jsx)("h3",{children:a.title})}),(0,v.jsx)("p",{children:a.authors&&a.authors.join(", ")}),(0,v.jsx)("div",{className:"meta",children:(0,v.jsx)("p",{children:(0,v.jsx)("i",{children:e})})}),(0,v.jsx)("br",{}),(0,v.jsx)("div",{className:"meta",children:(0,v.jsxs)("p",{children:[(0,v.jsx)("a",{className:"ui mini button",href:"http://doi.org/".concat(a.doi),target:"_blank",children:"DOI"}),(0,v.jsx)("a",{className:"ui mini button",href:a.paperUrl,target:"_blank",children:"Scholar"}),a.pdf&&(0,v.jsx)("a",{className:"ui mini button",href:a.pdf,target:"_blank",children:"PDF"})]})})]})]}),(0,v.jsx)("div",{className:"content",children:(0,v.jsx)("p",{children:a.abstract})}),(0,v.jsx)("div",{className:"content",children:(0,v.jsx)("div",{className:"ui images",children:a.images&&a.images.map((function(e,t){return(0,v.jsx)("div",{className:"image",children:(0,v.jsx)("img",{src:e})})}))})}),(0,v.jsx)("div",{className:"actions",children:(0,v.jsx)("div",{className:"ui right cancel button",children:"Close"})})]})})}}]),a}(e.Component),k=function(e){l(a,e);var t=h(a);function a(e){var n;i(this,a),n=t.call(this,e),window.App=s(n),window.figures=y,m=m.map((function(e){return e.rank=y.includes(e.key)?1:0,e})),m=g().sortBy(m,["year","rank"]),m=g().reverse(m);var o=["genccturk2019development","tanzi2021real","hoang2021virtual","nintendo-mklive","renner2018facilitating","mourtzis2017augmented","mercedes-f15","fang2013orientation","suzuki2012development","argyle1976gaze"];return m=m.filter((function(e){return!o.includes(e.key)})),n.state={items:m,current:{}},n}return d(a,[{key:"componentDidMount",value:function(){this.setState({test:"123"})}},{key:"render",value:function(){return(0,v.jsxs)(v.Fragment,{children:[(0,v.jsxs)("div",{children:[(0,v.jsxs)("div",{className:"ui vertical masthead center aligned segment",children:[(0,v.jsxs)("div",{className:"ui text container",children:[(0,v.jsxs)("h1",{className:"ui center aligned icon header",children:[(0,v.jsx)("i",{className:"settings icon"}),"Augmented Reality and Robotics"]}),(0,v.jsx)("p",{children:(0,v.jsx)("b",{children:"A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces"})}),(0,v.jsx)("p",{children:(0,v.jsx)("i",{children:"by Ryo Suzuki, Adnan Karim, Tian Xia, Hooman Hedayati, Nicolai Marquardt"})}),(0,v.jsx)("a",{href:"/ar-and-robotics/chi-2022/chi-2022.pdf",target:"_blank",children:(0,v.jsx)("img",{id:"teaser",src:"/ar-and-robotics/sketches/teaser.jpg"})})]}),(0,v.jsx)("div",{children:(0,v.jsx)("a",{className:"ui tiny images",href:"/ar-and-robotics/chi-2022/chi-2022.pdf",target:"_blank",children:o(Array(10).keys()).map((function(e){for(var t=String(e+1);t.length<2;)t="0"+t;return(0,v.jsx)("img",{src:"/ar-and-robotics/chi-2022/original/paper-".concat(t,".jpg")},e)}))})})]}),(0,v.jsx)("div",{className:"ui vertical segment",children:(0,v.jsx)("div",{id:"cards",className:"ui five column centered stackable grid",children:this.state.items.map((function(e,t){return(0,v.jsx)(w,{item:e},t)}))})})]}),(0,v.jsx)(x,{item:this.state.current})]})}}]),a}(e.Component),R=k;t.render((0,v.jsx)(e.StrictMode,{children:(0,v.jsx)(R,{})}),document.getElementById("root"))}()}();
//# sourceMappingURL=main.6f522686.js.map